# ğŸ¯ GuÃ­a de Preguntas - Fundamentos de Interpretabilidad en NLP

## Instrucciones
Responde cada pregunta con tus propias palabras. Puedes usar diagramas, ejemplos o cÃ³digo cuando sea Ãºtil.

---

## ğŸ“š BLOQUE 1: Interpretabilidad en Machine Learning

### Conceptos Fundamentales

**1.1** Â¿QuÃ© es interpretabilidad en machine learning? Â¿Por quÃ© es importante?

**1.2** Â¿CuÃ¡l es la diferencia entre interpretabilidad, explicabilidad y transparencia?

**1.3** Menciona 3 casos de uso reales donde la interpretabilidad es crÃ­tica (ej: medicina, finanzas, legal)

**1.4** Â¿QuÃ© es una "caja negra" en ML? Â¿Todos los modelos son cajas negras?

---

### Tipos de Interpretabilidad

**1.5** Â¿QuÃ© es interpretabilidad **global** vs **local**? Da un ejemplo de cada una.

**1.6** Â¿QuÃ© son mÃ©todos **model-agnostic** vs **model-specific**? Â¿CuÃ¡ndo usar cada uno?

**1.7** Â¿QuÃ© es una explicaciÃ³n **post-hoc**? Contrasta con modelos inherentemente interpretables.

**1.8** Â¿Feature importance tradicional es suficiente? Â¿QuÃ© limitaciones tiene?

---

### Trade-offs y Limitaciones

**1.9** Â¿Existe un trade-off entre performance y interpretabilidad? Explica con ejemplos.

**1.10** Â¿QuÃ© riesgos hay en confiar ciegamente en las explicaciones de un modelo?

**1.11** Â¿CÃ³mo validamos que una explicaciÃ³n es "correcta" o "Ãºtil"?

---

## ğŸ¤– BLOQUE 2: Arquitectura de Transformers

### Conceptos BÃ¡sicos

**2.1** Â¿QuÃ© problema resuelven los Transformers que las RNN/LSTM no podÃ­an resolver bien?

**2.2** Â¿QuÃ© es el mecanismo de **atenciÃ³n** (attention)? Explica con un ejemplo de traducciÃ³n.

**2.3** Â¿QuÃ© es **self-attention**? Â¿En quÃ© se diferencia de attention tradicional?

**2.4** Dibuja un diagrama simple de la arquitectura Transformer (encoder-decoder)

---

### Componentes Clave

**2.5** Â¿QuÃ© son los **positional embeddings**? Â¿Por quÃ© son necesarios?

**2.6** Â¿QuÃ© es **multi-head attention**? Â¿Por quÃ© usar mÃºltiples "cabezas"?

**2.7** Â¿QuÃ© hace la capa **Feed-Forward** en cada bloque del Transformer?

**2.8** Â¿QuÃ© es **layer normalization** y por quÃ© se usa?

---

### BERT y DistilBERT

**2.9** Â¿BERT es encoder-only, decoder-only o encoder-decoder? Â¿Por quÃ©?

**2.10** Â¿QuÃ© es el token `[CLS]` y para quÃ© se usa en clasificaciÃ³n?

**2.11** Â¿QuÃ© es **DistilBERT**? Â¿CÃ³mo se "destila" conocimiento de BERT?

**2.12** Â¿CuÃ¡ntas capas tiene DistilBERT vs BERT? Â¿CuÃ¡nto performance se pierde?

**2.13** Para clasificaciÃ³n de sentimientos, Â¿quÃ© parte del modelo DistilBERT usamos?

---

### Fine-tuning

**2.14** Â¿QuÃ© significa hacer **fine-tuning** de un modelo pre-entrenado?

**2.15** Â¿QuÃ© capas se entrenan durante fine-tuning para clasificaciÃ³n?

**2.16** Â¿Por quÃ© usar un modelo pre-entrenado en vez de entrenar desde cero?

---

## ğŸ” BLOQUE 3: SHAP (SHapley Additive exPlanations)

### Fundamentos TeÃ³ricos

**3.1** Â¿QuÃ© son los **valores de Shapley** en teorÃ­a de juegos?

**3.2** Explica la analogÃ­a de "coaliciones" en Shapley values aplicado a features de ML

**3.3** Â¿QuÃ© significa que SHAP tiene la propiedad de **eficiencia**? Â¿Y **simetrÃ­a**?

**3.4** Â¿QuÃ© es la propiedad **dummy** en SHAP? Da un ejemplo.

---

### SHAP en la PrÃ¡ctica

**3.5** Â¿QuÃ© es **KernelSHAP**? Â¿CÃ³mo aproxima los valores de Shapley?

**3.6** Para un modelo de texto, Â¿quÃ© representan las "features" en SHAP? (palabras, tokens, embeddings?)

**3.7** Â¿CÃ³mo se calculan SHAP values para un Transformer? Â¿QuÃ© estrategia de masking se usa?

**3.8** Â¿SHAP values son siempre positivos? Â¿QuÃ© significa un valor negativo?

**3.9** Â¿CÃ³mo se interpreta un SHAP value de +0.3 para la palabra "excelente"?

---

### Visualizaciones SHAP

**3.10** Â¿QuÃ© muestra un **waterfall plot** de SHAP?

**3.11** Â¿QuÃ© muestra un **bar plot** de importancia global?

**3.12** Â¿QuÃ© diferencia hay entre analizar una instancia vs el dataset completo con SHAP?

---

### Ventajas y Limitaciones

**3.13** Â¿CuÃ¡les son las 3 principales ventajas de SHAP sobre otros mÃ©todos?

**3.14** Â¿CuÃ¡l es la principal desventaja de SHAP? (pista: complejidad computacional)

**3.15** Â¿SHAP funciona para cualquier tipo de modelo? (Ã¡rboles, redes neuronales, etc.)

---

## ğŸ‹ BLOQUE 4: LIME (Local Interpretable Model-agnostic Explanations)

### Fundamentos del Algoritmo

**4.1** Â¿QuÃ© significa que LIME es **model-agnostic**?

**4.2** Â¿QuÃ© significa que LIME genera explicaciones **locales**?

**4.3** Explica el algoritmo de LIME en 4 pasos:
   - Paso 1: Â¿QuÃ© hace con el input?
   - Paso 2: Â¿QuÃ© modelo entrena?
   - Paso 3: Â¿CÃ³mo pondera los samples?
   - Paso 4: Â¿QuÃ© retorna?

**4.4** Â¿Por quÃ© LIME usa un modelo lineal si el modelo original es complejo?

---

### LIME para Texto

**4.5** Para texto, Â¿cÃ³mo "perturba" LIME una oraciÃ³n? Da un ejemplo concreto.

**4.6** Si la oraciÃ³n es "This movie is absolutely fantastic", Â¿quÃ© perturbaciones crearÃ­a LIME?

**4.7** Â¿CuÃ¡ntas perturbaciones genera LIME tÃ­picamente? Â¿MÃ¡s es siempre mejor?

**4.8** Â¿QuÃ© es el parÃ¡metro `num_features` en LIME? Â¿CÃ³mo elegir su valor?

---

### InterpretaciÃ³n de Resultados

**4.9** Si LIME dice que "terrible" tiene peso -0.45, Â¿quÃ© significa?

**4.10** Â¿Las explicaciones de LIME son siempre consistentes para el mismo input?

**4.11** Â¿QuÃ© pasa si ejecuto LIME dos veces sobre el mismo texto? Â¿Obtengo el mismo resultado?

---

### Ventajas y Limitaciones

**4.12** Â¿CuÃ¡l es la principal ventaja de LIME sobre SHAP?

**4.13** Â¿LIME garantiza alguna propiedad teÃ³rica (como SHAP)? Â¿Por quÃ© sÃ­ o no?

**4.14** Â¿QuÃ© significa que LIME es "inestable"? Da un ejemplo.

**4.15** Â¿CuÃ¡ndo preferirÃ­as LIME sobre SHAP?

---

## âš–ï¸ BLOQUE 5: SHAP vs LIME

### ComparaciÃ³n Conceptual

**5.1** Resume en una tabla las diferencias clave entre SHAP y LIME:
| Aspecto | SHAP | LIME |
|---------|------|------|
| Base teÃ³rica | ? | ? |
| Scope (global/local) | ? | ? |
| Velocidad | ? | ? |
| GarantÃ­as formales | ? | ? |

**5.2** Â¿CuÃ¡ndo usarÃ­as SHAP en vez de LIME?

**5.3** Â¿CuÃ¡ndo usarÃ­as LIME en vez de SHAP?

**5.4** Â¿Pueden SHAP y LIME dar explicaciones contradictorias para el mismo input?

---

### AplicaciÃ³n a NLP

**5.5** Para un modelo de anÃ¡lisis de sentimientos, Â¿quÃ© mÃ©todo serÃ­a mejor para:
   - Entender quÃ© palabras son mÃ¡s importantes globalmente?
   - Explicar una predicciÃ³n especÃ­fica a un usuario?
   - Depurar por quÃ© el modelo falla en ciertos casos?

**5.6** Â¿CÃ³mo medirÃ­as si SHAP o LIME da "mejores" explicaciones?

**5.7** Si SHAP y LIME identifican palabras diferentes como importantes, Â¿a cuÃ¡l le creerÃ­as? Â¿Por quÃ©?

---

## ğŸ¯ BLOQUE 6: MÃ©tricas de ValidaciÃ³n

### EvaluaciÃ³n de Explicaciones

**6.1** Â¿QuÃ© es **fidelidad** de una explicaciÃ³n? Â¿CÃ³mo se mide?

**6.2** Â¿QuÃ© es **estabilidad** de una explicaciÃ³n? Da un ejemplo de explicaciÃ³n inestable.

**6.3** Â¿QuÃ© es **comprensibilidad**? Â¿CÃ³mo la evaluarÃ­as?

**6.4** PropÃ³n un experimento para validar que una explicaciÃ³n es "correcta"

---

### Tests de Sanidad

**6.5** Si eliminas la palabra mÃ¡s importante segÃºn SHAP/LIME, Â¿quÃ© deberÃ­a pasar con la predicciÃ³n?

**6.6** Si duplicas una palabra importante, Â¿cÃ³mo deberÃ­a cambiar su SHAP value?

**6.7** Â¿CÃ³mo verificarÃ­as que SHAP/LIME no estÃ¡ dando explicaciones aleatorias?

---

## ğŸš€ BLOQUE 7: AplicaciÃ³n al Proyecto

### Dataset y Modelo

**7.1** Â¿Por quÃ© usamos el dataset **IMDb** para este proyecto?

**7.2** Â¿QuÃ© caracterÃ­sticas tiene el dataset IMDb? (tamaÃ±o, balance, longitud promedio)

**7.3** Â¿Por quÃ© elegimos **DistilBERT** en vez de BERT completo?

**7.4** Â¿El modelo que usaremos estÃ¡ pre-entrenado o entrenaremos desde cero?

---

### Plan de ImplementaciÃ³n

**7.5** Lista los 5 componentes principales que implementaremos:
   1. ?
   2. ?
   3. ?
   4. ?
   5. ?

**7.6** Â¿QuÃ© herramientas/librerÃ­as usaremos para:
   - Cargar el modelo?
   - Implementar SHAP?
   - Implementar LIME?
   - Visualizar resultados?

**7.7** Â¿QuÃ© visualizaciones queremos crear al final del proyecto?

---

### Preguntas de InvestigaciÃ³n

**7.8** Â¿QuÃ© queremos aprender de este proyecto? Lista 3 preguntas de investigaciÃ³n.

**7.9** Â¿QuÃ© considerarÃ­as un "Ã©xito" al finalizar este mÃ³dulo?

**7.10** Â¿QuÃ© habilidades nuevas habrÃ¡s adquirido despuÃ©s de completar este mÃ³dulo?

---

## âœ… Checklist de Completitud

Marca cuando hayas respondido cada bloque:

- [ ] BLOQUE 1: Interpretabilidad en ML (11 preguntas)
- [ ] BLOQUE 2: Arquitectura Transformers (16 preguntas)
- [ ] BLOQUE 3: SHAP (15 preguntas)
- [ ] BLOQUE 4: LIME (15 preguntas)
- [ ] BLOQUE 5: SHAP vs LIME (7 preguntas)
- [ ] BLOQUE 6: MÃ©tricas de ValidaciÃ³n (7 preguntas)
- [ ] BLOQUE 7: AplicaciÃ³n al Proyecto (10 preguntas)

**Total: 81 preguntas**

---

## ğŸ“š Recursos Recomendados

Para responder estas preguntas, consulta:

1. **Papers clave:**
   - Ribeiro et al. (2016) - "Why Should I Trust You?" (LIME)
   - Lundberg & Lee (2017) - "A Unified Approach to Interpreting Model Predictions" (SHAP)
   - Vaswani et al. (2017) - "Attention is All You Need" (Transformers)

2. **DocumentaciÃ³n:**
   - HuggingFace Transformers: https://huggingface.co/docs/transformers
   - SHAP documentation: https://shap.readthedocs.io
   - LIME documentation: https://lime-ml.readthedocs.io

3. **Tutoriales:**
   - Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/
   - SHAP examples: https://github.com/slundberg/shap

---

## ğŸ¯ Siguiente Paso

Una vez respondidas todas las preguntas:
1. Crea un documento `docs/01_fundamentos_respuestas.md` con tus respuestas
2. Revisa tus respuestas con los recursos mencionados
3. Identifica conceptos que necesitas reforzar
4. Â¡Avanza a la Semana 2 (implementaciÃ³n toy)!