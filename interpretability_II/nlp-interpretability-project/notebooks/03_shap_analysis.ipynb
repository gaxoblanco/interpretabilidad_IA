{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cc055c",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# NOTEBOOK: 03_shap_analysis.ipynb\n",
    "# AN√ÅLISIS DE INTERPRETABILIDAD CON SHAP\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 1 - CONFIGURACI√ìN INICIAL Y IMPORTS\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "# üìä An√°lisis de Interpretabilidad con SHAP\n",
    "## Explicando predicciones de DistilBERT en an√°lisis de sentimientos\n",
    "\n",
    "Este notebook implementa SHAP (SHapley Additive exPlanations) para interpretar\n",
    "las decisiones del modelo DistilBERT en la tarea de clasificaci√≥n de sentimientos.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports b√°sicos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Imports de ML\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "\n",
    "print(\"üîß CONFIGURACI√ìN DEL ENTORNO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üì¶ Versiones de librer√≠as:\")\n",
    "print(f\"  ‚Ä¢ PyTorch: {torch.__version__}\")\n",
    "print(f\"  ‚Ä¢ SHAP: {shap.__version__}\")\n",
    "print(f\"  ‚Ä¢ Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 2 - CARGAR MODELO Y DATOS PREPARADOS\n",
    "# ============================================================\n",
    "print(\"\\nüìö CARGANDO MODELO Y DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuraci√≥n\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "CACHE_DIR = \"./models_cache\"\n",
    "DATA_DIR = \"./explainability_analysis\"\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "print(\"\\n1Ô∏è‚É£ Cargando modelo DistilBERT...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "model.eval()\n",
    "\n",
    "# Mover a GPU si est√° disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"‚úÖ Modelo cargado en {device}\")\n",
    "\n",
    "# Cargar casos seleccionados (notebook 02_model_evaluation)\n",
    "print(\"\\n2Ô∏è‚É£ Cargando casos seleccionados...\")\n",
    "try:\n",
    "    # Buscar el archivo m√°s reciente\n",
    "    import glob\n",
    "    import os\n",
    "    \n",
    "    priority_files = glob.glob(f\"{DATA_DIR}/priority_cases_*.json\")\n",
    "    if priority_files:\n",
    "        latest_file = max(priority_files, key=os.path.getctime)\n",
    "        with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "            priority_cases = json.load(f)\n",
    "        print(f\"‚úÖ Cargados {len(priority_cases)} casos prioritarios desde {latest_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando casos: {e}\")\n",
    "    priority_cases = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f112dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 3 - Verificaci√≥n del modelo en crudo\n",
    "# ============================================================\n",
    "print(\"\\nüîß PREPARANDO FUNCI√ìN DE PREDICCI√ìN PARA SHAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def predict_for_shap(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Validamos manualmente el sesgo de predicci√≥n del modelo.\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista de textos a predecir\n",
    "        \n",
    "    Returns:\n",
    "        Array de probabilidades [n_samples, n_classes]\n",
    "    \"\"\"\n",
    "    # Tokenizar\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Predicci√≥n\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# Verificar funci√≥n\n",
    "test_text = [\"This is a test sentence.\"]\n",
    "test_proba = predict_for_shap(test_text)\n",
    "print(f\"‚úÖ Funci√≥n de predicci√≥n verificada\")\n",
    "print(f\"   ‚Ä¢ Input: '{test_text[0]}'\")\n",
    "print(f\"   ‚Ä¢ Output shape: {test_proba.shape}\")\n",
    "print(f\"   ‚Ä¢ Probabilidades: Neg={test_proba[0][0]:.3f}, Pos={test_proba[0][1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85032664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 4 - INICIALIZAR SHAP EXPLAINER\n",
    "# ============================================================\n",
    "\n",
    "# clear warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*xformers.*')\n",
    "\n",
    "print(\"\\nüéØ INICIALIZANDO SHAP EXPLAINER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# M√©todo 1: Transformers Pipeline (m√°s eficiente para transformers)\n",
    "print(\"\\nüìå Configurando SHAP con Transformers Pipeline...\")\n",
    "\n",
    "# Crear pipeline de transformers\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Inicializar SHAP Explainer\n",
    "print(\"Inicializando explainer (puede tomar unos segundos)...\")\n",
    "explainer = shap.Explainer(classifier)\n",
    "print(\"‚úÖ SHAP Explainer inicializado\")\n",
    "\n",
    "# Informaci√≥n del explainer\n",
    "print(f\"\\nüìä Configuraci√≥n del Explainer:\")\n",
    "print(f\"  ‚Ä¢ Tipo: {type(explainer).__name__}\")\n",
    "print(f\"  ‚Ä¢ Modelo base: DistilBERT\")\n",
    "print(f\"  ‚Ä¢ Clases: ['NEGATIVE', 'POSITIVE']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeebaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 5 - AN√ÅLISIS DE UN CASO SIMPLE\n",
    "# ============================================================\n",
    "print(\"\\nüîç AN√ÅLISIS DE CASO SIMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Texto de ejemplo\n",
    "simple_text = \"This movie is absolutely terrible and boring.\"\n",
    "print(f\"\\nüìù Texto: '{simple_text}'\")\n",
    "\n",
    "# Obtener predicci√≥n original\n",
    "pred = classifier(simple_text)[0] # -> predicci√≥n usando el ejemplo para validar que obtenemos lo esperado (negative)\n",
    "print(f\"üéØ Predicci√≥n original: {pred['label']} (confianza: {pred['score']:.3f})\")\n",
    "\n",
    "# Calcular valores SHAP\n",
    "print(\"\\n‚è≥ Calculando valores SHAP (puede tomar 10-30 segundos)...\")\n",
    "start_time = time.time()\n",
    "shap_values = explainer([simple_text])\n",
    "calc_time = time.time() - start_time\n",
    "print(f\"‚úÖ C√°lculo completado en {calc_time:.2f} segundos\")\n",
    "\n",
    "# Mostrar informaci√≥n de shap_values\n",
    "print(f\"\\nüìä Estructura de valores SHAP:\")\n",
    "print(f\"  ‚Ä¢ Tipo: {type(shap_values)}\")\n",
    "print(f\"  ‚Ä¢ Shape: {shap_values.shape}\")\n",
    "print(f\"  ‚Ä¢ Clases: {shap_values.output_names}\")\n",
    "\n",
    "# Mostrar tokens y sus valores\n",
    "print(f\"\\nüìù Tokens analizados y sus contribuciones:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Extraer tokens y valores\n",
    "tokens = shap_values[0].data\n",
    "values_neg = shap_values[0].values[:, 0]  # Impacto en NEGATIVE\n",
    "values_pos = shap_values[0].values[:, 1]  # Impacto en POSITIVE\n",
    "\n",
    "# # Mostrar cada token con su valor\n",
    "# for i, token in enumerate(tokens):\n",
    "#     print(f\"  [{i}] '{token}':\")\n",
    "#     print(f\"      ‚Üí NEGATIVE: {values_neg[i]:+.4f}\")\n",
    "#     print(f\"      ‚Üí POSITIVE: {values_pos[i]:+.4f}\")\n",
    "\n",
    "# Identificar palabras m√°s influyentes\n",
    "print(f\"\\nüéØ Palabras m√°s influyentes para POSITIVE:\")\n",
    "top_pos_idx = np.argsort(values_pos)[-3:][::-1]\n",
    "for idx in top_pos_idx:\n",
    "    print(f\"  ‚Ä¢ '{tokens[idx]}': {values_pos[idx]:+.4f}\")\n",
    "    \n",
    "print(f\"\\nüéØ Palabras m√°s influyentes para NEGATIVE:\")\n",
    "top_neg_idx = np.argsort(values_neg)[-3:][::-1]\n",
    "for idx in top_neg_idx:\n",
    "    print(f\"  ‚Ä¢ '{tokens[idx]}': {values_neg[idx]:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd46202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 6 - VISUALIZACI√ìN B√ÅSICA DE SHAP -> simple_text\n",
    "# ============================================================\n",
    "print(\"\\nüìä VISUALIZACIONES B√ÅSICAS DE SHAP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìå Analizando: '{simple_text}'\")\n",
    "print(f\"   Predicci√≥n: {pred['label']} ({pred['score']:.1%})\")\n",
    "\n",
    "# Visualizaci√≥n 1: Text plot (muestra importancia en el texto)\n",
    "print(\"\\n1Ô∏è‚É£ Text Plot - Importancia de palabras en contexto:\")\n",
    "shap.plots.text(shap_values[0])\n",
    "\n",
    "# Visualizaci√≥n 2: Bar plot (top palabras m√°s importantes)\n",
    "print(\"\\n2Ô∏è‚É£ Bar Plot - Top palabras m√°s influyentes:\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Para clasificaci√≥n binaria, usar la clase objetivo (POSITIVE = √≠ndice 1)\n",
    "shap.plots.bar(shap_values[0, :, 1], max_display=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Visualizaci√≥n 3: Waterfall (contribuci√≥n acumulativa)\n",
    "print(\"\\n3Ô∏è‚É£ Waterfall Plot - Contribuci√≥n acumulativa:\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.plots.waterfall(shap_values[0, :, 1], max_display=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 7 - AN√ÅLISIS DE M√öLTIPLES CASOS\n",
    "# ============================================================\n",
    "print(\"\\nüîç AN√ÅLISIS DE M√öLTIPLES CASOS PRIORITARIOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Seleccionar subset de casos para an√°lisis\n",
    "n_cases = min(5, len(priority_cases))\n",
    "cases_to_analyze = priority_cases[:n_cases]\n",
    "\n",
    "print(f\"\\nüìã Analizando {n_cases} casos prioritarios...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Almacenar resultados\n",
    "shap_results = []\n",
    "\n",
    "for i, case in enumerate(cases_to_analyze, 1):\n",
    "    text = case['text']\n",
    "    true_label = case['true_label']\n",
    "    \n",
    "    print(f\"\\nCaso {i}/{n_cases}:\")\n",
    "    print(f\"  Texto: '{text[:80]}...'\")\n",
    "    print(f\"  Categor√≠a: {case['category']}\")\n",
    "    print(f\"  Label real: {true_label}\")\n",
    "    \n",
    "    # Calcular SHAP\n",
    "    print(\"  ‚è≥ Calculando SHAP...\")\n",
    "    start = time.time()\n",
    "    shap_vals = explainer([text])\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Guardar resultados\n",
    "    result = {\n",
    "        'text': text,\n",
    "        'true_label': true_label,\n",
    "        'predicted_label': case.get('predicted_label', -1),\n",
    "        'category': case['category'],\n",
    "        'shap_values': shap_vals,\n",
    "        'computation_time': elapsed\n",
    "    }\n",
    "    shap_results.append(result)\n",
    "    \n",
    "    print(f\"  ‚úÖ Completado en {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\n‚úÖ An√°lisis completado para {len(shap_results)} casos\")\n",
    "avg_time = np.mean([r['computation_time'] for r in shap_results])\n",
    "print(f\"‚è±Ô∏è Tiempo promedio por caso: {avg_time:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celta 7.1 - EXTRACCI√ìN DE PALABRAS INFLUYENTES\n",
    "# ============================================================\n",
    "\n",
    "# Obtener top palabras influyentes por caso para cargar en el grafico\n",
    "for i, result in enumerate(shap_results, 1):\n",
    "    text = result['text']\n",
    "    shap_vals = result['shap_values']\n",
    "    \n",
    "    tokens = shap_vals[0].data\n",
    "    values_neg = shap_vals[0].values[:, 0]\n",
    "    values_pos = shap_vals[0].values[:, 1]\n",
    "    \n",
    "    # Top palabras POSITIVE\n",
    "    top_pos_idx = np.argsort(values_pos)[-3:][::-1]\n",
    "    top_pos_words = [(tokens[idx], values_pos[idx]) for idx in top_pos_idx]\n",
    "    \n",
    "    # Top palabras NEGATIVE\n",
    "    top_neg_idx = np.argsort(values_neg)[-3:][::-1]\n",
    "    top_neg_words = [(tokens[idx], values_neg[idx]) for idx in top_neg_idx]\n",
    "    \n",
    "    result['top_positive_words'] = top_pos_words\n",
    "    result['top_negative_words'] = top_neg_words\n",
    "\n",
    "print(\"\\n‚úÖ Extracci√≥n de palabras influyentes completada\")\n",
    "# muestro una comparativa en consola\n",
    "for i, result in enumerate(shap_results, 1):\n",
    "    print(f\"\\nüìã Caso {i} - Palabras m√°s influyentes:\")\n",
    "    print(f\"  Texto: '{result['text'][:80]}...'\")\n",
    "    print(f\"  Predicci√≥n: {result['predicted_label']}\")\n",
    "    print(f\"  Top POSITIVE:\")\n",
    "    for word, val in result['top_positive_words']:\n",
    "        print(f\"    ‚Ä¢ '{word}': {val:+.4f}\")\n",
    "    print(f\"  Top NEGATIVE:\")\n",
    "    for word, val in result['top_negative_words']:\n",
    "        print(f\"    ‚Ä¢ '{word}': {val:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c369b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celta 7.2 - VISUALIZACI√ìN DE PALABRAS INFLUYENTES\n",
    "# ============================================================\n",
    "\n",
    "# Grafico para identificar palabras m√°s influyentes POSITIVE y NEGATIVE\n",
    "for i, result in enumerate(shap_results, 1):\n",
    "    text = result['text']\n",
    "    shap_vals = result['shap_values']\n",
    "    pred_label = result['predicted_label']\n",
    "    \n",
    "    print(f\"\\nüìä Caso {i} - Visualizaci√≥n de palabras influyentes:\")\n",
    "    print(f\"  Texto: '{text[:80]}...'\")\n",
    "    print(f\"  Predicci√≥n: {pred_label}\")\n",
    "    \n",
    "    # Visualizaci√≥n Bar Plot para POSITIVE\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.bar(shap_vals[0, :, 1], max_display=10)\n",
    "    \n",
    "    # Visualizaci√≥n Bar Plot para NEGATIVE\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.bar(shap_vals[0, :, 0], max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 8 - COMPARACI√ìN DE EXPLICACIONES\n",
    "# ============================================================\n",
    "print(\"\\nüìä COMPARACI√ìN DE EXPLICACIONES ENTRE CASOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear figura comparativa\n",
    "fig, axes = plt.subplots(len(shap_results), 1, figsize=(12, 4*len(shap_results)))\n",
    "if len(shap_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (result, ax) in enumerate(zip(shap_results, axes)):\n",
    "    # Extraer valores SHAP para clase positiva\n",
    "    shap_val = result['shap_values'][0]\n",
    "    \n",
    "    # Obtener tokens y valores\n",
    "    tokens = shap_val.data\n",
    "    values = shap_val.values[:, 1]  # Clase POSITIVE\n",
    "    \n",
    "    # Seleccionar top 10 tokens m√°s importantes (por valor absoluto)\n",
    "    top_indices = np.argsort(np.abs(values))[-10:]\n",
    "    top_tokens = [tokens[i] for i in top_indices]\n",
    "    top_values = [values[i] for i in top_indices]\n",
    "    \n",
    "    # Crear bar plot\n",
    "    colors = ['green' if v > 0 else 'red' for v in top_values]\n",
    "    ax.barh(range(len(top_tokens)), top_values, color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(top_tokens)))\n",
    "    ax.set_yticklabels(top_tokens)\n",
    "    ax.set_xlabel('SHAP Value (impacto en predicci√≥n POSITIVE)')\n",
    "    ax.set_title(f\"Caso {idx+1}: {result['category']}\", fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Comparaci√≥n de Palabras M√°s Influyentes por Caso\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 9 - AN√ÅLISIS DE ESTABILIDAD\n",
    "# ============================================================\n",
    "print(\"\\nüî¨ AN√ÅLISIS DE ESTABILIDAD DE SHAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Usar un caso real ambiguo de tu dataset \"index\": 301, confianza ~51.6%\n",
    "stability_text = priority_cases[301]['text'] \n",
    "print(f\"\\nüìù Texto ambiguo (confianza: 51.6%):\")\n",
    "print(f\"'{stability_text[:100]}...'\")\n",
    "\n",
    "# Predicci√≥n inicial\n",
    "pred = classifier(stability_text)[0]\n",
    "print(f\"\\nüéØ Predicci√≥n: {pred['label']} (confianza: {pred['score']:.1%})\")\n",
    "print(\"   ‚ö†Ô∏è Caso fronterizo - ideal para probar estabilidad\")\n",
    "\n",
    "# Realizar m√∫ltiples explicaciones del mismo texto\n",
    "n_runs = 3\n",
    "stability_results = []\n",
    "print(f\"\\nüîÑ Ejecutando {n_runs} explicaciones del mismo texto...\")\n",
    "for run in range(n_runs):\n",
    "    print(f\"  Run {run+1}/{n_runs}...\", end='')\n",
    "    shap_vals = explainer([stability_text])\n",
    "    stability_results.append(shap_vals[0].values[:, 1])  # Clase POSITIVE\n",
    "    print(\" ‚úì\")\n",
    "\n",
    "# Calcular estad√≠sticas\n",
    "tokens = shap_vals[0].data\n",
    "variances = np.var([result for result in stability_results], axis=0)\n",
    "mean_values = np.mean([result for result in stability_results], axis=0)\n",
    "std_devs = np.std([result for result in stability_results], axis=0)\n",
    "\n",
    "# Verificar estabilidad global\n",
    "max_variance = np.max(variances)\n",
    "is_deterministic = max_variance < 1e-10\n",
    "\n",
    "print(f\"\\nüìä RESULTADOS DE ESTABILIDAD:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ VEREDICTO: {'DETERMIN√çSTICO' if is_deterministic else 'ESTOC√ÅSTICO'}\")\n",
    "print(f\"   ‚Ä¢ Varianza m√°xima: {max_variance:.2e}\")\n",
    "print(f\"   ‚Ä¢ {'Todos los runs dan valores id√©nticos' if is_deterministic else 'Los valores var√≠an entre runs'}\")\n",
    "\n",
    "# Palabras m√°s influyentes (por valor absoluto)\n",
    "abs_means = np.abs(mean_values)\n",
    "top_indices = np.argsort(abs_means)[-5:][::-1]\n",
    "\n",
    "print(f\"\\nüéØ TOP 5 PALABRAS M√ÅS INFLUYENTES:\")\n",
    "print(f\"\\n Si todos los valores son iguales entre runs, œÉ=0 para cada palabra\")\n",
    "print(f\"\\n Quiere decir que SHAP es reproducible y confiable\")\n",
    "print(\"-\"*40)\n",
    "for idx in top_indices:\n",
    "    token = tokens[idx] if tokens[idx].strip() else '[SPACE]'\n",
    "    impact = \"‚Üí POSITIVE\" if mean_values[idx] > 0 else \"‚Üí NEGATIVE\"\n",
    "    print(f\"  {token:20s}: Œº={mean_values[idx]:+.4f} {impact}\")\n",
    "    if not is_deterministic:\n",
    "        print(f\"  {'':20s}  œÉ={std_devs[idx]:.6f}\")\n",
    "\n",
    "# Mostrar comparaci√≥n entre runs si hay varianza\n",
    "if not is_deterministic:\n",
    "    print(f\"\\n‚ö†Ô∏è VARIABILIDAD DETECTADA:\")\n",
    "    print(f\"   ‚Ä¢ Algunas palabras tienen valores SHAP que var√≠an entre runs\")\n",
    "    varying_tokens = np.where(variances > 1e-10)[0]\n",
    "    for idx in varying_tokens[:3]:\n",
    "        print(f\"\\n  Token: '{tokens[idx]}'\")\n",
    "        for run_idx, result in enumerate(stability_results):\n",
    "            print(f\"    Run {run_idx+1}: {result[idx]:.6f}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ CONSISTENCIA PERFECTA:\")\n",
    "    print(f\"   ‚Ä¢ Cada palabra tiene exactamente el mismo valor SHAP en todos los runs\")\n",
    "    print(f\"   ‚Ä¢ Esto confirma que SHAP es reproducible y confiable\")\n",
    "\n",
    "# Interpretaci√≥n del caso\n",
    "print(f\"\\nüí° INTERPRETACI√ìN DEL CASO AMBIGUO:\")\n",
    "print(\"-\"*40)\n",
    "positive_words = sum(1 for v in mean_values if v > 0.01)\n",
    "negative_words = sum(1 for v in mean_values if v < -0.01)\n",
    "print(f\"  ‚Ä¢ Palabras positivas: {positive_words}\")\n",
    "print(f\"  ‚Ä¢ Palabras negativas: {negative_words}\")\n",
    "print(f\"  ‚Ä¢ Balance: {'Inclinado a NEGATIVE' if negative_words > positive_words else 'Inclinado a POSITIVE'}\")\n",
    "print(f\"  ‚Ä¢ Por eso la confianza es baja (~51%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d19227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 10 - EXPORTAR RESULTADOS\n",
    "# ============================================================\n",
    "print(\"\\nüíæ EXPORTANDO RESULTADOS DE SHAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Preparar datos para exportaci√≥n\n",
    "export_data = {\n",
    "    'metadata': {\n",
    "        'model': MODEL_NAME,\n",
    "        'method': 'SHAP',\n",
    "        'n_cases_analyzed': len(shap_results),\n",
    "        'avg_computation_time': avg_time,\n",
    "        'timestamp': pd.Timestamp.now().isoformat()\n",
    "    },\n",
    "    'cases': []\n",
    "}\n",
    "\n",
    "for result in shap_results:\n",
    "    case_data = {\n",
    "        'text': result['text'],\n",
    "        'true_label': result['true_label'],\n",
    "        'category': result['category'],\n",
    "        'computation_time': result['computation_time'],\n",
    "        'top_positive_words': [],\n",
    "        'top_negative_words': []\n",
    "    }\n",
    "    \n",
    "    # Extraer palabras m√°s importantes\n",
    "    shap_val = result['shap_values'][0]\n",
    "    tokens = shap_val.data\n",
    "    values = shap_val.values[:, 1]  # Clase POSITIVE\n",
    "    \n",
    "    # Top palabras positivas\n",
    "    pos_indices = np.where(values > 0)[0]\n",
    "    if len(pos_indices) > 0:\n",
    "        top_pos = pos_indices[np.argsort(values[pos_indices])[-5:]]\n",
    "        case_data['top_positive_words'] = [\n",
    "            {'token': tokens[i], 'value': float(values[i])}\n",
    "            for i in top_pos\n",
    "        ]\n",
    "    \n",
    "    # Top palabras negativas\n",
    "    neg_indices = np.where(values < 0)[0]\n",
    "    if len(neg_indices) > 0:\n",
    "        top_neg = neg_indices[np.argsort(values[neg_indices])[:5]]\n",
    "        case_data['top_negative_words'] = [\n",
    "            {'token': tokens[i], 'value': float(values[i])}\n",
    "            for i in top_neg\n",
    "        ]\n",
    "    \n",
    "    export_data['cases'].append(case_data)\n",
    "\n",
    "# Guardar JSON\n",
    "output_path = f\"{DATA_DIR}/shap_results_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Resultados guardados en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf83760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 11 - RESUMEN Y CONCLUSIONES\n",
    "# ============================================================\n",
    "print(\"\\nüìä RESUMEN DEL AN√ÅLISIS CON SHAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úÖ AN√ÅLISIS COMPLETADO:\")\n",
    "print(f\"  ‚Ä¢ Casos analizados: {len(shap_results)}\")\n",
    "print(f\"  ‚Ä¢ Tiempo promedio: {avg_time:.2f} segundos/caso\")\n",
    "print(f\"  ‚Ä¢ Tiempo total: {sum(r['computation_time'] for r in shap_results):.2f} segundos\")\n",
    "\n",
    "print(\"\\nüîç OBSERVACIONES CLAVE:\")\n",
    "print(\"  1. SHAP proporciona valores consistentes y matem√°ticamente fundamentados\")\n",
    "print(\"  2. El tiempo de c√≥mputo es significativo (~10-30s por texto)\")\n",
    "print(\"  3. Las explicaciones son determin√≠sticas (misma entrada = misma salida)\")\n",
    "print(\"  4. Permite identificar claramente palabras positivas vs negativas\")\n",
    "\n",
    "print(\"\\nüí° VENTAJAS DE SHAP:\")\n",
    "print(\"  ‚úì Base te√≥rica s√≥lida (teor√≠a de juegos)\")\n",
    "print(\"  ‚úì Garant√≠as matem√°ticas (aditividad, consistencia)\")\n",
    "print(\"  ‚úì Explicaciones globales y locales\")\n",
    "print(\"  ‚úì Resultados reproducibles\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è LIMITACIONES DE SHAP:\")\n",
    "print(\"  ‚úó Computacionalmente costoso\")\n",
    "print(\"  ‚úó Requiere acceso al modelo completo\")\n",
    "print(\"  ‚úó Puede ser dif√≠cil de interpretar para usuarios no t√©cnicos\")\n",
    "\n",
    "print(\"\\nüöÄ PR√ìXIMOS PASOS:\")\n",
    "print(\"  ‚Üí Comparar con resultados de LIME\")\n",
    "print(\"  ‚Üí Analizar casos donde SHAP y LIME difieren\")\n",
    "print(\"  ‚Üí Evaluar trade-off velocidad vs precisi√≥n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Notebook de an√°lisis SHAP completado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe15a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP Project)",
   "language": "python",
   "name": "nlp-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
