{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c37d5b",
   "metadata": {},
   "source": [
    "# üìä Evaluaci√≥n del Modelo Base - DistilBERT\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos del Notebook\n",
    "\n",
    "1. ‚úÖ **Setup completo**: Cargar configuraci√≥n, modelo y datos\n",
    "2. ‚úÖ **Evaluaci√≥n cuantitativa**: M√©tricas de clasificaci√≥n (Accuracy, F1, etc.)\n",
    "3. ‚úÖ **An√°lisis cualitativo**: Ejemplos de predicciones correctas e incorrectas\n",
    "4. ‚úÖ **Distribuci√≥n de confianza**: Analizar probabilidades del modelo\n",
    "5. ‚úÖ **Selecci√≥n de casos**: Identificar ejemplos interesantes para SHAP/LIME\n",
    "\n",
    "---\n",
    "\n",
    "**Proyecto:** Interpretabilidad en NLP - M√≥dulo II \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python ejecutable:\", sys.executable)\n",
    "print(\"Ruta de Python:\", sys.prefix)\n",
    "\n",
    "# Verificar que torch est√© disponible\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} encontrado\")\n",
    "    print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62920c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1 Imports b√°sicos\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Agregar src al path para poder importar nuestros m√≥dulos\n",
    "sys.path.append('..')\n",
    "\n",
    "print(\"‚úÖ Imports b√°sicos completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2 Imports del proyecto\n",
    "import pyarrow as pa\n",
    "print(f\"PyArrow version: {pa.__version__}\")\n",
    "\n",
    "import datasets\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "# Imports del proyecto\n",
    "from src.config import setup_project, print_config_summary\n",
    "from src.models import ModelLoader\n",
    "from src.utils import DataLoader\n",
    "\n",
    "print(\"‚úÖ M√≥dulos del proyecto importados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3 - Imports para an√°lisis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configurar estilo de gr√°ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as de an√°lisis importadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ab95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4 - Setup del proyecto\n",
    "config = setup_project()\n",
    "\n",
    "# Imprimir resumen\n",
    "print_config_summary(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c787ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5 - Cargar modelo\n",
    "print(\"üöÄ Cargando modelo DistilBERT...\\n\")\n",
    "model = ModelLoader(config)\n",
    "\n",
    "# Informaci√≥n del modelo\n",
    "print(\"\\nüìä Informaci√≥n del Modelo:\")\n",
    "info = model.get_model_info()\n",
    "for key, value in info.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5.1 Diagn√≥stico del modelo - Entrada/Salida\n",
    "from src.utils.data_model import verificar_entrada_salida_modelo\n",
    "print(\"üî¨ DIAGN√ìSTICO COMPLETO DE ENTRADA/SALIDA DEL MODELO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "info_io = verificar_entrada_salida_modelo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6 - Cargar dataset // devemos volver a usar el servicio de config\n",
    "print(\"üì• CARGANDO DATASET SST-2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar dataset completo sin procesamiento\n",
    "print(\"\\nCargando dataset SST-2 desde HuggingFace...\")\n",
    "dataset_raw = load_dataset(\"sst2\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset cargado exitosamente\")\n",
    "print(f\"Splits disponibles: {list(dataset_raw.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3efe9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Celda 6 - Cargar dataset\n",
    "# print(\"üì• Cargando dataset IMDb...\\n\")\n",
    "# dataset_raw = DataLoader(config)\n",
    "\n",
    "# # Informaci√≥n del dataset\n",
    "# print(\"\\nüìä Informaci√≥n del Dataset:\")\n",
    "# dataset_info = dataset_raw.get_dataset_info()\n",
    "# for key, value in dataset_info.items():\n",
    "#     if isinstance(value, dict):\n",
    "#         print(f\"  ‚Ä¢ {key}:\")\n",
    "#         for k, v in value.items():\n",
    "#             print(f\"      - {k}: {v}\")\n",
    "#     else:\n",
    "#         print(f\"  ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6.1 - An√°lisis exploratorio del dataset SST-2\n",
    "print(\"\\nüìä AN√ÅLISIS EXPLORATORIO DEL DATASET SST-2\")\n",
    "print(\"=\"*60)\n",
    "from src.utils.data_view import view_distributions\n",
    "view = view_distributions(dataset_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_model import diagnosticar_problema_completo\n",
    "diagnostico = diagnosticar_problema_completo(model, dataset_raw['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Celda 6.2 - Visualizaci√≥n de Distribuciones\n",
    "# print(\"\\nüìà VISUALIZACI√ìN DE DISTRIBUCIONES\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# # Configurar estilo\n",
    "# plt.style.use('seaborn-v0_8-darkgrid')\n",
    "# sns.set_palette(\"husl\")\n",
    "\n",
    "# # Crear figura con subplots\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "# fig.suptitle('An√°lisis del Dataset SST-2', fontsize=16, fontweight='bold')\n",
    "\n",
    "# # 1. Distribuci√≥n de longitudes - Train\n",
    "# ax1 = axes[0, 0]\n",
    "# ax1.hist(train_lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "# ax1.axvline(np.mean(train_lengths), color='red', linestyle='--', label=f'Media: {np.mean(train_lengths):.1f}')\n",
    "# ax1.set_xlabel('N√∫mero de palabras')\n",
    "# ax1.set_ylabel('Frecuencia')\n",
    "# ax1.set_title('Distribuci√≥n de Longitudes - Train')\n",
    "# ax1.legend()\n",
    "\n",
    "# # 2. Distribuci√≥n de longitudes - Validation\n",
    "# ax2 = axes[0, 1]\n",
    "# ax2.hist(val_lengths, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "# ax2.axvline(np.mean(val_lengths), color='red', linestyle='--', label=f'Media: {np.mean(val_lengths):.1f}')\n",
    "# ax2.set_xlabel('N√∫mero de palabras')\n",
    "# ax2.set_ylabel('Frecuencia')\n",
    "# ax2.set_title('Distribuci√≥n de Longitudes - Validation')\n",
    "# ax2.legend()\n",
    "\n",
    "# # 3. Balance de clases - Train\n",
    "# ax3 = axes[0, 2]\n",
    "# ax3.bar(['Negativo', 'Positivo'], [train_neg, train_pos], color=['#FF6B6B', '#4ECDC4'])\n",
    "# ax3.set_ylabel('Cantidad')\n",
    "# ax3.set_title('Balance de Clases - Train')\n",
    "# for i, v in enumerate([train_neg, train_pos]):\n",
    "#     ax3.text(i, v + 500, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# # 4. Balance de clases - Validation\n",
    "# ax4 = axes[1, 0]\n",
    "# ax4.bar(['Negativo', 'Positivo'], [val_neg, val_pos], color=['#FF6B6B', '#4ECDC4'])\n",
    "# ax4.set_ylabel('Cantidad')\n",
    "# ax4.set_title('Balance de Clases - Validation')\n",
    "# for i, v in enumerate([val_neg, val_pos]):\n",
    "#     ax4.text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# # 5. Boxplot de longitudes\n",
    "# ax5 = axes[1, 1]\n",
    "# ax5.boxplot([train_lengths, val_lengths], labels=['Train', 'Validation'])\n",
    "# ax5.set_ylabel('N√∫mero de palabras')\n",
    "# ax5.set_title('Comparaci√≥n de Longitudes')\n",
    "\n",
    "# # 6. Pie chart de balance general\n",
    "# ax6 = axes[1, 2]\n",
    "# total_neg = train_neg + val_neg\n",
    "# total_pos = train_pos + val_pos\n",
    "# ax6.pie([total_neg, total_pos], labels=['Negativo', 'Positivo'], \n",
    "#         autopct='%1.1f%%', colors=['#FF6B6B', '#4ECDC4'])\n",
    "# ax6.set_title('Balance Global del Dataset')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(\"\\n‚úÖ Visualizaciones completadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 6.3 - VALIDACI√ìN Y CORRECCI√ìN AUTOM√ÅTICA DE LABELS\n",
    "# ============================================================\n",
    "from src.utils.data_format import validate_and_fix_labels\n",
    "\n",
    "# Aplicar la validaci√≥n y correcci√≥n al dataset cargado\n",
    "dataset_raw = validate_and_fix_labels(dataset_raw)\n",
    "\n",
    "print(\"\\nüìã Dataset listo para usar con labels en formato correcto (0/1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 6.4 - PREPARACI√ìN Y DIVISI√ìN DE DATOS\n",
    "# ============================================================\n",
    "# Esta celda prepara los datos para la evaluaci√≥n del modelo\n",
    "# ============================================================\n",
    "\n",
    "# Importar la funci√≥n de preparaci√≥n\n",
    "from src.utils.data_preparation import preparar_datos_para_evaluacion\n",
    "\n",
    "# Opci√≥n 1: Usar el split de validaci√≥n completo\n",
    "test_texts, test_labels = preparar_datos_para_evaluacion(\n",
    "    dataset_raw, \n",
    "    split_test='validation',  # Puedes cambiar a 'test' si prefieres\n",
    "    sample_size=None,         # None = usar todo el dataset\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Opci√≥n 2: Si quieres una muestra m√°s peque√±a para pruebas r√°pidas\n",
    "# test_texts, test_labels = preparar_datos_para_evaluacion(\n",
    "#     dataset_raw, \n",
    "#     split_test='validation',\n",
    "#     sample_size=100,  # Solo 100 ejemplos para pruebas r√°pidas\n",
    "#     random_seed=42\n",
    "# )\n",
    "\n",
    "print(f\"\\n‚úÖ Datos listos para evaluaci√≥n:\")\n",
    "print(f\"  ‚Ä¢ Total de textos: {len(test_texts)}\")\n",
    "print(f\"  ‚Ä¢ Total de etiquetas: {len(test_labels)}\")\n",
    "print(f\"\\nüìù Ejemplos de los datos preparados:\")\n",
    "for i in range(min(3, len(test_texts))):\n",
    "    print(f\"\\n  Ejemplo {i+1}:\")\n",
    "    print(f\"  ‚Ä¢ Texto: '{test_texts[i][:60]}...'\")\n",
    "    print(f\"  ‚Ä¢ Etiqueta: {test_labels[i]} ({'Positive' if test_labels[i]==1 else 'Negative'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bdbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 7 - REALIZAR PREDICCIONES\n",
    "# ============================================================\n",
    "# Esta celda genera las predicciones usando el modelo cargado\n",
    "# y aplica la correcci√≥n necesaria para el campo 'prediction'\n",
    "# ============================================================\n",
    "\n",
    "from src.utils.data_model import corregir_predicciones_modelo\n",
    "import time\n",
    "\n",
    "print(\"üöÄ GENERACI√ìN DE PREDICCIONES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 1: Realizar predicciones\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£ Ejecutando predicciones en el conjunto de prueba...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Medir tiempo de inferencia\n",
    "start_time = time.time()\n",
    "\n",
    "# Hacer predicciones en batch (m√°s eficiente)\n",
    "print(f\"  ‚Ä¢ Procesando {len(test_texts)} textos...\")\n",
    "predictions_raw = model.predict_batch(test_texts)\n",
    "\n",
    "# Calcular tiempo transcurrido\n",
    "elapsed_time = time.time() - start_time\n",
    "avg_time = elapsed_time / len(test_texts)\n",
    "\n",
    "print(f\"  ‚Ä¢ ‚úÖ Predicciones completadas\")\n",
    "print(f\"  ‚Ä¢ ‚è±Ô∏è Tiempo total: {elapsed_time:.2f} segundos\")\n",
    "print(f\"  ‚Ä¢ ‚è±Ô∏è Tiempo promedio por texto: {avg_time*1000:.2f} ms\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 2: Verificar estructura de predicciones\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£ Verificaci√≥n de estructura de predicciones...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Mostrar ejemplo de predicci√≥n raw\n",
    "print(\"  Ejemplo de predicci√≥n completa:\")\n",
    "ejemplo = predictions_raw[0]\n",
    "for key, value in ejemplo.items():\n",
    "    if key != 'text':  # No mostrar el texto completo\n",
    "        print(f\"    ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 3: Corregir predicciones (usar label_id)\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£ Aplicando correcci√≥n a las predicciones...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Usar la funci√≥n que creamos para corregir\n",
    "predicted_labels = corregir_predicciones_modelo(predictions_raw)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 4: Validaci√≥n de integridad\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£ Validaci√≥n de integridad...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Verificar dimensiones\n",
    "assert len(predicted_labels) == len(test_labels), \"Error: Dimensiones no coinciden\"\n",
    "print(f\"  ‚Ä¢ ‚úÖ Dimensiones correctas: {len(predicted_labels)} predicciones\")\n",
    "\n",
    "# Verificar tipos de datos\n",
    "assert all(isinstance(label, int) for label in predicted_labels[:10]), \"Error: Labels deben ser enteros\"\n",
    "assert all(label in [0, 1] for label in predicted_labels[:10]), \"Error: Labels deben ser 0 o 1\"\n",
    "print(f\"  ‚Ä¢ ‚úÖ Tipos de datos correctos\")\n",
    "\n",
    "# Mostrar distribuci√≥n de predicciones\n",
    "pred_neg = predicted_labels.count(0)\n",
    "pred_pos = predicted_labels.count(1)\n",
    "print(f\"\\nüìä Distribuci√≥n de predicciones:\")\n",
    "print(f\"  ‚Ä¢ Negativos (0): {pred_neg} ({pred_neg/len(predicted_labels)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Positivos (1): {pred_pos} ({pred_pos/len(predicted_labels)*100:.1f}%)\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 5: Guardar predicciones para an√°lisis posterior\n",
    "# --------------------------------------------------------\n",
    "# Guardar las predicciones completas para an√°lisis detallado\n",
    "predictions_data = {\n",
    "    'raw_predictions': predictions_raw,\n",
    "    'corrected_labels': predicted_labels,\n",
    "    'true_labels': test_labels,\n",
    "    'texts': test_texts\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Predicciones generadas y corregidas exitosamente\")\n",
    "print(f\"üì¶ Variables disponibles:\")\n",
    "print(f\"  ‚Ä¢ predicted_labels: Lista de etiquetas predichas (0/1)\")\n",
    "print(f\"  ‚Ä¢ predictions_raw: Predicciones completas con probabilidades\")\n",
    "print(f\"  ‚Ä¢ predictions_data: Diccionario con todos los datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64bf090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 8 - EVALUACI√ìN COMPLETA DEL MODELO\n",
    "# ============================================================\n",
    "# Esta celda calcula todas las m√©tricas de evaluaci√≥n\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä EVALUACI√ìN COMPLETA DEL MODELO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# M√âTRICAS B√ÅSICAS\n",
    "# --------------------------------------------------------\n",
    "print(\"\\nüéØ M√âTRICAS PRINCIPALES:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(f\"  ‚Ä¢ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Precision, Recall, F1 por clase\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    test_labels, predicted_labels, average=None\n",
    ")\n",
    "\n",
    "print(f\"\\n  Clase 0 (Negative):\")\n",
    "print(f\"    ‚Ä¢ Precision: {precision[0]:.4f}\")\n",
    "print(f\"    ‚Ä¢ Recall: {recall[0]:.4f}\")\n",
    "print(f\"    ‚Ä¢ F1-Score: {f1[0]:.4f}\")\n",
    "print(f\"    ‚Ä¢ Support: {support[0]}\")\n",
    "\n",
    "print(f\"\\n  Clase 1 (Positive):\")\n",
    "print(f\"    ‚Ä¢ Precision: {precision[1]:.4f}\")\n",
    "print(f\"    ‚Ä¢ Recall: {recall[1]:.4f}\")\n",
    "print(f\"    ‚Ä¢ F1-Score: {f1[1]:.4f}\")\n",
    "print(f\"    ‚Ä¢ Support: {support[1]}\")\n",
    "\n",
    "# M√©tricas promedio\n",
    "avg_precision = np.mean(precision)\n",
    "avg_recall = np.mean(recall)\n",
    "avg_f1 = np.mean(f1)\n",
    "\n",
    "print(f\"\\n  Promedios (Macro):\")\n",
    "print(f\"    ‚Ä¢ Avg Precision: {avg_precision:.4f}\")\n",
    "print(f\"    ‚Ä¢ Avg Recall: {avg_recall:.4f}\")\n",
    "print(f\"    ‚Ä¢ Avg F1-Score: {avg_f1:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# M√âTRICAS AVANZADAS\n",
    "# --------------------------------------------------------\n",
    "print(\"\\nüî¨ M√âTRICAS AVANZADAS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Matthews Correlation Coefficient (bueno para clases desbalanceadas)\n",
    "mcc = matthews_corrcoef(test_labels, predicted_labels)\n",
    "print(f\"  ‚Ä¢ Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "\n",
    "# ROC-AUC si tenemos probabilidades\n",
    "if predictions_raw and 'probabilities' in predictions_raw[0]:\n",
    "    # Extraer probabilidades de la clase positiva\n",
    "    probs_positive = [pred['probabilities']['POSITIVE'] for pred in predictions_raw]\n",
    "    roc_auc = roc_auc_score(test_labels, probs_positive)\n",
    "    print(f\"  ‚Ä¢ ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CLASSIFICATION REPORT DETALLADO\n",
    "# --------------------------------------------------------\n",
    "print(\"\\nüìã CLASSIFICATION REPORT DETALLADO:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "report = classification_report(\n",
    "    test_labels, \n",
    "    predicted_labels,\n",
    "    target_names=['Negative', 'Positive'],\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CONFUSION MATRIX\n",
    "# --------------------------------------------------------\n",
    "print(\"\\nüî≤ CONFUSION MATRIX:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Mostrar matriz formateada\n",
    "print(\"\\n                 Predicci√≥n\")\n",
    "print(\"                 Neg    Pos\")\n",
    "print(f\"Real Negative: [{tn:4d}] [{fp:4d}]  ‚Üí {tn+fp} total\")\n",
    "print(f\"     Positive: [{fn:4d}] [{tp:4d}]  ‚Üí {fn+tp} total\")\n",
    "print(f\"                ----   ----\")\n",
    "print(f\"               {tn+fn:5d}  {fp+tp:5d}\")\n",
    "\n",
    "# Tasas de error\n",
    "print(f\"\\nüìä An√°lisis de Errores:\")\n",
    "print(f\"  ‚Ä¢ False Positive Rate: {fp/(fp+tn)*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ False Negative Rate: {fn/(fn+tp)*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Total de errores: {fp+fn} de {len(test_labels)} ({(fp+fn)/len(test_labels)*100:.2f}%)\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# INTERPRETACI√ìN DE RESULTADOS\n",
    "# --------------------------------------------------------\n",
    "print(\"\\nüí° INTERPRETACI√ìN DE RESULTADOS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Interpretar accuracy\n",
    "if accuracy >= 0.95:\n",
    "    nivel = \"üèÜ EXCELENTE\"\n",
    "    desc = \"Rendimiento sobresaliente\"\n",
    "elif accuracy >= 0.90:\n",
    "    nivel = \"üéØ MUY BUENO\"\n",
    "    desc = \"Rendimiento muy s√≥lido\"\n",
    "elif accuracy >= 0.85:\n",
    "    nivel = \"‚úÖ BUENO\"\n",
    "    desc = \"Rendimiento satisfactorio\"\n",
    "elif accuracy >= 0.80:\n",
    "    nivel = \"üëç ACEPTABLE\"\n",
    "    desc = \"Rendimiento decente\"\n",
    "else:\n",
    "    nivel = \"‚ö†Ô∏è MEJORABLE\"\n",
    "    desc = \"Requiere optimizaci√≥n\"\n",
    "\n",
    "print(f\"  Nivel de rendimiento: {nivel}\")\n",
    "print(f\"  {desc} con {accuracy:.1%} de accuracy\")\n",
    "\n",
    "# An√°lisis de balance entre precision y recall\n",
    "if abs(avg_precision - avg_recall) < 0.05:\n",
    "    print(f\"\\n  ‚úÖ Modelo balanceado: Precision y Recall similares\")\n",
    "elif avg_precision > avg_recall:\n",
    "    print(f\"\\n  üìå Modelo conservador: Mayor precision que recall\")\n",
    "    print(f\"     (Pocos falsos positivos, pero puede perder algunos positivos)\")\n",
    "else:\n",
    "    print(f\"\\n  üìå Modelo agresivo: Mayor recall que precision\")\n",
    "    print(f\"     (Captura muchos positivos, pero con m√°s falsos positivos)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Evaluaci√≥n completada exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Celda 8 - Predicciones CON TRUNCAMIENTO FORZADO \n",
    "# print(\"üîç Realizando predicciones en el conjunto de test...\\n\")\n",
    "\n",
    "# # Funci√≥n para predecir con truncamiento\n",
    "# def predict_with_truncation(model, texts):\n",
    "#     \"\"\"Predice con truncamiento forzado a 512 tokens\"\"\"\n",
    "#     results = []\n",
    "#     for text in texts:\n",
    "#         # Tokenizar con truncamiento\n",
    "#         inputs = model.tokenizer(\n",
    "#             text, \n",
    "#             max_length=512,\n",
    "#             truncation=True,\n",
    "#             padding='max_length',\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "        \n",
    "#         # Mover inputs a device\n",
    "#         inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "#         # Predecir\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.model(**inputs)\n",
    "        \n",
    "#         # Obtener probabilidades\n",
    "#         probs = torch.softmax(outputs.logits, dim=-1)\n",
    "#         conf, pred_class = torch.max(probs, dim=-1)\n",
    "        \n",
    "#         pred_label = model.model.config.id2label[pred_class.item()] \n",
    "        \n",
    "#         results.append({\n",
    "#             'text': text,\n",
    "#             'predicted_label': pred_label, \n",
    "#             'predicted_class': pred_class.item(),\n",
    "#             'confidence': conf.item(),\n",
    "#             'probabilities': probs[0].tolist()\n",
    "#         })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Predecir en batches\n",
    "# batch_size = config.model.batch_size\n",
    "# all_preds = []\n",
    "\n",
    "# for i in tqdm(range(0, len(test_texts), batch_size), desc=\"Prediciendo\"):\n",
    "#     batch_texts = test_texts[i : i+batch_size]\n",
    "    \n",
    "#     try:\n",
    "#         # Intentar predecir normalmente primero\n",
    "#         preds = model.predict_batch(batch_texts)\n",
    "#         all_preds.extend(preds)\n",
    "#     except RuntimeError:\n",
    "#         print(f\"‚ö†Ô∏è Error en batch {i//batch_size}, aplicando truncamiento...\")\n",
    "#         preds = predict_with_truncation(model, batch_texts)\n",
    "#         all_preds.extend(preds)\n",
    "\n",
    "# print(f\"\\n‚úÖ Predicciones completadas: {len(all_preds)} ejemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 9 - CREAR DATAFRAME CON RESULTADOS\n",
    "# ============================================================\n",
    "# Esta celda organiza todos los resultados en un DataFrame\n",
    "# para facilitar el an√°lisis y exportaci√≥n\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "print(\"üìä CREACI√ìN DE DATAFRAME CON RESULTADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 1: Crear DataFrame principal\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£ Construyendo DataFrame con predicciones...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Crear DataFrame con todos los datos\n",
    "results_df = pd.DataFrame({\n",
    "    'text': test_texts,\n",
    "    'true_label': test_labels,\n",
    "    'predicted_label': predicted_labels,\n",
    "    'true_sentiment': ['Positive' if label == 1 else 'Negative' for label in test_labels],\n",
    "    'predicted_sentiment': ['Positive' if label == 1 else 'Negative' for label in predicted_labels]\n",
    "})\n",
    "\n",
    "# Agregar probabilidades\n",
    "results_df['prob_negative'] = [pred['probabilities']['NEGATIVE'] for pred in predictions_raw]\n",
    "results_df['prob_positive'] = [pred['probabilities']['POSITIVE'] for pred in predictions_raw]\n",
    "\n",
    "# Agregar confianza del modelo (m√°xima probabilidad)\n",
    "results_df['confidence'] = results_df[['prob_negative', 'prob_positive']].max(axis=1)\n",
    "\n",
    "# Marcar si la predicci√≥n fue correcta\n",
    "results_df['correct'] = results_df['true_label'] == results_df['predicted_label']\n",
    "\n",
    "# Categorizar tipo de error\n",
    "def categorize_error(row):\n",
    "    if row['correct']:\n",
    "        return 'Correct'\n",
    "    elif row['true_label'] == 0 and row['predicted_label'] == 1:\n",
    "        return 'False Positive'\n",
    "    elif row['true_label'] == 1 and row['predicted_label'] == 0:\n",
    "        return 'False Negative'\n",
    "    return 'Unknown'\n",
    "\n",
    "results_df['error_type'] = results_df.apply(categorize_error, axis=1)\n",
    "\n",
    "print(f\"‚úÖ DataFrame creado con {len(results_df)} filas y {len(results_df.columns)} columnas\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 2: Mostrar informaci√≥n del DataFrame\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£ Informaci√≥n del DataFrame:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"\\nColumnas disponibles:\")\n",
    "for col in results_df.columns:\n",
    "    print(f\"  ‚Ä¢ {col}: {results_df[col].dtype}\")\n",
    "\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "print(results_df[['text', 'true_sentiment', 'predicted_sentiment', 'confidence', 'correct']].head())\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 3: Estad√≠sticas r√°pidas\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£ Estad√≠sticas del DataFrame:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Distribuci√≥n de predicciones\n",
    "print(\"\\nüìä Distribuci√≥n de resultados:\")\n",
    "print(results_df['error_type'].value_counts())\n",
    "print(\"\\nüìä Porcentajes:\")\n",
    "print(results_df['error_type'].value_counts(normalize=True).round(4) * 100)\n",
    "\n",
    "# Confianza promedio por tipo\n",
    "print(\"\\nüìä Confianza promedio por tipo de resultado:\")\n",
    "for error_type in results_df['error_type'].unique():\n",
    "    avg_conf = results_df[results_df['error_type'] == error_type]['confidence'].mean()\n",
    "    print(f\"  ‚Ä¢ {error_type}: {avg_conf:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 4: Identificar casos interesantes\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£ Casos interesantes para an√°lisis:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Predicciones incorrectas con alta confianza\n",
    "high_conf_errors = results_df[(~results_df['correct']) & (results_df['confidence'] > 0.9)]\n",
    "print(f\"\\n‚ö†Ô∏è Errores con alta confianza (>90%): {len(high_conf_errors)} casos\")\n",
    "if len(high_conf_errors) > 0:\n",
    "    print(\"\\nEjemplos de errores con alta confianza:\")\n",
    "    for idx in high_conf_errors.head(3).index:\n",
    "        row = results_df.loc[idx]\n",
    "        print(f\"\\n  ‚Ä¢ Texto: '{row['text'][:60]}...'\")\n",
    "        print(f\"    Real: {row['true_sentiment']}, Predicho: {row['predicted_sentiment']}\")\n",
    "        print(f\"    Confianza: {row['confidence']:.2%}\")\n",
    "\n",
    "# Predicciones correctas con baja confianza\n",
    "low_conf_correct = results_df[(results_df['correct']) & (results_df['confidence'] < 0.6)]\n",
    "print(f\"\\nüìå Aciertos con baja confianza (<60%): {len(low_conf_correct)} casos\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 5: Preparar DataFrames espec√≠ficos\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£ DataFrames espec√≠ficos creados:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# DataFrame solo con errores\n",
    "errors_df = results_df[~results_df['correct']].copy()\n",
    "print(f\"  ‚Ä¢ errors_df: {len(errors_df)} errores para an√°lisis detallado\")\n",
    "\n",
    "# DataFrame con casos extremos\n",
    "extreme_cases_df = results_df[\n",
    "    (results_df['confidence'] > 0.95) | (results_df['confidence'] < 0.55)\n",
    "].copy()\n",
    "print(f\"  ‚Ä¢ extreme_cases_df: {len(extreme_cases_df)} casos con confianza muy alta o muy baja\")\n",
    "\n",
    "# DataFrame ordenado por confianza\n",
    "sorted_df = results_df.sort_values('confidence', ascending=False).copy()\n",
    "print(f\"  ‚Ä¢ sorted_df: DataFrame ordenado por confianza\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ DataFrames creados exitosamente\")\n",
    "print(\"\\nüì¶ Variables disponibles:\")\n",
    "print(\"  ‚Ä¢ results_df: DataFrame principal con todos los resultados\")\n",
    "print(\"  ‚Ä¢ errors_df: Solo las predicciones incorrectas\")\n",
    "print(\"  ‚Ä¢ extreme_cases_df: Casos con confianza extrema\")\n",
    "print(\"  ‚Ä¢ sorted_df: Resultados ordenados por confianza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Celda 9 - Crear DataFrame\n",
    "# predicted_labels = [pred['label_id'] for pred in all_preds]\n",
    "# predicted_probs = [pred['confidence'] for pred in all_preds]\n",
    "# predicted_classes = [pred['prediction'] for pred in all_preds]\n",
    "\n",
    "# # Crear DataFrame con resultados\n",
    "# results_df = pd.DataFrame({\n",
    "#     'text': test_texts,\n",
    "#     'true_label': test_labels,\n",
    "#     'predicted_label': predicted_labels,\n",
    "#     'prediction': predicted_classes,\n",
    "#     'confidence': predicted_probs,\n",
    "#     'correct': [t == p for t, p in zip(test_labels, predicted_labels)]\n",
    "# })\n",
    "\n",
    "# # Mostrar primeros ejemplos\n",
    "# print(\"\\nüìä Primeros 5 resultados:\")\n",
    "# display(results_df[['text', 'true_label', 'prediction', 'confidence', 'correct']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fe321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 10 - M√©tricas\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "precision = precision_score(test_labels, predicted_labels)\n",
    "recall = recall_score(test_labels, predicted_labels)\n",
    "f1 = f1_score(test_labels, predicted_labels)\n",
    "\n",
    "print(\"üìà M√âTRICAS DE CLASIFICACI√ìN\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy:.2%})\")\n",
    "print(f\"Precision: {precision:.4f} ({precision:.2%})\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall:.2%})\")\n",
    "print(f\"F1-Score:  {f1:.4f} ({f1:.2%})\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Reporte completo\n",
    "print(\"\\nüìã Reporte de Clasificaci√≥n:\")\n",
    "print(classification_report(\n",
    "    test_labels, \n",
    "    predicted_labels,\n",
    "    target_names=['NEGATIVE', 'POSITIVE']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b4f1b",
   "metadata": {},
   "source": [
    "Predicci√≥n\n",
    "              NEG    POS\n",
    "Real  NEG  [  TN  |  FP  ]\n",
    "      POS  [  FN  |  TP  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 11 - VISUALIZACIONES DE RESULTADOS\n",
    "# ============================================================\n",
    "# Esta celda crea visualizaciones para analizar el rendimiento\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Configurar estilo de visualizaci√≥n\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä VISUALIZACIONES DE RESULTADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear figura con subplots\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1. MATRIZ DE CONFUSI√ìN\n",
    "# --------------------------------------------------------\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'],\n",
    "            cbar_kws={'label': 'Cantidad'})\n",
    "plt.title('Matriz de Confusi√≥n', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "\n",
    "# Agregar porcentajes a la matriz\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / cm.sum() * 100\n",
    "        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=9, color='gray')\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2. DISTRIBUCI√ìN DE PROBABILIDADES\n",
    "# --------------------------------------------------------\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "\n",
    "# Separar probabilidades por clase real\n",
    "neg_probs = [predictions_raw[i]['probabilities']['POSITIVE'] \n",
    "             for i in range(len(test_labels)) if test_labels[i] == 0]\n",
    "pos_probs = [predictions_raw[i]['probabilities']['POSITIVE'] \n",
    "             for i in range(len(test_labels)) if test_labels[i] == 1]\n",
    "\n",
    "# Crear histograma\n",
    "bins = np.linspace(0, 1, 31)\n",
    "ax2.hist(neg_probs, bins=bins, alpha=0.5, label='Real Negative', color='red', edgecolor='black')\n",
    "ax2.hist(pos_probs, bins=bins, alpha=0.5, label='Real Positive', color='green', edgecolor='black')\n",
    "ax2.axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Threshold')\n",
    "ax2.set_xlabel('Probabilidad de Positivo')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_title('Distribuci√≥n de Probabilidades', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3. CURVA ROC\n",
    "# --------------------------------------------------------\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "\n",
    "# Calcular curva ROC\n",
    "probs_positive = [pred['probabilities']['POSITIVE'] for pred in predictions_raw]\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, probs_positive)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotear\n",
    "ax3.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "ax3.fill_between(fpr, tpr, alpha=0.2, color='darkorange')\n",
    "ax3.set_xlim([0.0, 1.0])\n",
    "ax3.set_ylim([0.0, 1.05])\n",
    "ax3.set_xlabel('False Positive Rate')\n",
    "ax3.set_ylabel('True Positive Rate')\n",
    "ax3.set_title('Curva ROC', fontsize=14, fontweight='bold')\n",
    "ax3.legend(loc=\"lower right\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4. M√âTRICAS POR CLASE\n",
    "# --------------------------------------------------------\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "\n",
    "# Calcular m√©tricas para esta celda\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    test_labels, predicted_labels, average=None\n",
    ")\n",
    "\n",
    "# Datos de m√©tricas\n",
    "metrics_data = {\n",
    "    'Precision': [precision[0], precision[1]],\n",
    "    'Recall': [recall[0], recall[1]],\n",
    "    'F1-Score': [f1[0], f1[1]]\n",
    "}\n",
    "\n",
    "x = np.arange(2)\n",
    "width = 0.25\n",
    "\n",
    "# Crear barras\n",
    "for i, (metric, values) in enumerate(metrics_data.items()):\n",
    "    offset = (i - 1) * width\n",
    "    bars = ax4.bar(x + offset, values, width, label=metric)\n",
    "    \n",
    "    # Agregar valores encima de las barras\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax4.set_xlabel('Clase')\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.set_title('M√©tricas por Clase', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(['Negative', 'Positive'])\n",
    "ax4.legend()\n",
    "ax4.set_ylim([0, 1.1])\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 5. DISTRIBUCI√ìN DE CONFIANZA\n",
    "# --------------------------------------------------------\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "\n",
    "# Calcular confianza para cada predicci√≥n\n",
    "confidences = results_df['confidence'].values\n",
    "\n",
    "# Separar por correctas e incorrectas\n",
    "correct_conf = results_df[results_df['correct']]['confidence'].values\n",
    "incorrect_conf = results_df[~results_df['correct']]['confidence'].values\n",
    "\n",
    "# Crear boxplot\n",
    "bp = ax5.boxplot([correct_conf, incorrect_conf], \n",
    "                  labels=['Correctas', 'Incorrectas'],\n",
    "                  patch_artist=True,\n",
    "                  showmeans=True)\n",
    "\n",
    "# Colorear las cajas\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax5.set_ylabel('Confianza')\n",
    "ax5.set_title('Distribuci√≥n de Confianza', fontsize=14, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Agregar estad√≠sticas\n",
    "ax5.text(1, 0.55, f'Œº={correct_conf.mean():.3f}', ha='center', fontsize=9)\n",
    "ax5.text(2, 0.55, f'Œº={incorrect_conf.mean():.3f}', ha='center', fontsize=9)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 6. AN√ÅLISIS DE ERRORES\n",
    "# --------------------------------------------------------\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "\n",
    "# Contar tipos de errores\n",
    "error_counts = results_df['error_type'].value_counts()\n",
    "\n",
    "# Crear gr√°fico de torta\n",
    "colors_pie = ['#90EE90', '#FFB6C1', '#FFA07A']\n",
    "wedges, texts, autotexts = ax6.pie(error_counts.values, \n",
    "                                     labels=error_counts.index,\n",
    "                                     autopct='%1.1f%%',\n",
    "                                     colors=colors_pie,\n",
    "                                     startangle=90,\n",
    "                                     explode=(0, 0.1, 0.1))\n",
    "\n",
    "# Mejorar apariencia\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(10)\n",
    "\n",
    "ax6.set_title('Distribuci√≥n de Predicciones', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Agregar leyenda con cantidades\n",
    "legend_labels = [f'{label}: {count}' for label, count in error_counts.items()]\n",
    "ax6.legend(legend_labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Ajustar layout\n",
    "plt.suptitle(f'An√°lisis de Resultados - Accuracy: {accuracy:.2%}', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar el gr√°fico\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# ESTAD√çSTICAS ADICIONALES\n",
    "# --------------------------------------------------------\n",
    "print(\"\\nüìä ESTAD√çSTICAS ADICIONALES:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# An√°lisis de errores\n",
    "print(\"\\nüîç An√°lisis detallado de errores:\")\n",
    "false_positives = len(results_df[results_df['error_type'] == 'False Positive'])\n",
    "false_negatives = len(results_df[results_df['error_type'] == 'False Negative'])\n",
    "total_errors = false_positives + false_negatives\n",
    "\n",
    "print(f\"  ‚Ä¢ False Positives: {false_positives} ({false_positives/len(results_df)*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ False Negatives: {false_negatives} ({false_negatives/len(results_df)*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Total de errores: {total_errors} ({total_errors/len(results_df)*100:.2f}%)\")\n",
    "\n",
    "# Confianza promedio\n",
    "print(f\"\\nüìà Confianza del modelo:\")\n",
    "print(f\"  ‚Ä¢ Confianza promedio global: {results_df['confidence'].mean():.4f}\")\n",
    "print(f\"  ‚Ä¢ Confianza en predicciones correctas: {correct_conf.mean():.4f}\")\n",
    "print(f\"  ‚Ä¢ Confianza en predicciones incorrectas: {incorrect_conf.mean():.4f}\")\n",
    "print(f\"  ‚Ä¢ Diferencia: {correct_conf.mean() - incorrect_conf.mean():.4f}\")\n",
    "\n",
    "# Casos extremos\n",
    "very_confident_errors = results_df[(~results_df['correct']) & (results_df['confidence'] > 0.95)]\n",
    "print(f\"\\n‚ö†Ô∏è Errores con muy alta confianza (>95%):\")\n",
    "print(f\"  ‚Ä¢ Cantidad: {len(very_confident_errors)} casos\")\n",
    "if len(very_confident_errors) > 0:\n",
    "    print(f\"  ‚Ä¢ Porcentaje del total de errores: {len(very_confident_errors)/total_errors*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Visualizaciones completadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 11 - Matriz de Confusi√≥n\n",
    "# Para mi objetivo esta visualizaci√≥n es clave\n",
    "# Calcular matriz de confusi√≥n\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True,           # Mostrar n√∫meros\n",
    "    fmt='d',              # Formato entero\n",
    "    cmap='Blues',         # Colores azules\n",
    "    xticklabels=['NEGATIVE', 'POSITIVE'],\n",
    "    yticklabels=['NEGATIVE', 'POSITIVE'],\n",
    "    cbar_kws={'label': 'Cantidad'}\n",
    ")\n",
    "plt.title('Matriz de Confusi√≥n', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Etiqueta Real', fontsize=12)\n",
    "plt.xlabel('Predicci√≥n', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de errores\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\nüìä An√°lisis de la Matriz:\")\n",
    "print(f\"  ‚Ä¢ True Negatives (TN):  {tn} - Negativos correctos\")\n",
    "print(f\"  ‚Ä¢ False Positives (FP): {fp} - Negativos clasificados como positivos\")\n",
    "print(f\"  ‚Ä¢ False Negatives (FN): {fn} - Positivos clasificados como negativos\")\n",
    "print(f\"  ‚Ä¢ True Positives (TP):  {tp} - Positivos correctos\")\n",
    "print(f\"\\n  ‚Ä¢ Total de errores: {fp + fn} ({(fp + fn)/len(test_labels):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ed406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 12 - DIAGN√ìSTICO PARA INTERPRETABILIDAD\n",
    "# ============================================================\n",
    "# Esta celda identifica qu√© herramientas tenemos disponibles\n",
    "# para interpretar las decisiones del modelo\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîç DIAGN√ìSTICO PASO A PASO - IDENTIFICAR QU√â TENEMOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 1: VERIFICAR QU√â TIPO DE MODELO TENEMOS\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£ TIPO DE MODELO Y ARQUITECTURA:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Informaci√≥n b√°sica del modelo\n",
    "model_info = model.get_model_info()\n",
    "print(f\"  ‚Ä¢ Nombre del modelo: {model_info.get('model_name', 'No especificado')}\")\n",
    "print(f\"  ‚Ä¢ Tipo: Transformer (DistilBERT)\")\n",
    "print(f\"  ‚Ä¢ Tarea: Clasificaci√≥n de sentimientos binaria\")\n",
    "print(f\"  ‚Ä¢ Clases: {model_info.get('id2label', 'No disponible')}\")\n",
    "\n",
    "# Verificar si tenemos acceso al modelo de HuggingFace\n",
    "if hasattr(model, 'pipeline'):\n",
    "    print(f\"  ‚Ä¢ ‚úÖ Pipeline de HuggingFace disponible\")\n",
    "    \n",
    "    # Verificar componentes del pipeline\n",
    "    if hasattr(model.pipeline, 'model'):\n",
    "        print(f\"  ‚Ä¢ ‚úÖ Acceso al modelo base\")\n",
    "        model_base = model.pipeline.model\n",
    "        print(f\"      - Tipo: {type(model_base).__name__}\")\n",
    "        \n",
    "    if hasattr(model.pipeline, 'tokenizer'):\n",
    "        print(f\"  ‚Ä¢ ‚úÖ Acceso al tokenizer\")\n",
    "        tokenizer = model.pipeline.tokenizer\n",
    "        print(f\"      - Tipo: {type(tokenizer).__name__}\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ ‚ö†Ô∏è Pipeline no directamente accesible\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 2: DATOS DISPONIBLES PARA INTERPRETACI√ìN\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£ DATOS DISPONIBLES PARA AN√ÅLISIS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Verificar qu√© tenemos en memoria\n",
    "available_data = {\n",
    "    'test_texts': 'Textos de entrada' if 'test_texts' in locals() else None,\n",
    "    'test_labels': 'Etiquetas reales' if 'test_labels' in locals() else None,\n",
    "    'predicted_labels': 'Predicciones' if 'predicted_labels' in locals() else None,\n",
    "    'predictions_raw': 'Predicciones completas' if 'predictions_raw' in locals() else None,\n",
    "    'results_df': 'DataFrame de resultados' if 'results_df' in locals() else None,\n",
    "    'errors_df': 'DataFrame de errores' if 'errors_df' in locals() else None\n",
    "}\n",
    "\n",
    "for var_name, description in available_data.items():\n",
    "    if description:\n",
    "        if var_name in locals():\n",
    "            var = eval(var_name)\n",
    "            if hasattr(var, '__len__'):\n",
    "                print(f\"  ‚Ä¢ ‚úÖ {var_name}: {description} ({len(var)} elementos)\")\n",
    "            else:\n",
    "                print(f\"  ‚Ä¢ ‚úÖ {var_name}: {description}\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ ‚ùå {var_name}: No disponible\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 3: INFORMACI√ìN EN LAS PREDICCIONES\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£ INFORMACI√ìN DISPONIBLE EN PREDICCIONES:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if 'predictions_raw' in locals() and len(predictions_raw) > 0:\n",
    "    sample_pred = predictions_raw[0]\n",
    "    print(\"  Campos en cada predicci√≥n:\")\n",
    "    for key in sample_pred.keys():\n",
    "        print(f\"    ‚Ä¢ {key}: {type(sample_pred[key]).__name__}\")\n",
    "    \n",
    "    # Verificar si tenemos probabilidades por clase\n",
    "    if 'probabilities' in sample_pred:\n",
    "        print(f\"\\n  üìä Probabilidades disponibles para:\")\n",
    "        for class_name in sample_pred['probabilities'].keys():\n",
    "            print(f\"    ‚Ä¢ {class_name}\")\n",
    "        print(\"  ‚úÖ Podemos analizar la confianza del modelo\")\n",
    "    \n",
    "    # Verificar si tenemos scores de atenci√≥n o tokens\n",
    "    if 'attention_scores' in sample_pred:\n",
    "        print(\"  ‚úÖ Scores de atenci√≥n disponibles\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Scores de atenci√≥n no disponibles en predicciones\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 4: CAPACIDADES DE INTERPRETABILIDAD\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£ T√âCNICAS DE INTERPRETABILIDAD APLICABLES:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"\\nüìä Con los datos actuales podemos hacer:\")\n",
    "print(\"  ‚úÖ An√°lisis de confianza y probabilidades\")\n",
    "print(\"  ‚úÖ An√°lisis de errores por patrones\")\n",
    "print(\"  ‚úÖ An√°lisis de casos extremos (alta/baja confianza)\")\n",
    "print(\"  ‚úÖ An√°lisis estad√≠stico de predicciones\")\n",
    "\n",
    "print(\"\\nüîß Para interpretabilidad avanzada necesitar√≠amos:\")\n",
    "print(\"  ‚Ä¢ LIME: Explicaciones locales (requiere instalar lime)\")\n",
    "print(\"  ‚Ä¢ SHAP: Valores de Shapley (requiere instalar shap)\")\n",
    "print(\"  ‚Ä¢ Attention Weights: Pesos de atenci√≥n del transformer\")\n",
    "print(\"  ‚Ä¢ Integrated Gradients: Gradientes del modelo\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 5: AN√ÅLISIS B√ÅSICO DE INTERPRETABILIDAD\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£ AN√ÅLISIS B√ÅSICO DE INTERPRETABILIDAD:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Analizar casos m√°s seguros vs m√°s inciertos\n",
    "if 'results_df' in locals():\n",
    "    # Casos m√°s seguros (alta confianza y correctos)\n",
    "    most_confident = results_df[results_df['correct']].nlargest(5, 'confidence')\n",
    "    print(\"\\nüéØ Predicciones m√°s confiables (correctas con alta confianza):\")\n",
    "    for idx in most_confident.index[:3]:\n",
    "        row = results_df.loc[idx]\n",
    "        print(f\"\\n  ‚Ä¢ Texto: '{row['text'][:60]}...'\")\n",
    "        print(f\"    Sentimiento: {row['true_sentiment']} (confianza: {row['confidence']:.2%})\")\n",
    "    \n",
    "    # Casos m√°s inciertos\n",
    "    most_uncertain = results_df.nsmallest(5, 'confidence')\n",
    "    print(\"\\nü§î Predicciones m√°s inciertas (baja confianza):\")\n",
    "    for idx in most_uncertain.index[:3]:\n",
    "        row = results_df.loc[idx]\n",
    "        print(f\"\\n  ‚Ä¢ Texto: '{row['text'][:60]}...'\")\n",
    "        print(f\"    Predicci√≥n: {row['predicted_sentiment']} (confianza: {row['confidence']:.2%})\")\n",
    "        print(f\"    Prob Neg: {row['prob_negative']:.3f}, Prob Pos: {row['prob_positive']:.3f}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 6: PREPARACI√ìN PARA INTERPRETABILIDAD AVANZADA\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n6Ô∏è‚É£ PREPARACI√ìN PARA INTERPRETABILIDAD AVANZADA:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Verificar qu√© librer√≠as est√°n disponibles\n",
    "libraries_check = {\n",
    "    'lime': False,\n",
    "    'shap': False,\n",
    "    'captum': False,\n",
    "    'transformers_interpret': False\n",
    "}\n",
    "\n",
    "for lib_name in libraries_check.keys():\n",
    "    try:\n",
    "        __import__(lib_name)\n",
    "        libraries_check[lib_name] = True\n",
    "        print(f\"  ‚úÖ {lib_name}: Instalado y disponible\")\n",
    "    except ImportError:\n",
    "        print(f\"  ‚ùå {lib_name}: No instalado\")\n",
    "\n",
    "# Sugerencias de instalaci√≥n\n",
    "if not any(libraries_check.values()):\n",
    "    print(\"\\nüí° Para an√°lisis de interpretabilidad avanzado, instala:\")\n",
    "    print(\"  pip install lime               # Para explicaciones locales\")\n",
    "    print(\"  pip install shap               # Para valores SHAP\")\n",
    "    print(\"  pip install captum             # Para PyTorch (si aplica)\")\n",
    "    print(\"  pip install transformers-interpret  # Para transformers\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# RESUMEN Y PR√ìXIMOS PASOS\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã RESUMEN Y PR√ìXIMOS PASOS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"\\n‚úÖ PODEMOS HACER AHORA:\")\n",
    "print(\"  1. An√°lisis de confianza y distribuci√≥n de probabilidades\")\n",
    "print(\"  2. Identificaci√≥n de patrones en errores\")\n",
    "print(\"  3. An√°lisis de casos extremos\")\n",
    "print(\"  4. Visualizaci√≥n de m√©tricas por subgrupos\")\n",
    "\n",
    "print(\"\\nüîÑ PR√ìXIMOS PASOS SUGERIDOS:\")\n",
    "print(\"  1. Analizar palabras clave que influyen en predicciones\")\n",
    "print(\"  2. Estudiar casos donde el modelo falla consistentemente\")\n",
    "print(\"  3. Implementar LIME para explicaciones locales\")\n",
    "print(\"  4. Extraer attention weights si es posible\")\n",
    "\n",
    "print(\"\\nüíæ VARIABLES LISTAS PARA INTERPRETABILIDAD:\")\n",
    "print(f\"  ‚Ä¢ results_df: DataFrame con todos los resultados\")\n",
    "print(f\"  ‚Ä¢ errors_df: Solo los errores para an√°lisis profundo\")\n",
    "print(f\"  ‚Ä¢ predictions_raw: Predicciones con probabilidades completas\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Diagn√≥stico completado - Listo para an√°lisis de interpretabilidad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 13 - DISTRIBUCI√ìN DE CONFIANZA\n",
    "# ============================================================\n",
    "# Esta celda analiza en detalle la distribuci√≥n de confianza\n",
    "# del modelo en sus predicciones\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "print(\"üìä AN√ÅLISIS DE DISTRIBUCI√ìN DE CONFIANZA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 1: ESTAD√çSTICAS DE CONFIANZA\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£ ESTAD√çSTICAS GENERALES DE CONFIANZA:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Estad√≠sticas b√°sicas\n",
    "confidence_stats = results_df['confidence'].describe()\n",
    "print(\"\\nüìà Resumen estad√≠stico de confianza:\")\n",
    "print(confidence_stats)\n",
    "\n",
    "# Estad√≠sticas por tipo de resultado\n",
    "print(\"\\nüìä Confianza por tipo de resultado:\")\n",
    "for error_type in results_df['error_type'].unique():\n",
    "    conf_values = results_df[results_df['error_type'] == error_type]['confidence']\n",
    "    print(f\"\\n  {error_type}:\")\n",
    "    print(f\"    ‚Ä¢ Media: {conf_values.mean():.4f}\")\n",
    "    print(f\"    ‚Ä¢ Mediana: {conf_values.median():.4f}\")\n",
    "    print(f\"    ‚Ä¢ Std: {conf_values.std():.4f}\")\n",
    "    print(f\"    ‚Ä¢ Min: {conf_values.min():.4f}\")\n",
    "    print(f\"    ‚Ä¢ Max: {conf_values.max():.4f}\")\n",
    "    print(f\"    ‚Ä¢ Cantidad: {len(conf_values)} ({len(conf_values)/len(results_df)*100:.1f}%)\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 2: AN√ÅLISIS POR RANGOS DE CONFIANZA\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£ AN√ÅLISIS POR RANGOS DE CONFIANZA:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Definir rangos de confianza\n",
    "confidence_ranges = [\n",
    "    (0.5, 0.6, 'Muy baja (50-60%)'),\n",
    "    (0.6, 0.7, 'Baja (60-70%)'),\n",
    "    (0.7, 0.8, 'Media (70-80%)'),\n",
    "    (0.8, 0.9, 'Alta (80-90%)'),\n",
    "    (0.9, 1.0, 'Muy alta (90-100%)')\n",
    "]\n",
    "\n",
    "print(\"\\nüìä Distribuci√≥n por rangos:\")\n",
    "for min_conf, max_conf, label in confidence_ranges:\n",
    "    mask = (results_df['confidence'] >= min_conf) & (results_df['confidence'] < max_conf)\n",
    "    count = mask.sum()\n",
    "    correct_in_range = (results_df[mask]['correct']).sum()\n",
    "    accuracy_in_range = correct_in_range / count if count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n  {label}:\")\n",
    "    print(f\"    ‚Ä¢ Cantidad: {count} ({count/len(results_df)*100:.1f}%)\")\n",
    "    if count > 0:\n",
    "        print(f\"    ‚Ä¢ Correctas: {correct_in_range} ({accuracy_in_range*100:.1f}%)\")\n",
    "        print(f\"    ‚Ä¢ Incorrectas: {count - correct_in_range} ({(1-accuracy_in_range)*100:.1f}%)\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 3: CALIBRACI√ìN DEL MODELO\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£ CALIBRACI√ìN DEL MODELO:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Analizar si la confianza refleja la probabilidad real de acierto\n",
    "calibration_bins = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
    "expected_acc = []\n",
    "actual_acc = []\n",
    "bin_centers = []\n",
    "\n",
    "for min_conf, max_conf in calibration_bins:\n",
    "    mask = (results_df['confidence'] >= min_conf) & (results_df['confidence'] < max_conf)\n",
    "    if mask.sum() > 0:\n",
    "        # Confianza esperada (centro del bin)\n",
    "        expected = (min_conf + max_conf) / 2\n",
    "        # Accuracy real en ese rango\n",
    "        actual = results_df[mask]['correct'].mean()\n",
    "        \n",
    "        expected_acc.append(expected)\n",
    "        actual_acc.append(actual)\n",
    "        bin_centers.append(expected)\n",
    "        \n",
    "        print(f\"  Confianza {min_conf:.0%}-{max_conf:.0%}:\")\n",
    "        print(f\"    ‚Ä¢ Accuracy esperado: {expected:.1%}\")\n",
    "        print(f\"    ‚Ä¢ Accuracy real: {actual:.1%}\")\n",
    "        print(f\"    ‚Ä¢ Diferencia: {(actual - expected)*100:+.1f}%\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 4: VISUALIZACIONES\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£ GENERANDO VISUALIZACIONES...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Crear figura con subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('An√°lisis Detallado de Distribuci√≥n de Confianza', fontsize=16, fontweight='bold')\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Subplot 1: Histograma de confianza general\n",
    "# --------------------------------------------------------\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(results_df['confidence'], bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "ax1.axvline(results_df['confidence'].mean(), color='red', linestyle='--', \n",
    "            label=f'Media: {results_df[\"confidence\"].mean():.3f}')\n",
    "ax1.axvline(results_df['confidence'].median(), color='green', linestyle='--', \n",
    "            label=f'Mediana: {results_df[\"confidence\"].median():.3f}')\n",
    "ax1.set_xlabel('Confianza')\n",
    "ax1.set_ylabel('Frecuencia')\n",
    "ax1.set_title('Distribuci√≥n General de Confianza')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Subplot 2: Distribuci√≥n por tipo de resultado\n",
    "# --------------------------------------------------------\n",
    "ax2 = axes[0, 1]\n",
    "correct_conf = results_df[results_df['correct']]['confidence']\n",
    "incorrect_conf = results_df[~results_df['correct']]['confidence']\n",
    "\n",
    "ax2.hist([correct_conf, incorrect_conf], bins=20, label=['Correctas', 'Incorrectas'], \n",
    "         color=['green', 'red'], alpha=0.6, edgecolor='black')\n",
    "ax2.set_xlabel('Confianza')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_title('Confianza: Correctas vs Incorrectas')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Subplot 3: Curva de calibraci√≥n\n",
    "# --------------------------------------------------------\n",
    "ax3 = axes[0, 2]\n",
    "if len(expected_acc) > 0:\n",
    "    ax3.plot(expected_acc, actual_acc, 'o-', markersize=8, linewidth=2, label='Calibraci√≥n real')\n",
    "    ax3.plot([0.5, 1], [0.5, 1], 'k--', alpha=0.5, label='Perfectamente calibrado')\n",
    "    \n",
    "    # √Årea sombreada para mostrar desviaci√≥n\n",
    "    ax3.fill_between([0.5, 1], [0.45, 0.95], [0.55, 1.05], \n",
    "                     alpha=0.2, color='gray', label='¬±5% margen')\n",
    "    \n",
    "    ax3.set_xlabel('Confianza Esperada')\n",
    "    ax3.set_ylabel('Accuracy Real')\n",
    "    ax3.set_title('Curva de Calibraci√≥n del Modelo')\n",
    "    ax3.set_xlim([0.5, 1.0])\n",
    "    ax3.set_ylim([0.5, 1.0])\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Subplot 4: Violin plot por sentimiento\n",
    "# --------------------------------------------------------\n",
    "ax4 = axes[1, 0]\n",
    "data_for_violin = []\n",
    "labels_for_violin = []\n",
    "\n",
    "for sentiment in ['Negative', 'Positive']:\n",
    "    for pred_type in ['Correct', 'Incorrect']:\n",
    "        mask = (results_df['true_sentiment'] == sentiment) & \\\n",
    "               (results_df['correct'] == (pred_type == 'Correct'))\n",
    "        if mask.sum() > 0:\n",
    "            data_for_violin.append(results_df[mask]['confidence'].values)\n",
    "            labels_for_violin.append(f'{sentiment}\\n{pred_type}')\n",
    "\n",
    "if data_for_violin:\n",
    "    parts = ax4.violinplot(data_for_violin, positions=range(len(data_for_violin)), \n",
    "                           widths=0.7, showmeans=True, showmedians=True)\n",
    "    ax4.set_xticks(range(len(labels_for_violin)))\n",
    "    ax4.set_xticklabels(labels_for_violin, rotation=45)\n",
    "    ax4.set_ylabel('Confianza')\n",
    "    ax4.set_title('Distribuci√≥n de Confianza por Sentimiento y Resultado')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Subplot 5: Densidad de probabilidades\n",
    "# --------------------------------------------------------\n",
    "ax5 = axes[1, 1]\n",
    "\n",
    "# KDE plot para distribuciones suaves\n",
    "if len(correct_conf) > 1:\n",
    "    correct_conf.plot.kde(ax=ax5, label='Predicciones Correctas', color='green', linewidth=2)\n",
    "if len(incorrect_conf) > 1:\n",
    "    incorrect_conf.plot.kde(ax=ax5, label='Predicciones Incorrectas', color='red', linewidth=2)\n",
    "\n",
    "ax5.set_xlabel('Confianza')\n",
    "ax5.set_ylabel('Densidad')\n",
    "ax5.set_title('Densidad de Probabilidad de Confianza')\n",
    "ax5.set_xlim([0.5, 1.0])\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Subplot 6: Matriz de confusi√≥n por nivel de confianza\n",
    "# --------------------------------------------------------\n",
    "ax6 = axes[1, 2]\n",
    "\n",
    "# Crear matriz de confusi√≥n para diferentes niveles de confianza\n",
    "conf_levels = ['Baja\\n(50-70%)', 'Media\\n(70-90%)', 'Alta\\n(90-100%)']\n",
    "conf_matrix_data = []\n",
    "\n",
    "for level in [(0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]:\n",
    "    mask = (results_df['confidence'] >= level[0]) & (results_df['confidence'] < level[1])\n",
    "    if mask.sum() > 0:\n",
    "        tn = ((results_df[mask]['true_label'] == 0) & (results_df[mask]['predicted_label'] == 0)).sum()\n",
    "        fp = ((results_df[mask]['true_label'] == 0) & (results_df[mask]['predicted_label'] == 1)).sum()\n",
    "        fn = ((results_df[mask]['true_label'] == 1) & (results_df[mask]['predicted_label'] == 0)).sum()\n",
    "        tp = ((results_df[mask]['true_label'] == 1) & (results_df[mask]['predicted_label'] == 1)).sum()\n",
    "        total = mask.sum()\n",
    "        conf_matrix_data.append([tn/total*100, fp/total*100, fn/total*100, tp/total*100])\n",
    "    else:\n",
    "        conf_matrix_data.append([0, 0, 0, 0])\n",
    "\n",
    "# Visualizar como barras apiladas\n",
    "x = np.arange(len(conf_levels))\n",
    "width = 0.35\n",
    "colors = ['#2E7D32', '#C62828', '#F57C00', '#1976D2']  # Verde, Rojo, Naranja, Azul\n",
    "labels = ['TN', 'FP', 'FN', 'TP']\n",
    "\n",
    "bottom = np.zeros(len(conf_levels))\n",
    "for i, label in enumerate(labels):\n",
    "    values = [row[i] for row in conf_matrix_data]\n",
    "    ax6.bar(x, values, width*2, bottom=bottom, label=label, color=colors[i])\n",
    "    bottom += values\n",
    "\n",
    "ax6.set_ylabel('Porcentaje (%)')\n",
    "ax6.set_xlabel('Nivel de Confianza')\n",
    "ax6.set_title('Distribuci√≥n de Predicciones por Confianza')\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(conf_levels)\n",
    "ax6.legend(loc='upper right')\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 5: CASOS EXTREMOS E INTERESANTES\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£ CASOS EXTREMOS E INTERESANTES:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Errores con alta confianza\n",
    "high_conf_errors = results_df[(~results_df['correct']) & (results_df['confidence'] > 0.95)]\n",
    "print(f\"\\n‚ö†Ô∏è Errores con muy alta confianza (>95%): {len(high_conf_errors)} casos\")\n",
    "if len(high_conf_errors) > 0:\n",
    "    print(\"\\nEjemplos:\")\n",
    "    for idx in high_conf_errors.head(3).index:\n",
    "        row = results_df.loc[idx]\n",
    "        print(f\"\\n  ‚Ä¢ Texto: '{row['text'][:70]}...'\")\n",
    "        print(f\"    Real: {row['true_sentiment']}, Predicho: {row['predicted_sentiment']}\")\n",
    "        print(f\"    Confianza: {row['confidence']:.3f}\")\n",
    "\n",
    "# Aciertos con baja confianza\n",
    "low_conf_correct = results_df[(results_df['correct']) & (results_df['confidence'] < 0.6)]\n",
    "print(f\"\\nü§î Aciertos con baja confianza (<60%): {len(low_conf_correct)} casos\")\n",
    "if len(low_conf_correct) > 0:\n",
    "    print(\"\\nEjemplos:\")\n",
    "    for idx in low_conf_correct.head(3).index:\n",
    "        row = results_df.loc[idx]\n",
    "        print(f\"\\n  ‚Ä¢ Texto: '{row['text'][:70]}...'\")\n",
    "        print(f\"    Sentimiento: {row['true_sentiment']}\")\n",
    "        print(f\"    Confianza: {row['confidence']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ An√°lisis de distribuci√≥n de confianza completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 14 - SELECCI√ìN DE CASOS PARA SHAP Y LIME\n",
    "# ============================================================\n",
    "# Esta celda identifica y selecciona casos estrat√©gicos\n",
    "# para an√°lisis detallado con herramientas de explicabilidad\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üéØ SELECCI√ìN ESTRAT√âGICA DE CASOS PARA AN√ÅLISIS DE EXPLICABILIDAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 1: DEFINIR CATEGOR√çAS DE CASOS INTERESANTES\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£ IDENTIFICACI√ìN DE CATEGOR√çAS DE CASOS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Diccionario para almacenar casos seleccionados\n",
    "selected_cases = {\n",
    "    'high_confidence_correct': [],\n",
    "    'high_confidence_errors': [],\n",
    "    'low_confidence_correct': [],\n",
    "    'low_confidence_errors': [],\n",
    "    'borderline_cases': [],\n",
    "    'false_positives': [],\n",
    "    'false_negatives': [],\n",
    "    'representative_positive': [],\n",
    "    'representative_negative': []\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CATEGOR√çA 1: Casos de alta confianza correctos (baseline)\n",
    "# --------------------------------------------------------\n",
    "high_conf_correct = results_df[\n",
    "    (results_df['correct'] == True) & \n",
    "    (results_df['confidence'] > 0.95)\n",
    "].sort_values('confidence', ascending=False)\n",
    "\n",
    "selected_cases['high_confidence_correct'] = high_conf_correct.head(5).index.tolist()\n",
    "\n",
    "print(\"\\n‚úÖ ALTA CONFIANZA CORRECTOS (Casos baseline):\")\n",
    "print(f\"   ‚Ä¢ Total disponibles: {len(high_conf_correct)}\")\n",
    "print(f\"   ‚Ä¢ Seleccionados: {len(selected_cases['high_confidence_correct'])}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CATEGOR√çA 2: Errores con alta confianza (m√°s preocupantes)\n",
    "# --------------------------------------------------------\n",
    "high_conf_errors = results_df[\n",
    "    (results_df['correct'] == False) & \n",
    "    (results_df['confidence'] > 0.90)\n",
    "].sort_values('confidence', ascending=False)\n",
    "\n",
    "selected_cases['high_confidence_errors'] = high_conf_errors.head(5).index.tolist()\n",
    "\n",
    "print(\"\\n‚ùå ALTA CONFIANZA INCORRECTOS (Casos cr√≠ticos):\")\n",
    "print(f\"   ‚Ä¢ Total disponibles: {len(high_conf_errors)}\")\n",
    "print(f\"   ‚Ä¢ Seleccionados: {len(selected_cases['high_confidence_errors'])}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CATEGOR√çA 3: Aciertos con baja confianza\n",
    "# --------------------------------------------------------\n",
    "low_conf_correct = results_df[\n",
    "    (results_df['correct'] == True) & \n",
    "    (results_df['confidence'] < 0.65)\n",
    "].sort_values('confidence', ascending=True)\n",
    "\n",
    "selected_cases['low_confidence_correct'] = low_conf_correct.head(5).index.tolist()\n",
    "\n",
    "print(\"\\nü§î BAJA CONFIANZA CORRECTOS (Aciertos por suerte?):\")\n",
    "print(f\"   ‚Ä¢ Total disponibles: {len(low_conf_correct)}\")\n",
    "print(f\"   ‚Ä¢ Seleccionados: {len(selected_cases['low_confidence_correct'])}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CATEGOR√çA 4: Errores con baja confianza\n",
    "# --------------------------------------------------------\n",
    "low_conf_errors = results_df[\n",
    "    (results_df['correct'] == False) & \n",
    "    (results_df['confidence'] < 0.65)\n",
    "].sort_values('confidence', ascending=True)\n",
    "\n",
    "selected_cases['low_confidence_errors'] = low_conf_errors.head(5).index.tolist()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è BAJA CONFIANZA INCORRECTOS (Casos dif√≠ciles):\")\n",
    "print(f\"   ‚Ä¢ Total disponibles: {len(low_conf_errors)}\")\n",
    "print(f\"   ‚Ä¢ Seleccionados: {len(selected_cases['low_confidence_errors'])}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CATEGOR√çA 5: Casos fronterizos (cerca del 50%)\n",
    "# --------------------------------------------------------\n",
    "borderline = results_df[\n",
    "    (results_df['confidence'] >= 0.48) & \n",
    "    (results_df['confidence'] <= 0.52)\n",
    "].sort_values('confidence')\n",
    "\n",
    "selected_cases['borderline_cases'] = borderline.head(5).index.tolist()\n",
    "\n",
    "print(\"\\nüéØ CASOS FRONTERIZOS (Confianza ~50%):\")\n",
    "print(f\"   ‚Ä¢ Total disponibles: {len(borderline)}\")\n",
    "print(f\"   ‚Ä¢ Seleccionados: {len(selected_cases['borderline_cases'])}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CATEGOR√çA 6 y 7: Falsos Positivos y Negativos\n",
    "# --------------------------------------------------------\n",
    "false_positives = results_df[\n",
    "    (results_df['true_label'] == 0) & \n",
    "    (results_df['predicted_label'] == 1)\n",
    "].sort_values('confidence', ascending=False)\n",
    "\n",
    "false_negatives = results_df[\n",
    "    (results_df['true_label'] == 1) & \n",
    "    (results_df['predicted_label'] == 0)\n",
    "].sort_values('confidence', ascending=False)\n",
    "\n",
    "selected_cases['false_positives'] = false_positives.head(5).index.tolist()\n",
    "selected_cases['false_negatives'] = false_negatives.head(5).index.tolist()\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISIS DE ERRORES POR TIPO:\")\n",
    "print(f\"   ‚Ä¢ Falsos Positivos: {len(false_positives)} total, {len(selected_cases['false_positives'])} seleccionados\")\n",
    "print(f\"   ‚Ä¢ Falsos Negativos: {len(false_negatives)} total, {len(selected_cases['false_negatives'])} seleccionados\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CATEGOR√çA 8 y 9: Casos representativos por clase\n",
    "# --------------------------------------------------------\n",
    "# Positivos correctamente clasificados con confianza media-alta\n",
    "representative_pos_mask = (\n",
    "    (results_df['true_label'] == 1) & \n",
    "    (results_df['predicted_label'] == 1) &\n",
    "    (results_df['confidence'] >= 0.75) &\n",
    "    (results_df['confidence'] <= 0.85)\n",
    ")\n",
    "representative_pos_df = results_df[representative_pos_mask]\n",
    "\n",
    "# Solo hacer sample si hay suficientes datos\n",
    "if len(representative_pos_df) > 0:\n",
    "    n_samples_pos = min(5, len(representative_pos_df))\n",
    "    representative_pos = representative_pos_df.sample(n_samples_pos, random_state=42)\n",
    "    selected_cases['representative_positive'] = representative_pos.index.tolist()\n",
    "else:\n",
    "    # Si no hay casos en ese rango, tomar los m√°s cercanos\n",
    "    representative_pos_alt = results_df[\n",
    "        (results_df['true_label'] == 1) & \n",
    "        (results_df['predicted_label'] == 1) &\n",
    "        (results_df['confidence'] >= 0.70)\n",
    "    ].head(5)\n",
    "    selected_cases['representative_positive'] = representative_pos_alt.index.tolist()\n",
    "\n",
    "# Negativos correctamente clasificados con confianza media-alta\n",
    "representative_neg_mask = (\n",
    "    (results_df['true_label'] == 0) & \n",
    "    (results_df['predicted_label'] == 0) &\n",
    "    (results_df['confidence'] >= 0.75) &\n",
    "    (results_df['confidence'] <= 0.85)\n",
    ")\n",
    "representative_neg_df = results_df[representative_neg_mask]\n",
    "\n",
    "# Solo hacer sample si hay suficientes datos\n",
    "if len(representative_neg_df) > 0:\n",
    "    n_samples_neg = min(5, len(representative_neg_df))\n",
    "    representative_neg = representative_neg_df.sample(n_samples_neg, random_state=42)\n",
    "    selected_cases['representative_negative'] = representative_neg.index.tolist()\n",
    "else:\n",
    "    # Si no hay casos en ese rango, tomar los m√°s cercanos\n",
    "    representative_neg_alt = results_df[\n",
    "        (results_df['true_label'] == 0) & \n",
    "        (results_df['predicted_label'] == 0) &\n",
    "        (results_df['confidence'] >= 0.70)\n",
    "    ].head(5)\n",
    "    selected_cases['representative_negative'] = representative_neg_alt.index.tolist()\n",
    "\n",
    "print(\"\\nüìå CASOS REPRESENTATIVOS (Confianza 75-85%):\")\n",
    "print(f\"   ‚Ä¢ Positivos representativos: {len(selected_cases['representative_positive'])} seleccionados\")\n",
    "print(f\"   ‚Ä¢ Negativos representativos: {len(selected_cases['representative_negative'])} seleccionados\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 2: CREAR DATAFRAME CON CASOS SELECCIONADOS\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£ CONSOLIDACI√ìN DE CASOS SELECCIONADOS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Crear lista de todos los √≠ndices √∫nicos seleccionados\n",
    "all_selected_indices = []\n",
    "case_categories = []\n",
    "\n",
    "for category, indices in selected_cases.items():\n",
    "    for idx in indices:\n",
    "        if idx not in all_selected_indices:\n",
    "            all_selected_indices.append(idx)\n",
    "            case_categories.append(category)\n",
    "\n",
    "# Crear DataFrame con los casos seleccionados\n",
    "selected_df = results_df.loc[all_selected_indices].copy()\n",
    "selected_df['case_category'] = case_categories\n",
    "\n",
    "print(f\"\\nüìä Resumen de selecci√≥n:\")\n",
    "print(f\"   ‚Ä¢ Total de casos √∫nicos seleccionados: {len(selected_df)}\")\n",
    "print(f\"   ‚Ä¢ Categor√≠as cubiertas: {len(set(case_categories))}\")\n",
    "\n",
    "# Mostrar distribuci√≥n por categor√≠a\n",
    "print(\"\\nüìã Distribuci√≥n por categor√≠a:\")\n",
    "for category in selected_cases.keys():\n",
    "    count = sum(1 for c in case_categories if c == category)\n",
    "    if count > 0:\n",
    "        print(f\"   ‚Ä¢ {category}: {count} casos\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 3: PREPARAR CASOS PARA SHAP/LIME\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£ PREPARACI√ìN DE DATOS PARA EXPLICABILIDAD:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Crear estructura optimizada para an√°lisis\n",
    "explainability_cases = []\n",
    "\n",
    "for idx in selected_df.index:\n",
    "    row = selected_df.loc[idx]\n",
    "    case = {\n",
    "        'index': idx,\n",
    "        'text': row['text'],\n",
    "        'true_label': row['true_label'],\n",
    "        'predicted_label': row['predicted_label'],\n",
    "        'confidence': row['confidence'],\n",
    "        'prob_negative': row['prob_negative'],\n",
    "        'prob_positive': row['prob_positive'],\n",
    "        'correct': row['correct'],\n",
    "        'category': row['case_category'],\n",
    "        'text_length': len(row['text']),\n",
    "        'word_count': len(row['text'].split())\n",
    "    }\n",
    "    explainability_cases.append(case)\n",
    "\n",
    "# Convertir a DataFrame para f√°cil manipulaci√≥n\n",
    "explainability_df = pd.DataFrame(explainability_cases)\n",
    "\n",
    "print(f\"‚úÖ Datos preparados para an√°lisis de explicabilidad\")\n",
    "print(f\"   ‚Ä¢ Estructura creada con {len(explainability_df)} casos\")\n",
    "print(f\"   ‚Ä¢ Campos incluidos: {list(explainability_df.columns)}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 4: MOSTRAR EJEMPLOS DE CADA CATEGOR√çA\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£ EJEMPLOS DE CASOS SELECCIONADOS POR CATEGOR√çA:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Mostrar 1-2 ejemplos de cada categor√≠a principal\n",
    "categories_to_show = [\n",
    "    'high_confidence_errors',\n",
    "    'borderline_cases',\n",
    "    'false_positives',\n",
    "    'false_negatives'\n",
    "]\n",
    "\n",
    "for category in categories_to_show:\n",
    "    cases_in_category = explainability_df[explainability_df['category'] == category]\n",
    "    if len(cases_in_category) > 0:\n",
    "        print(f\"\\nüìå {category.upper().replace('_', ' ')}:\")\n",
    "        for i, (idx, case) in enumerate(cases_in_category.head(2).iterrows()):\n",
    "            print(f\"\\n  Caso {i+1}:\")\n",
    "            print(f\"    ‚Ä¢ Texto: '{case['text'][:80]}...'\")\n",
    "            print(f\"    ‚Ä¢ Real: {case['true_label']}, Predicho: {case['predicted_label']}\")\n",
    "            print(f\"    ‚Ä¢ Confianza: {case['confidence']:.3f}\")\n",
    "            print(f\"    ‚Ä¢ Prob [Neg/Pos]: [{case['prob_negative']:.3f}/{case['prob_positive']:.3f}]\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 5: GUARDAR SELECCI√ìN PARA USO POSTERIOR\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£ GUARDADO DE SELECCI√ìN:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Crear diccionario con los textos para f√°cil acceso\n",
    "selected_texts = {\n",
    "    category: [results_df.loc[idx]['text'] for idx in indices if idx in results_df.index]\n",
    "    for category, indices in selected_cases.items()\n",
    "}\n",
    "\n",
    "# Funci√≥n helper para obtener casos espec√≠ficos\n",
    "def get_cases_for_explanation(category=None, n=3):\n",
    "    \"\"\"\n",
    "    Obtiene casos espec√≠ficos para an√°lisis con SHAP/LIME\n",
    "    \n",
    "    Args:\n",
    "        category: Categor√≠a espec√≠fica o None para obtener variados\n",
    "        n: N√∫mero de casos a retornar\n",
    "    \n",
    "    Returns:\n",
    "        Lista de tuplas (texto, √≠ndice, informaci√≥n)\n",
    "    \"\"\"\n",
    "    if category and category in selected_cases:\n",
    "        indices = selected_cases[category][:n]\n",
    "    else:\n",
    "        # Selecci√≥n variada\n",
    "        indices = []\n",
    "        for cat, idx_list in selected_cases.items():\n",
    "            if idx_list:\n",
    "                indices.append(idx_list[0])\n",
    "            if len(indices) >= n:\n",
    "                break\n",
    "    \n",
    "    cases = []\n",
    "    for idx in indices:\n",
    "        if idx in results_df.index:\n",
    "            row = results_df.loc[idx]\n",
    "            cases.append({\n",
    "                'text': row['text'],\n",
    "                'index': idx,\n",
    "                'true_label': row['true_label'],\n",
    "                'predicted_label': row['predicted_label'],\n",
    "                'confidence': row['confidence'],\n",
    "                'category': next((cat for cat, idx_list in selected_cases.items() if idx in idx_list), 'unknown')\n",
    "            })\n",
    "    \n",
    "    return cases\n",
    "\n",
    "# Ejemplo de uso\n",
    "example_cases = get_cases_for_explanation(category='high_confidence_errors', n=2)\n",
    "print(f\"\\n‚úÖ Funci√≥n helper creada: get_cases_for_explanation()\")\n",
    "print(f\"   ‚Ä¢ Ejemplo: {len(example_cases)} casos obtenidos de 'high_confidence_errors'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ VARIABLES DISPONIBLES PARA SHAP/LIME:\")\n",
    "print(\"  ‚Ä¢ selected_df: DataFrame con todos los casos seleccionados\")\n",
    "print(\"  ‚Ä¢ explainability_df: DataFrame optimizado para an√°lisis\")\n",
    "print(\"  ‚Ä¢ selected_cases: Diccionario con √≠ndices por categor√≠a\")\n",
    "print(\"  ‚Ä¢ selected_texts: Diccionario con textos por categor√≠a\")\n",
    "print(\"  ‚Ä¢ get_cases_for_explanation(): Funci√≥n para obtener casos espec√≠ficos\")\n",
    "print(\"\\n‚úÖ Casos listos para an√°lisis de explicabilidad con SHAP y LIME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef50756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Celda 15 - GUARDAR SELECCI√ìN Y PREPARAR PARA LIME/SHAP\n",
    "# ============================================================\n",
    "# Esta celda guarda los casos seleccionados y prepara todo\n",
    "# para el an√°lisis de explicabilidad\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üíæ GUARDADO Y PREPARACI√ìN PARA AN√ÅLISIS DE EXPLICABILIDAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 1: CREAR DIRECTORIO PARA GUARDAR RESULTADOS\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£ PREPARANDO ESTRUCTURA DE ARCHIVOS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Crear directorio si no existe\n",
    "output_dir = \"explainability_analysis\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"‚úÖ Directorio creado: {output_dir}/\")\n",
    "else:\n",
    "    print(f\"üìÅ Usando directorio existente: {output_dir}/\")\n",
    "\n",
    "# Timestamp para versionado\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 2: GUARDAR CASOS SELECCIONADOS\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£ GUARDANDO CASOS SELECCIONADOS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Guardar el DataFrame con casos seleccionados\n",
    "csv_path = f\"{output_dir}/selected_cases_{timestamp}.csv\"\n",
    "selected_df.to_csv(csv_path, index=True)\n",
    "print(f\"‚úÖ DataFrame guardado: {csv_path}\")\n",
    "print(f\"   ‚Ä¢ {len(selected_df)} casos guardados\")\n",
    "\n",
    "# Guardar diccionario de casos como JSON\n",
    "json_path = f\"{output_dir}/cases_by_category_{timestamp}.json\"\n",
    "# Convertir a formato serializable\n",
    "cases_for_json = {\n",
    "    category: {\n",
    "        'indices': indices,\n",
    "        'count': len(indices),\n",
    "        'texts': [results_df.loc[idx]['text'][:200] if idx in results_df.index else None for idx in indices]\n",
    "    }\n",
    "    for category, indices in selected_cases.items()\n",
    "}\n",
    "\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(cases_for_json, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Categor√≠as guardadas: {json_path}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 3: CREAR ARCHIVO DE CONFIGURACI√ìN PARA LIME/SHAP\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£ CREANDO CONFIGURACI√ìN PARA EXPLICABILIDAD:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Seleccionar casos prioritarios para an√°lisis (top 3 de cada categor√≠a importante)\n",
    "priority_cases = []\n",
    "\n",
    "priority_categories = [\n",
    "    'high_confidence_errors',    # M√°s cr√≠ticos\n",
    "    'borderline_cases',          # M√°s inciertos\n",
    "    'false_positives',           # Errores tipo I\n",
    "    'false_negatives',           # Errores tipo II\n",
    "    'high_confidence_correct'    # Baseline\n",
    "]\n",
    "\n",
    "for category in priority_categories:\n",
    "    if category in selected_cases:\n",
    "        for idx in selected_cases[category][:3]:  # Top 3 de cada categor√≠a\n",
    "            if idx in results_df.index:\n",
    "                row = results_df.loc[idx]\n",
    "                priority_cases.append({\n",
    "                    'index': idx,\n",
    "                    'category': category,\n",
    "                    'text': row['text'],\n",
    "                    'true_label': int(row['true_label']),\n",
    "                    'predicted_label': int(row['predicted_label']),\n",
    "                    'confidence': float(row['confidence']),\n",
    "                    'correct': bool(row['correct'])\n",
    "                })\n",
    "\n",
    "# Guardar casos prioritarios\n",
    "priority_path = f\"{output_dir}/priority_cases_{timestamp}.json\"\n",
    "with open(priority_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(priority_cases, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Casos prioritarios guardados: {priority_path}\")\n",
    "print(f\"   ‚Ä¢ {len(priority_cases)} casos prioritarios seleccionados\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 4: CREAR FUNCI√ìN DE PREDICCI√ìN PARA LIME/SHAP\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£ PREPARANDO FUNCI√ìN DE PREDICCI√ìN PARA LIME/SHAP:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "def predict_proba_for_lime(texts):\n",
    "    \"\"\"\n",
    "    Funci√≥n de predicci√≥n compatible con LIME.\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista de textos a predecir\n",
    "    \n",
    "    Returns:\n",
    "        Array de probabilidades [prob_neg, prob_pos] para cada texto\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    # Hacer predicciones con el modelo\n",
    "    predictions = model.predict_batch(list(texts))\n",
    "    \n",
    "    # Extraer probabilidades en formato LIME\n",
    "    proba_array = []\n",
    "    for pred in predictions:\n",
    "        prob_neg = pred['probabilities']['NEGATIVE']\n",
    "        prob_pos = pred['probabilities']['POSITIVE']\n",
    "        proba_array.append([prob_neg, prob_pos])\n",
    "    \n",
    "    return np.array(proba_array)\n",
    "\n",
    "# Verificar que funciona\n",
    "test_text = \"This is a test sentence.\"\n",
    "test_proba = predict_proba_for_lime(test_text)\n",
    "print(f\"‚úÖ Funci√≥n de predicci√≥n creada y verificada\")\n",
    "print(f\"   ‚Ä¢ Input test: '{test_text}'\")\n",
    "print(f\"   ‚Ä¢ Output shape: {test_proba.shape}\")\n",
    "print(f\"   ‚Ä¢ Probabilidades: [Neg={test_proba[0][0]:.3f}, Pos={test_proba[0][1]:.3f}]\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 5: CREAR SCRIPT DE EJEMPLO PARA LIME\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£ CREANDO SCRIPT DE EJEMPLO PARA LIME:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "lime_script = f\"\"\"\n",
    "# Script de ejemplo para an√°lisis con LIME\n",
    "# Generado: {timestamp}\n",
    "\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "# Configurar LIME\n",
    "explainer = LimeTextExplainer(\n",
    "    class_names=['NEGATIVE', 'POSITIVE'],\n",
    "    split_expression=r'\\\\s+',  # Separar por espacios\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Casos prioritarios para analizar\n",
    "priority_cases = {priority_cases[:2]}  # Primeros 2 casos\n",
    "\n",
    "# Analizar cada caso\n",
    "for case in priority_cases:\n",
    "    text = case['text']\n",
    "    true_label = case['true_label']\n",
    "    predicted_label = case['predicted_label']\n",
    "    \n",
    "    print(f\"\\\\nAnalizando: '{{text[:100]}}...'\")\n",
    "    print(f\"Real: {{true_label}}, Predicho: {{predicted_label}}\")\n",
    "    \n",
    "    # Generar explicaci√≥n\n",
    "    explanation = explainer.explain_instance(\n",
    "        text,\n",
    "        predict_proba_for_lime,  # Tu funci√≥n de predicci√≥n\n",
    "        num_features=10,         # Top 10 palabras m√°s importantes\n",
    "        num_samples=500          # N√∫mero de perturbaciones\n",
    "    )\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(\"\\\\nPalabras m√°s importantes:\")\n",
    "    for word, importance in explanation.as_list():\n",
    "        print(f\"  ‚Ä¢ {{word}}: {{importance:.4f}}\")\n",
    "\"\"\"\n",
    "\n",
    "# Guardar script\n",
    "script_path = f\"{output_dir}/lime_example_script.py\"\n",
    "with open(script_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(lime_script)\n",
    "print(f\"‚úÖ Script de ejemplo guardado: {script_path}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 6: RESUMEN DE ARCHIVOS CREADOS\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n6Ô∏è‚É£ RESUMEN DE ARCHIVOS CREADOS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(f\"\\nüìÅ Directorio: {output_dir}/\")\n",
    "print(f\"  ‚Ä¢ selected_cases_{timestamp}.csv - DataFrame completo\")\n",
    "print(f\"  ‚Ä¢ cases_by_category_{timestamp}.json - Casos organizados por categor√≠a\")\n",
    "print(f\"  ‚Ä¢ priority_cases_{timestamp}.json - Casos prioritarios para LIME/SHAP\")\n",
    "print(f\"  ‚Ä¢ lime_example_script.py - Script de ejemplo\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# PASO 7: VERIFICAR INSTALACI√ìN DE LIME/SHAP\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n7Ô∏è‚É£ VERIFICACI√ìN DE HERRAMIENTAS DE EXPLICABILIDAD:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "tools_status = {}\n",
    "\n",
    "# Verificar LIME\n",
    "try:\n",
    "    import lime\n",
    "    tools_status['LIME'] = f\"‚úÖ Instalado (versi√≥n {lime.__version__})\"\n",
    "except ImportError:\n",
    "    tools_status['LIME'] = \"‚ùå No instalado - Ejecuta: pip install lime\"\n",
    "\n",
    "# Verificar SHAP\n",
    "try:\n",
    "    import shap\n",
    "    tools_status['SHAP'] = f\"‚úÖ Instalado (versi√≥n {shap.__version__})\"\n",
    "except ImportError:\n",
    "    tools_status['SHAP'] = \"‚ùå No instalado - Ejecuta: pip install shap\"\n",
    "\n",
    "for tool, status in tools_status.items():\n",
    "    print(f\"  ‚Ä¢ {tool}: {status}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# RESUMEN FINAL\n",
    "# --------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ PREPARACI√ìN COMPLETADA\")\n",
    "print(\"\\nüìä Resumen de casos guardados:\")\n",
    "total_cases = sum(len(indices) for indices in selected_cases.values())\n",
    "print(f\"  ‚Ä¢ Total de casos √∫nicos: {len(selected_df)}\")\n",
    "print(f\"  ‚Ä¢ Casos prioritarios: {len(priority_cases)}\")\n",
    "print(f\"  ‚Ä¢ Categor√≠as cubiertas: {len(selected_cases)}\")\n",
    "\n",
    "print(\"\\nüöÄ PR√ìXIMOS PASOS:\")\n",
    "print(\"  1. Instalar LIME/SHAP si no est√°n instalados\")\n",
    "print(\"  2. Usar predict_proba_for_lime() como funci√≥n de predicci√≥n\")\n",
    "print(\"  3. Ejecutar an√°lisis con los casos prioritarios\")\n",
    "print(\"  4. Visualizar explicaciones para cada categor√≠a\")\n",
    "\n",
    "print(\"\\nüì¶ VARIABLES DISPONIBLES:\")\n",
    "print(\"  ‚Ä¢ predict_proba_for_lime(): Funci√≥n compatible con LIME/SHAP\")\n",
    "print(\"  ‚Ä¢ priority_cases: Lista de casos m√°s importantes\")\n",
    "print(\"  ‚Ä¢ selected_df: DataFrame con todos los casos\")\n",
    "print(f\"  ‚Ä¢ Archivos en: {output_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
