"""
üîß Utilidades para Manejo de Tokens
M√≥dulo centralizado para filtrar y procesar tokens no sem√°nticos
"""

from typing import List, Set, Tuple
import numpy as np


class TokenFilter:
    """
    Clase para gestionar el filtrado de tokens no sem√°nticos
    """

    # Conjuntos de tokens a filtrar (definidos una sola vez)
    PUNCTUATION = {
        ',', '.', '!', '?', ';', ':', '-', '(', ')', '[', ']',
        '"', "'", '...', '--', '``', "''", '/', '\\', '{', '}',
        '<', '>', '|', '~', '`', '@', '#', '$', '%', '^', '&', '*',
        '+', '=', '_'
    }

    CONNECTORS = {
        # Art√≠culos
        'the', 'a', 'an',

        # Preposiciones
        'of', 'to', 'in', 'on', 'at', 'for', 'with', 'by', 'from',
        'as', 'into', 'about', 'after', 'before', 'between', 'through',

        # Conjunciones
        'and', 'or', 'but', 'nor', 'so', 'yet',

        # Verbos auxiliares comunes
        'is', 'are', 'was', 'were', 'be', 'been', 'being',
        'have', 'has', 'had',

        # Pronombres comunes (opcional - puedes comentar si quieres mantenerlos)
        'it', 'this', 'that', 'these', 'those'
    }

    # Tokens especiales de diferentes tokenizadores
    SPECIAL_TOKENS = {
        '[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]',  # BERT
        '<s>', '</s>', '<pad>', '<unk>', '<mask>',      # RoBERTa
        'ƒ†', '##', '‚ñÅ'                                  # Prefijos
    }

    @classmethod
    def is_semantic(cls, token: str) -> bool:
        """
        Determina si un token es sem√°nticamente significativo

        Args:
            token: Token a evaluar

        Returns:
            bool: True si el token es sem√°ntico, False si debe filtrarse
        """
        # Limpiar prefijos de tokenizador
        clean_token = token.replace('ƒ†', '').replace(
            '##', '').replace('‚ñÅ', '').lower().strip()

        # Verificar si es token vac√≠o
        if not clean_token:
            return False

        # Verificar si es puntuaci√≥n
        if clean_token in cls.PUNCTUATION:
            return False

        # Verificar si es conector
        if clean_token in cls.CONNECTORS:
            return False

        # Verificar si es token especial
        if clean_token in cls.SPECIAL_TOKENS:
            return False

        # Si pasa todas las pruebas, es sem√°ntico
        return True

    @classmethod
    def filter_importance_values(cls, tokens: List[str], importance_values: np.ndarray) -> np.ndarray:
        """
        Filtra valores de importancia poniendo en 0 los tokens no sem√°nticos

        Args:
            tokens: Lista de tokens
            importance_values: Array de valores de importancia

        Returns:
            np.ndarray: Array filtrado con 0s en posiciones no sem√°nticas
        """
        filtered_values = importance_values.copy()

        for i, token in enumerate(tokens):
            if i >= len(filtered_values):
                break

            if not cls.is_semantic(token):
                filtered_values[i] = 0.0

        return filtered_values

    @classmethod
    def get_semantic_indices(cls, tokens: List[str]) -> List[int]:
        """
        Obtiene √≠ndices de tokens sem√°nticamente significativos

        Args:
            tokens: Lista de tokens

        Returns:
            List[int]: Lista de √≠ndices de tokens sem√°nticos
        """
        return [i for i, token in enumerate(tokens) if cls.is_semantic(token)]

    @classmethod
    def filter_token_list(cls, tokens: List[str]) -> List[str]:
        """
        Filtra una lista de tokens, manteniendo solo los sem√°nticos

        Args:
            tokens: Lista de tokens original

        Returns:
            List[str]: Lista filtrada de tokens sem√°nticos
        """
        return [token for token in tokens if cls.is_semantic(token)]

    @classmethod
    def get_top_k_semantic_indices(cls, tokens: List[str], importance_values: np.ndarray,
                                   k: int) -> Tuple[List[int], np.ndarray]:
        """
        Obtiene los top-k √≠ndices de tokens sem√°nticos m√°s importantes

        Args:
            tokens: Lista de tokens
            importance_values: Array de valores de importancia
            k: N√∫mero de tokens a seleccionar

        Returns:
            Tuple[List[int], np.ndarray]: (√≠ndices top-k, valores filtrados)
        """
        # Primero filtrar valores poniendo 0 en no sem√°nticos
        filtered_values = cls.filter_importance_values(
            tokens, importance_values)

        # Obtener √≠ndices de tokens sem√°nticos
        semantic_indices = cls.get_semantic_indices(tokens)

        # Si hay menos tokens sem√°nticos que k, ajustar k
        k_adjusted = min(k, len(semantic_indices))

        if k_adjusted == 0:
            return [], filtered_values

        # Obtener top-k solo de tokens sem√°nticos
        # Filtrar solo valores sem√°nticos para argsort
        semantic_values = filtered_values[semantic_indices]

        # Obtener √≠ndices relativos en el array sem√°ntico
        top_k_relative = np.argsort(np.abs(semantic_values))[-k_adjusted:]

        # Convertir a √≠ndices absolutos en el array original
        top_k_absolute = [semantic_indices[i] for i in top_k_relative]

        return top_k_absolute, filtered_values

    @classmethod
    def clean_token(cls, token: str) -> str:
        """
        Limpia un token de prefijos de tokenizador

        Args:
            token: Token a limpiar

        Returns:
            str: Token limpio
        """
        return token.replace('ƒ†', '').replace('##', '').replace('‚ñÅ', '').strip()

    @classmethod
    def add_to_filters(cls, new_punctuation: Set[str] = None,
                       new_connectors: Set[str] = None):
        """
        Permite agregar tokens adicionales a los filtros

        Args:
            new_punctuation: Nuevos tokens de puntuaci√≥n
            new_connectors: Nuevos conectores
        """
        if new_punctuation:
            cls.PUNCTUATION.update(new_punctuation)

        if new_connectors:
            cls.CONNECTORS.update(new_connectors)
