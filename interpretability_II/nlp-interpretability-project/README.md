# üîç M√≥dulo II - Interpretabilidad en NLP con Transformers

Proyecto educativo enfocado en dominar t√©cnicas de interpretabilidad (SHAP y LIME) para modelos de lenguaje natural.

---

## üìÇ Estructura del Proyecto

```
interpretability_II/
‚îÇ
‚îú‚îÄ‚îÄ nlp-interpretability-project/     # üìö NOTEBOOKS DE APRENDIZAJE
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_model_evaluation.ipynb      # Evaluaci√≥n del modelo base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_shap_analysis.ipynb         # Implementaci√≥n y an√°lisis SHAP
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 04_lime_analysis.ipynb         # Implementaci√≥n y an√°lisis LIME
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/                               # Datasets y cache de modelos
‚îÇ   ‚îú‚îÄ‚îÄ explainability_analysis/            # Resultados de an√°lisis
‚îÇ   ‚îú‚îÄ‚îÄ models_cache/                       # Modelos descargados
‚îÇ   ‚îú‚îÄ‚îÄ results/                            # Outputs de experimentos
‚îÇ   ‚îî‚îÄ‚îÄ venv/                               # Entorno virtual local
‚îÇ
‚îî‚îÄ‚îÄ nlp-interpretability-dashboard/   # üöÄ DASHBOARD DEPLOYADO
    ‚îú‚îÄ‚îÄ app.py                              # Aplicaci√≥n Streamlit principal
    ‚îú‚îÄ‚îÄ requirements.txt                    # Dependencias para deployment
    ‚îú‚îÄ‚îÄ README.md                           # Documentaci√≥n del dashboard
    ‚îÇ
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ config/
        ‚îÇ   ‚îî‚îÄ‚îÄ config.yaml                 # Configuraci√≥n centralizada
        ‚îÇ
        ‚îú‚îÄ‚îÄ models/
        ‚îÇ   ‚îî‚îÄ‚îÄ model_loader.py             # Carga de modelos Transformer
        ‚îÇ
        ‚îî‚îÄ‚îÄ utils/
            ‚îú‚îÄ‚îÄ dashboard.py                # Funciones de visualizaci√≥n
            ‚îú‚îÄ‚îÄ data_loader.py              # Carga de datos
            ‚îú‚îÄ‚îÄ fidelity_explanation.py     # Validaci√≥n de explicaciones
            ‚îî‚îÄ‚îÄ [otros m√≥dulos]
```

---

## üéØ Objetivo del Proyecto

Responder la pregunta:
> **"¬øQu√© m√©todo de interpretabilidad (SHAP o LIME) proporciona explicaciones m√°s √∫tiles para modelos Transformer en an√°lisis de sentimientos, y en qu√© situaciones usar cada uno?"**

---

## üìä Componentes del Proyecto

### 1Ô∏è‚É£ **Notebooks de Aprendizaje** (`nlp-interpretability-project/`)

Notebooks Jupyter donde se implementaron y probaron las t√©cnicas:

#### **02_model_evaluation.ipynb**
- Carga y evaluaci√≥n de DistilBERT pre-entrenado
- M√©tricas: Accuracy, Precision, Recall, F1-Score
- An√°lisis de errores y casos extremos
- Dataset: SST-2 (Stanford Sentiment Treebank)

#### **03_shap_analysis.ipynb**
- Implementaci√≥n de SHAP para transformers
- C√°lculo de valores de Shapley para tokens
- Visualizaciones: waterfall plots, force plots
- An√°lisis de importancia global de palabras

#### **04_lime_analysis.ipynb**
- Implementaci√≥n de LIME para texto
- Configuraci√≥n de perturbaciones locales
- Comparaci√≥n de estabilidad entre ejecuciones
- Trade-offs: velocidad vs precisi√≥n

---

### 2Ô∏è‚É£ **Dashboard Interactivo** (`nlp-interpretability-dashboard/`)

Aplicaci√≥n Streamlit deployada en Hugging Face Spaces.

**üîó Demo en vivo:** [https://huggingface.co/spaces/gaxoblanco/nlp-interpretability-dashboard](https://huggingface.co/spaces/gaxoblanco/nlp-interpretability-dashboard)

#### **Funcionalidades:**

**Modelos disponibles:**
- DistilBERT (sentimientos binarios)
- RoBERTa (sentimientos binarios)
- DistilRoBERTa (6 emociones)
- BERT Emotion (6 emociones)

**M√©todos de explicaci√≥n:**
- Solo SHAP
- Solo LIME
- Ambos (comparaci√≥n lado a lado)

**Visualizaciones:**
- Predicci√≥n con confianza
- Importancia de palabras por m√©todo
- Comparaci√≥n SHAP vs LIME
- M√©tricas de validaci√≥n (fidelidad, correlaci√≥n)

**Casos de uso:**
- Input personalizado del usuario
- 5 ejemplos predefinidos (positivo, negativo, mixto, sarc√°stico, neutral)

---

## üõ†Ô∏è Stack Tecnol√≥gico

### Core ML
```
transformers==4.30.0    # HuggingFace Transformers
torch==2.0.0            # PyTorch backend
datasets==2.18.0        # Datasets de HuggingFace
```

### Interpretabilidad
```
shap==0.42.0            # SHAP explainer
lime==0.2.0.1           # LIME explainer
```

### Visualizaci√≥n & Dashboard
```
streamlit==1.22.0       # Framework del dashboard
matplotlib==3.7.1       # Visualizaciones b√°sicas
seaborn==0.12.2         # Visualizaciones estad√≠sticas
plotly==5.14.1          # Gr√°ficos interactivos
```

### Data Science
```
pandas==2.0.3           # Manipulaci√≥n de datos
numpy==1.24.3           # Operaciones num√©ricas
scikit-learn==1.2.2     # M√©tricas y utilidades
```

---

## üöÄ Inicio R√°pido

### **Opci√≥n 1: Probar el Dashboard Online** (Recomendado)

Visita directamente: [https://huggingface.co/spaces/gaxoblanco/nlp-interpretability-dashboard](https://huggingface.co/spaces/gaxoblanco/nlp-interpretability-dashboard)

---

### **Opci√≥n 2: Ejecutar Localmente**

#### **A) Dashboard Interactivo**

```bash
# 1. Clonar el repositorio
git clone [tu-repo]
cd interpretability_II/nlp-interpretability-dashboard

# 2. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Ejecutar dashboard
streamlit run app.py
```

El dashboard abrir√° en: `http://localhost:8501`

---

#### **B) Notebooks de Aprendizaje**

```bash
# 1. Ir a la carpeta de notebooks
cd interpretability_II/nlp-interpretability-project

# 2. Activar entorno virtual
source venv/bin/activate

# 3. Lanzar Jupyter
jupyter notebook

# 4. Abrir cualquier notebook:
#    - 02_model_evaluation.ipynb
#    - 03_shap_analysis.ipynb
#    - 04_lime_analysis.ipynb
```

---

## üìö Roadmap de Desarrollo (Completado)

| Semana | Objetivo | Entregable | Estado |
|--------|----------|------------|--------|
| **1-2** | Fundamentos te√≥ricos | Documentaci√≥n conceptual | ‚úÖ |
| **3** | Setup y evaluaci√≥n | `02_model_evaluation.ipynb` | ‚úÖ |
| **4** | Implementaci√≥n SHAP | `03_shap_analysis.ipynb` | ‚úÖ |
| **5** | Implementaci√≥n LIME | `04_lime_analysis.ipynb` | ‚úÖ |
| **6** | Dashboard | `app.py` deployado | ‚úÖ |
| **7** | Validaci√≥n | M√©tricas de fidelidad | ‚úÖ |


---

## üéì Aprendizajes Clave

### **SHAP vs LIME: ¬øCu√°ndo usar cada uno?**

#### **Usa SHAP cuando:**
- ‚úÖ Necesitas garant√≠as matem√°ticas formales
- ‚úÖ Quieres explicaciones globales (todo el dataset)
- ‚úÖ Importa m√°s la precisi√≥n que la velocidad
- ‚úÖ Necesitas consistencia perfecta entre ejecuciones

#### **Usa LIME cuando:**
- ‚úÖ Necesitas explicaciones r√°pidas (producci√≥n)
- ‚úÖ Solo te interesan explicaciones locales (instancia espec√≠fica)
- ‚úÖ Quieres explorar m√∫ltiples perturbaciones
- ‚úÖ El modelo es completamente black-box

#### **Usa ambos cuando:**
- ‚úÖ Quieres validar cruzada de explicaciones
- ‚úÖ Est√°s en fase de research/an√°lisis
- ‚úÖ Necesitas detectar artefactos del m√©todo

---

## üë®‚Äçüíª Autor

**Proyecto Educativo - M√≥dulo II**
- Plan de estudio en interpretabilidad de ML
- Desarrollado por Gaston Blanco
---

## üìÑ Licencia

MIT License - Proyecto educativo de c√≥digo abierto

---
