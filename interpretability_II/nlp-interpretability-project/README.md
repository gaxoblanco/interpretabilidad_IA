# ğŸ” Interpretabilidad en NLP con DistilBERT

Proyecto educativo del **MÃ³dulo II** enfocado en explicar predicciones de modelos Transformer usando **SHAP** y **LIME**.

# Para iniciar el dashboard interactivo:
```bash
streamlit run app.py
```
---

## ğŸ¯ Objetivo del Proyecto

Implementar y comparar dos tÃ©cnicas de interpretabilidad (SHAP y LIME) para explicar las predicciones de un modelo **DistilBERT** en la tarea de **anÃ¡lisis de sentimientos**.

### Pregunta Central
> **"Para este modelo de sentimientos, Â¿quÃ© mÃ©todo (SHAP o LIME) me da explicaciones mÃ¡s Ãºtiles y en quÃ© situaciones?"**

---

## ğŸ“Š Dataset

- **Fuente:** IMDb Movie Reviews (HuggingFace Datasets)
- **Tarea:** ClasificaciÃ³n binaria de sentimientos (Positivo/Negativo)
- **TamaÃ±o:** 50,000 reviews (25k train / 25k test)
- **Promedio de palabras por review:** ~230 tokens

---

## ğŸ§  Modelo Base

**DistilBERT** (`distilbert-base-uncased-finetuned-sst-2-english`)
- VersiÃ³n destilada de BERT (66% menos parÃ¡metros)
- Pre-entrenado en SST-2 (Stanford Sentiment Treebank)
- **Arquitectura:**
  - 6 capas Transformer
  - 12 attention heads por capa
  - 768 dimensiones de embedding
  - 66M parÃ¡metros totales

**MÃ©tricas esperadas en IMDb:**
- Accuracy: ~91-93%
- F1-Score: ~0.92

---

## ğŸ› ï¸ Stack TecnolÃ³gico

```
Python 3.9+
transformers 4.30.0    # HuggingFace Transformers
torch 2.0.0            # PyTorch
datasets 2.12.0        # HuggingFace Datasets
shap 0.42.0            # SHAP explainer
lime 0.2.0.1           # LIME explainer
streamlit 1.22.0       # Dashboard interactivo
pandas 2.0.0           # ManipulaciÃ³n de datos
matplotlib 3.7.0       # Visualizaciones
scikit-learn 1.2.0     # MÃ©tricas y utilidades
```

---

## ğŸ“ Estructura del Proyecto

```
nlp-interpretability-project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ model_loader.py          # Carga y predicciÃ³n con DistilBERT
â”‚   â”‚
â”‚   â”œâ”€â”€ interpretability/
â”‚   â”‚   â”œâ”€â”€ shap_analyzer.py         # Explicaciones SHAP
â”‚   â”‚   â””â”€â”€ lime_analyzer.py         # Explicaciones LIME
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ data_loader.py           # Carga del dataset IMDb
â”‚   â”‚   â””â”€â”€ text_preprocessor.py     # Preprocesamiento de texto
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â””â”€â”€ text_viz.py              # Visualizaciones de explicaciones
â”‚   â”‚
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ config.yaml              # Configuraciones centralizadas
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_shap_lime_toy_example.ipynb     # Ejemplo toy (datos tabulares)
â”‚   â”œâ”€â”€ 02_model_evaluation.ipynb          # EvaluaciÃ³n del modelo base
â”‚   â”œâ”€â”€ 03_shap_analysis.ipynb             # AnÃ¡lisis con SHAP
â”‚   â”œâ”€â”€ 04_lime_analysis.ipynb             # AnÃ¡lisis con LIME
â”‚   â”œâ”€â”€ 05_visualization.ipynb             # GalerÃ­a de visualizaciones
â”‚   â””â”€â”€ 06_validation.ipynb                # MÃ©tricas de validaciÃ³n
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ fundamentos.md                     # Conceptos teÃ³ricos respondidos
â”‚   â””â”€â”€ LEARNINGS.md                       # Insights finales del proyecto
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cache/                             # Cache de modelos y datasets
â”‚
â”œâ”€â”€ app.py                                 # Dashboard Streamlit
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ“– Uso BÃ¡sico

### PredicciÃ³n Simple

```python
from src.models.model_loader import ModelLoader

# Cargar modelo
model = ModelLoader(model_name="distilbert-base-uncased-finetuned-sst-2-english")

# Predecir sentimiento
text = "This movie was absolutely fantastic!"
result = model.predict(text)

print(f"PredicciÃ³n: {result['predictions'][0]}")  # 'POSITIVE'
print(f"Confianza: {result['probabilities'][0]:.2%}")  # 98.5%
```

### Explicar con SHAP

```python
from src.interpretability.shap_analyzer import SHAPAnalyzer

# Inicializar analizador
shap_analyzer = SHAPAnalyzer(model.model, model.tokenizer)

# Obtener explicaciÃ³n
explanation = shap_analyzer.explain_instance(text)

# Visualizar
shap_analyzer.plot_waterfall(explanation)
```

### Explicar con LIME

```python
from src.interpretability.lime_analyzer import LIMEAnalyzer

# Inicializar analizador
lime_analyzer = LIMEAnalyzer(model.model, model.tokenizer)

# Obtener explicaciÃ³n
explanation = lime_analyzer.explain_instance(text, num_features=10)

# Visualizar
lime_analyzer.plot_explanation(explanation)
```

---

## ğŸ¨ Dashboard Interactivo

Ejecutar la aplicaciÃ³n Streamlit:

```bash
streamlit run app.py
```

Funcionalidades:
- âœï¸ Ingresar texto personalizado para anÃ¡lisis
- ğŸ”® Ver predicciÃ³n y probabilidades
- ğŸ“Š Comparar explicaciones SHAP vs LIME lado a lado
- ğŸ¯ Identificar palabras mÃ¡s influyentes
- ğŸ“ˆ Visualizar importancia global de tokens

---

## ğŸ“š Roadmap de Desarrollo (7 Semanas)

| Semana | Objetivo | Entregable |
|--------|----------|------------|
| **1** | Fundamentos teÃ³ricos | `docs/fundamentos.md` |
| **2** | TeorÃ­a SHAP/LIME | `notebooks/01_toy_example.ipynb` |
| **3** | Setup del proyecto | `src/models/` + evaluaciÃ³n base |
| **4** | ImplementaciÃ³n SHAP | `src/interpretability/shap_analyzer.py` |
| **5** | ImplementaciÃ³n LIME | `src/interpretability/lime_analyzer.py` |
| **6** | VisualizaciÃ³n | `app.py` (Dashboard) |
| **7** | ValidaciÃ³n y anÃ¡lisis | `LEARNINGS.md` |

---

## ğŸ”¬ MetodologÃ­a de ComparaciÃ³n

### MÃ©tricas de EvaluaciÃ³n

1. **Fidelidad**: Â¿Las explicaciones reflejan fielmente el modelo?
2. **Estabilidad**: Â¿Explicaciones consistentes para textos similares?
3. **Eficiencia**: Tiempo de cÃ³mputo por explicaciÃ³n
4. **Interpretabilidad**: Facilidad de comprensiÃ³n humana

### Casos de Estudio

Analizaremos 10 casos que incluyen:
- Sentimientos claramente positivos/negativos
- Casos ambiguos o sarcÃ¡sticos
- Textos largos vs cortos
- Acuerdo y desacuerdo entre SHAP y LIME

---

## ğŸ“ Conceptos Clave Aprendidos

### SHAP (SHapley Additive exPlanations)
- Base en teorÃ­a de juegos (valores de Shapley)
- GarantÃ­as matemÃ¡ticas formales
- Explicaciones globales + locales
- MÃ¡s lento pero mÃ¡s riguroso

### LIME (Local Interpretable Model-agnostic Explanations)
- AproximaciÃ³n lineal local
- Basado en perturbaciones del input
- Solo explicaciones locales
- MÃ¡s rÃ¡pido pero estocÃ¡stico

### Transformers
- Mecanismo de self-attention
- Positional embeddings
- Multi-head attention
- Fine-tuning de modelos pre-entrenados

---

## âš ï¸ Limitaciones Conocidas

1. **SHAP en texto es lento**: ~30-60 seg por explicaciÃ³n
2. **LIME es estocÃ¡stico**: Resultados varÃ­an entre ejecuciones
3. **Contexto limitado**: DistilBERT tiene lÃ­mite de 512 tokens
4. **Masking vs Removal**: Diferentes estrategias de perturbaciÃ³n
5. **OOD (Out-of-distribution)**: Perturbaciones pueden crear textos irreales

---

## ğŸ“ Resultados Esperados

Al finalizar el proyecto, serÃ¡s capaz de:
- âœ… Explicar predicciones de cualquier modelo Transformer
- âœ… Identificar quÃ© palabras influyen mÃ¡s en las decisiones
- âœ… Comparar ventajas/desventajas de SHAP vs LIME
- âœ… Crear visualizaciones interpretables
- âœ… Validar calidad de explicaciones

---

## ğŸ”— Referencias

### Papers Fundamentales
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al. (2017)
- [LIME Paper](https://arxiv.org/abs/1602.04938) - Ribeiro et al. (2016)
- [SHAP Paper](https://arxiv.org/abs/1705.07874) - Lundberg & Lee (2017)
- [DistilBERT Paper](https://arxiv.org/abs/1910.01108) - Sanh et al. (2019)

### Recursos Adicionales
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [HuggingFace Course](https://huggingface.co/course)
- [SHAP Documentation](https://shap.readthedocs.io/)
- [LIME Documentation](https://lime-ml.readthedocs.io/)

---

## ğŸ‘¨â€ğŸ’» Autor

Proyecto educativo del **MÃ³dulo II: Interpretabilidad en NLP**

---

## ğŸ“„ Licencia

MIT License - Proyecto educativo de cÃ³digo abierto

---

## ğŸ¤ Contribuciones

Este es un proyecto educativo. Si encuentras errores o tienes sugerencias:
1. Abre un Issue describiendo el problema
2. PropÃ³n mejoras mediante Pull Requests
3. Comparte tus propios experimentos

---

## â­ï¸ PrÃ³ximos Pasos (MÃ³dulo III)

- Interpretabilidad en modelos generativos (GPT)
- Attention visualization
- Probing tasks
- Adversarial examples

---
