# ğŸ¯ GuÃ­a de Preguntas - Fundamentos MÃ³dulo II (Solo Conceptos Nuevos)

## Instrucciones
Estas son las preguntas sobre conceptos **nuevos** que no vimos en MÃ³dulo I. EnfÃ³cate en lo que necesitas para NLP con Transformers.

---

## ğŸ¤– BLOQUE 1: Arquitectura de Transformers (LO NUEVO)

### Conceptos Esenciales

**1.1** Â¿QuÃ© es el mecanismo de **self-attention**?
- Permite que una palabra "mire/compare" con otras en la misma oraciÃ³n. Con esto podemos darle un peso a cada palabra, igual que en interretavility_I las values tienen un "peso" que cuanto afectan para la decision final.

**1.2** Â¿QuÃ© es **multi-head attention**? Â¿Por quÃ© usar 12 "cabezas" en vez de 1?
- Utilizar multiples "cabezas" permite al modelo capturar diferentes tipos de relaciones entre palabras en paralelo. Cada cabeza puede enfocarse en distintos aspectos del contexto. Pero segun cada caso se puede usar mas o menos cabezas.

**1.3** Â¿CÃ³mo fluye el texto desde input hasta predicciÃ³n en un Transformer encoder para clasificaciÃ³n?
INPUT TEXT
    â†“
[TOKENIZER] â†’ [CLS] word1 word2 ... [SEP]
    â†“
[EMBEDDINGS] â†’ Vectores + Posiciones
    â†“
[LAYER 1] â†’ Multi-Head Attention + FFN
    â†“
[LAYER 2] â†’ Multi-Head Attention + FFN
    â†“
    ...
    â†“
[LAYER N] â†’ Multi-Head Attention + FFN
    â†“
[EXTRACT [CLS]] â†’ Vector de 768 dims
    â†“
[LINEAR + SOFTMAX] â†’ [prob_clase1, prob_clase2, ...]
    â†“
PREDICTION

[CLS] vector (768) â†’ Linear â†’ [2.3, -1.8] â†’ Softmax â†’ [0.92, 0.08]
                                                         â†‘      â†‘
                                                      Positivo Negativo

**1.4** Â¿QuÃ© es el token `[CLS]` y por quÃ© lo usamos para clasificaciÃ³n?
- Seria el puntero para marcar el inicio de la oracion, y es el que se usa para clasificacion porque contiene informacion de toda la oracion. Utiliza por dentro self-attention para "mirar" todas las palabras y resumir su significado en un solo vector.

**1.41** Â¿QuÃ© es el token `[SEP]` y por quÃ© lo usamos?
- Se usa para marcar el final de una oracion o separar inputs en tareas de pares de oraciones (ej. pregunta-respuesta). y asi facilitar que el modelo entienda los limites del texto.

**1.5** Â¿QuÃ© son **positional embeddings**? Â¿Por quÃ© los Transformers los necesitan?
- Como trabajamos en paralelo, necesitamos contar con la posicion de cada palabra en la oracion. Los positional embeddings son vectores que se suman a los word embeddings para darle al modelo informacion sobre el orden de las palabras.
   # PseudocÃ³digo simplificado
   tokens = ["[CLS]", "This", "movie", "is", "great", "[SEP]"]

   # Token IDs
   token_ids = [101, 2023, 3185, 2003, 2307, 102]

   # Token embeddings (768 dims cada uno)
   token_embeds = embedding_layer(token_ids)  # Shape: [6, 768]

   # Positional embeddings (una por posiciÃ³n)
   positions = [0, 1, 2, 3, 4, 5]
   pos_embeds = position_embedding_layer(positions)  # Shape: [6, 768]

   # Sumar ambos
   final_embeds = token_embeds + pos_embeds  # Shape: [6, 768]
---

### BERT vs DistilBERT

**1.6** Â¿CuÃ¡l es la diferencia principal entre BERT y DistilBERT? (capas, parÃ¡metros, velocidad)

**1.7** Â¿QuÃ© significa "destilar" conocimiento de un modelo grande a uno pequeÃ±o?

**1.8** Para nuestro proyecto, Â¿por quÃ© elegimos DistilBERT en vez de BERT completo?

---

### Fine-tuning (EspecÃ­fico para ClasificaciÃ³n)

**1.9** Â¿QuÃ© significa hacer **fine-tuning** de un modelo pre-entrenado?
- Pre-entrenamiento: Aprende lenguaje (semanas, millones de textos)
- Fine-tuning: Aprende tu tarea especÃ­fica (horas, miles de ejemplos)

**1.10** Â¿QuÃ© capa se agrega encima de DistilBERT para hacer clasificaciÃ³n de sentimientos?

**1.11** Â¿QuÃ© ventaja tiene usar un modelo pre-entrenado vs entrenar desde cero?

---

## ğŸ” BLOQUE 2: SHAP para NLP (LO NUEVO)

### Diferencias con SHAP Tabular

**2.1** En MÃ³dulo I usamos SHAP con XGBoost (datos tabulares). Â¿QuÃ© cambia al aplicarlo a texto?
- En texto, las "features" son palabras o tokens, no columnas numÃ©ricas. Al tener NÂ° variables (palabras), el espacio de combinaciones es mucho mayor. Usamos token masking en vez de valores nulos. Y el modelo es un Transformer, no un Ã¡rbol. Por esto ultimo usamos SHAP Explainer genÃ©rico, no TreeExplainer.
**2.2** Para un modelo de texto, Â¿las "features" son palabras, tokens o algo mÃ¡s?
- Son tokens, que pueden ser palabras completas o sub-palabras (subwords) dependiendo del tokenizador usado. Por ejemplo, "fantÃ¡stico" puede dividirse en "fan", "tÃ¡s" y "tico".

**2.3** Â¿CÃ³mo "enmascara" SHAP palabras en una oraciÃ³n para calcular importancia?
- Reemplaza palabras con un token especial (ej. `[MASK]`) o las elimina, y observa cÃ³mo cambia la predicciÃ³n del modelo. Esto ayuda a medir la contribuciÃ³n de cada palabra.

**2.4** Â¿QuÃ© estrategia de masking se usa con Transformers? (Â¿tokens en blanco, [MASK], eliminaciÃ³n?)
- Se usa el token `[MASK]` para reemplazar palabras, ya que los Transformers estÃ¡n entrenados para manejar este token y pueden inferir el contexto faltante.

**2.4.1** Â¿Que tipos de Tokenizers existen y cuÃ¡l usaremos? (word-level, subword, byte-level)
- Word-level: Cada palabra es una feature. Problemas con OOV y vocab grande.
- Subword: Divide palabras en partes (WordPiece, BPE). Balance entre vocab y OOV. Usaremos WordPiece.
- Byte-level: Cada byte es una feature. Resuelve OOV y multilingÃ¼ismo, pero es lento y complejo.
- subword (WordPiece) es la mejor opciÃ³n para nuestro proyecto.

- El tema de tokenizaciÃ³n es crucial para NLP. Cuanto mas se optimice este paso, mejores resultados se obtendrÃ¡n en las tareas posteriores a la hora de interpretar los modelos.

---

### SHAP con Transformers

**2.5** Â¿Usaremos `TreeExplainer` (como en MÃ³dulo I) o `Explainer` genÃ©rico? Â¿Por quÃ©?
- Usaremos `Explainer` genÃ©rico porque los Transformers no son modelos basados en Ã¡rboles. `TreeExplainer` estÃ¡ optimizado para modelos como XGBoost o Random Forest, pero no funciona bien con redes neuronales complejas como DistilBERT.

**2.6** Â¿QuÃ© significa un SHAP value de +0.5 para la palabra "excelente" en anÃ¡lisis de sentimientos?
- Significa que la palabra "excelente" contribuye positivamente a la predicciÃ³n de sentimiento positivo, aumentando la probabilidad de esa clase en 0.5 unidades en la escala log-odds del modelo.

**2.7** Â¿CÃ³mo agregamos SHAP values de mÃºltiples ejemplos para obtener importancia global de palabras?
- Calculamos el valor absoluto promedio de SHAP para cada palabra a lo largo de todos los ejemplos. Esto nos da una medida de la importancia global de cada palabra en el conjunto de datos.

---

## ğŸ‹ BLOQUE 3: LIME para Texto (LO NUEVO)

### Algoritmo de PerturbaciÃ³n

**3.1** Para texto, Â¿cÃ³mo "perturba" LIME una oraciÃ³n? Da un ejemplo concreto.

Ejemplo: `"This movie is absolutely fantastic"`
Â¿QuÃ© perturbaciones crearÃ­a LIME?
- `"This movie is absolutely [MASK]"`
- Para generar la perturbaciÃ³n, LIME enmascara palabras al azar en la oraciÃ³n original.

**3.2** Â¿Por quÃ© LIME usa un modelo lineal local si DistilBERT es sÃºper complejo?
- Porque LIME asume que cerca del punto de interÃ©s (la oraciÃ³n original), el comportamiento del modelo puede aproximarse linealmente. Esto simplifica la interpretaciÃ³n y permite entender quÃ© palabras son mÃ¡s importantes para esa predicciÃ³n especÃ­fica.

**3.3** Â¿CuÃ¡ntas perturbaciones genera LIME tÃ­picamente para una explicaciÃ³n? Â¿MÃ¡s es mejor?
- Usualmente entre 500 y 1000 perturbaciones. MÃ¡s perturbaciones pueden mejorar la estabilidad de la explicaciÃ³n, pero tambiÃ©n aumentan el tiempo de cÃ³mputo. Hay un punto de rendimientos decrecientes donde agregar mÃ¡s perturbaciones no mejora significativamente la explicaciÃ³n.
- Opciones: Feature Subtitution, Random Deletion, Synonym Replacement.
---

### ConfiguraciÃ³n para Transformers

**3.4** Â¿QuÃ© es el parÃ¡metro `num_features` en LIME? Â¿CuÃ¡nto usaremos (5, 10, 20)?
- `num_features` define cuÃ¡ntas palabras (features) se incluirÃ¡n en la explicaciÃ³n final. Usaremos 10 para equilibrar detalle y claridad, ya que demasiadas pueden hacer la explicaciÃ³n confusa.

**3.5** Â¿LIME da siempre las mismas explicaciones para el mismo input? Â¿Por quÃ© sÃ­ o no?
- No, LIME puede dar explicaciones ligeramente diferentes en cada ejecuciÃ³n debido a la naturaleza aleatoria de las perturbaciones que genera. Sin embargo, con un nÃºmero suficiente de perturbaciones, las explicaciones tienden a ser consistentes.
- Se analiza la estabilidad de las explicaciones ejecutando LIME varias veces y comparando los resultados.

**3.6** Â¿QuÃ© diferencia hay entre aplicar LIME a un texto corto (1 lÃ­nea) vs largo (pÃ¡rrafo)?
- En textos cortos, cada palabra tiene un impacto mÃ¡s significativo en la predicciÃ³n, por lo que las explicaciones pueden ser mÃ¡s claras y directas. En textos largos, la importancia de cada palabra puede diluirse, y LIME puede identificar mÃ¡s palabras como relevantes, lo que puede complicar la interpretaciÃ³n.
- Ejemplo, una palabra que se repite en un pÃ¡rrafo largo puede tener una importancia acumulada mayor que en una oraciÃ³n corta, y no ncesariamente deberÃ­a ser la mÃ¡s relevante para la predicciÃ³n.
---

## âš–ï¸ BLOQUE 4: SHAP vs LIME en NLP

### ComparaciÃ³n EspecÃ­fica para Texto

**4.1** Resume en una tabla para tu proyecto:
| Aspecto | SHAP (Transformers) | LIME (Texto) |
|---------|---------------------|--------------|
| Velocidad tÃ­pica | ? segundos | ? segundos |
| Aspecto | SHAP (Transformers) | LIME (Texto) |
|---------|---------------------|--------------|
| **Base teÃ³rica** | TeorÃ­a de juegos (Shapley values) | AproximaciÃ³n lineal local |
| **Scope** | Global + Local | Solo Local |
| **PerturbaciÃ³n** | Masking `[MASK]` | Removal (eliminar palabras) |
| **Num. perturbaciones** | Todas las coaliciones (aprox.) | 5,000 - 10,000 muestras |
| **Velocidad** | 30-60 seg/texto | 45-90 seg/texto (5k samples) |
| **Estabilidad** | âœ… Alta (determinÃ­stico) | ğŸŸ¡ Media (estocÃ¡stico) |
| **GarantÃ­as matemÃ¡ticas** | âœ… SÃ­ (propiedades formales) | âŒ No |
| **Out-of-distribution** | ğŸŸ¡ Moderado (BERT conoce [MASK]) | âŒ Alto (oraciones incompletas) |
| **Mejor para** | Importancia global + anÃ¡lisis riguroso | ExplicaciÃ³n rÃ¡pida individual |
| **LimitaciÃ³n** | Muy lento | Inestable, sin garantÃ­as |

**4.2** Si SHAP dice "excelente" es importante (+0.5) pero LIME dice "fantastic" (+0.8), Â¿cÃ³mo interpretas eso?

**4.3** Â¿CuÃ¡ndo preferirÃ­as SHAP sobre LIME en este proyecto de sentimientos?

**4.4** Â¿Para quÃ© usarÃ¡s cada uno en tu dashboard final?

---

## ğŸ¯ BLOQUE 5: ValidaciÃ³n de Explicaciones (LO NUEVO)

### MÃ©tricas que ImplementarÃ¡s

**5.1** Â¿QuÃ© es **fidelidad** de una explicaciÃ³n? Describe cÃ³mo la medirÃ¡s en cÃ³digo.

**5.2** PropÃ³n un test simple: Si elimino la palabra mÃ¡s importante segÃºn SHAP, Â¿quÃ© deberÃ­a pasar?

**5.3** Â¿CÃ³mo medirÃ­as si SHAP y LIME "estÃ¡n de acuerdo" en un ejemplo?

**5.4** Â¿QuÃ© experimento harÃ­as para verificar que no estÃ¡n dando explicaciones aleatorias?

---

## ğŸš€ BLOQUE 6: ImplementaciÃ³n PrÃ¡ctica

### Tu Stack TÃ©cnico

**6.1** Â¿QuÃ© librerÃ­a usarÃ¡s para cargar DistilBERT? (nombre exacto)

**6.2** Â¿QuÃ© dataset de HuggingFace usarÃ¡s? Â¿CuÃ¡ntos ejemplos?

**6.3** Lista las 3 clases principales que implementarÃ¡s:
   - `ModelLoader`: Â¿QuÃ© hace?
   - `SHAPAnalyzer`: Â¿QuÃ© hace?
   - `LIMEAnalyzer`: Â¿QuÃ© hace?

---

### Visualizaciones

**6.4** Describe 2 visualizaciones que crearÃ¡s:
   - Para SHAP: ?
   - Para LIME: ?

**6.5** En tu dashboard Streamlit, Â¿quÃ© podrÃ¡ hacer el usuario?

---

## âœ… Checklist de Completitud

- [ ] **BLOQUE 1:** Transformers (11 preguntas) - Arquitectura que no viste en MÃ³dulo I
- [ ] **BLOQUE 2:** SHAP para NLP (7 preguntas) - Diferencias con SHAP tabular
- [ ] **BLOQUE 3:** LIME para Texto (6 preguntas) - MÃ©todo completamente nuevo
- [ ] **BLOQUE 4:** ComparaciÃ³n especÃ­fica (4 preguntas) - Para tu proyecto
- [ ] **BLOQUE 5:** ValidaciÃ³n (4 preguntas) - MÃ©tricas nuevas
- [ ] **BLOQUE 6:** ImplementaciÃ³n (5 preguntas) - Tu cÃ³digo especÃ­fico

**Total: 37 preguntas** (enfocadas en lo nuevo)

---

## ğŸ“š Recursos MÃ­nimos Necesarios

**Para Transformers:**
- Tutorial: "The Illustrated Transformer" (jalammar.github.io)
- Paper: "Attention is All You Need" (solo secciÃ³n 3)
- Docs: HuggingFace DistilBERT model card

**Para SHAP/LIME en NLP:**
- SHAP docs: SecciÃ³n "Text models"
- LIME docs: `LimeTextExplainer` API
- Tutorial: SHAP text examples en GitHub oficial

---

## ğŸ¯ Siguiente Paso

1. Responde estas 37 preguntas (guarda en `docs/01_fundamentos_respuestas.md`)
2. Identifica las 3-5 preguntas mÃ¡s difÃ­ciles
3. Busca recursos especÃ­ficos para esas
4. Â¡Pasa a Semana 2 (ejemplo toy)!

---

## ğŸ’¡ Tip: Estrategia de Estudio

**Prioridad ALTA (responde primero):**
- Bloque 1: Preguntas 1.1, 1.3, 1.4, 1.9
- Bloque 2: Preguntas 2.1, 2.3, 2.5
- Bloque 3: Preguntas 3.1, 3.2
- Bloque 6: Todas

**Prioridad MEDIA:**
- El resto de Bloques 2 y 3
- Bloque 4 completo

**Prioridad BAJA (puedes aprender haciendo):**
- Bloque 5 (aprenderÃ¡s implementando)