# üéØ Gu√≠a de Preguntas - Fundamentos M√≥dulo II (Solo Conceptos Nuevos)

## Instrucciones
Estas son las preguntas sobre conceptos **nuevos** que no vimos en M√≥dulo I. Enf√≥cate en lo que necesitas para NLP con Transformers.

---

## ü§ñ BLOQUE 1: Arquitectura de Transformers (LO NUEVO)

### Conceptos Esenciales

**1.1** ¬øQu√© es el mecanismo de **self-attention**?
- Permite que una palabra "mire/compare" con otras en la misma oraci√≥n. Con esto podemos darle un peso a cada palabra, igual que en interretavility_I las values tienen un "peso" que cuanto afectan para la decision final.

**1.2** ¬øQu√© es **multi-head attention**? ¬øPor qu√© usar 12 "cabezas" en vez de 1?
- Utilizar multiples "cabezas" permite al modelo capturar diferentes tipos de relaciones entre palabras en paralelo. Cada cabeza puede enfocarse en distintos aspectos del contexto. Pero segun cada caso se puede usar mas o menos cabezas.

**1.3** ¬øC√≥mo fluye el texto desde input hasta predicci√≥n en un Transformer encoder para clasificaci√≥n?
INPUT TEXT
    ‚Üì
[TOKENIZER] ‚Üí [CLS] word1 word2 ... [SEP]
    ‚Üì
[EMBEDDINGS] ‚Üí Vectores + Posiciones
    ‚Üì
[LAYER 1] ‚Üí Multi-Head Attention + FFN
    ‚Üì
[LAYER 2] ‚Üí Multi-Head Attention + FFN
    ‚Üì
    ...
    ‚Üì
[LAYER N] ‚Üí Multi-Head Attention + FFN
    ‚Üì
[EXTRACT [CLS]] ‚Üí Vector de 768 dims
    ‚Üì
[LINEAR + SOFTMAX] ‚Üí [prob_clase1, prob_clase2, ...]
    ‚Üì
PREDICTION

[CLS] vector (768) ‚Üí Linear ‚Üí [2.3, -1.8] ‚Üí Softmax ‚Üí [0.92, 0.08]
                                                         ‚Üë      ‚Üë
                                                      Positivo Negativo

**1.4** ¬øQu√© es el token `[CLS]` y por qu√© lo usamos para clasificaci√≥n?
- Seria el puntero para marcar el inicio de la oracion, y es el que se usa para clasificacion porque contiene informacion de toda la oracion. Utiliza por dentro self-attention para "mirar" todas las palabras y resumir su significado en un solo vector.

**1.41** ¬øQu√© es el token `[SEP]` y por qu√© lo usamos?
- Se usa para marcar el final de una oracion o separar inputs en tareas de pares de oraciones (ej. pregunta-respuesta). y asi facilitar que el modelo entienda los limites del texto.

**1.5** ¬øQu√© son **positional embeddings**? ¬øPor qu√© los Transformers los necesitan?
- Como trabajamos en paralelo, necesitamos contar con la posicion de cada palabra en la oracion. Los positional embeddings son vectores que se suman a los word embeddings para darle al modelo informacion sobre el orden de las palabras.
   # Pseudoc√≥digo simplificado
   tokens = ["[CLS]", "This", "movie", "is", "great", "[SEP]"]

   # Token IDs
   token_ids = [101, 2023, 3185, 2003, 2307, 102]

   # Token embeddings (768 dims cada uno)
   token_embeds = embedding_layer(token_ids)  # Shape: [6, 768]

   # Positional embeddings (una por posici√≥n)
   positions = [0, 1, 2, 3, 4, 5]
   pos_embeds = position_embedding_layer(positions)  # Shape: [6, 768]

   # Sumar ambos
   final_embeds = token_embeds + pos_embeds  # Shape: [6, 768]
---

### BERT vs DistilBERT

**1.6** ¬øCu√°l es la diferencia principal entre BERT y DistilBERT? (capas, par√°metros, velocidad)

**1.7** ¬øQu√© significa "destilar" conocimiento de un modelo grande a uno peque√±o?

**1.8** Para nuestro proyecto, ¬øpor qu√© elegimos DistilBERT en vez de BERT completo?

---

### Fine-tuning (Espec√≠fico para Clasificaci√≥n)

**1.9** ¬øQu√© significa hacer **fine-tuning** de un modelo pre-entrenado?
- Pre-entrenamiento: Aprende lenguaje (semanas, millones de textos)
- Fine-tuning: Aprende tu tarea espec√≠fica (horas, miles de ejemplos)

**1.10** ¬øQu√© capa se agrega encima de DistilBERT para hacer clasificaci√≥n de sentimientos?

**1.11** ¬øQu√© ventaja tiene usar un modelo pre-entrenado vs entrenar desde cero?

---

## üîç BLOQUE 2: SHAP para NLP (LO NUEVO)

### Diferencias con SHAP Tabular

**2.1** En M√≥dulo I usamos SHAP con XGBoost (datos tabulares). ¬øQu√© cambia al aplicarlo a texto?
- En texto, las "features" son palabras o tokens, no columnas num√©ricas. Al tener N¬∞ variables (palabras), el espacio de combinaciones es mucho mayor. Usamos token masking en vez de valores nulos. Y el modelo es un Transformer, no un √°rbol. Por esto ultimo usamos SHAP Explainer gen√©rico, no TreeExplainer.
**2.2** Para un modelo de texto, ¬ølas "features" son palabras, tokens o algo m√°s?
- Son tokens, que pueden ser palabras completas o sub-palabras (subwords) dependiendo del tokenizador usado. Por ejemplo, "fant√°stico" puede dividirse en "fan", "t√°s" y "tico".

**2.3** ¬øC√≥mo "enmascara" SHAP palabras en una oraci√≥n para calcular importancia?
- Reemplaza palabras con un token especial (ej. `[MASK]`) o las elimina, y observa c√≥mo cambia la predicci√≥n del modelo. Esto ayuda a medir la contribuci√≥n de cada palabra.

**2.4** ¬øQu√© estrategia de masking se usa con Transformers? (¬øtokens en blanco, [MASK], eliminaci√≥n?)
- Se usa el token `[MASK]` para reemplazar palabras, ya que los Transformers est√°n entrenados para manejar este token y pueden inferir el contexto faltante.

**2.4.1** ¬øQue tipos de Tokenizers existen y cu√°l usaremos? (word-level, subword, byte-level)
- Word-level: Cada palabra es una feature. Problemas con OOV y vocab grande.
- Subword: Divide palabras en partes (WordPiece, BPE). Balance entre vocab y OOV. Usaremos WordPiece.
- Byte-level: Cada byte es una feature. Resuelve OOV y multiling√ºismo, pero es lento y complejo.
- subword (WordPiece) es la mejor opci√≥n para nuestro proyecto.

- El tema de tokenizaci√≥n es crucial para NLP. Cuanto mas se optimice este paso, mejores resultados se obtendr√°n en las tareas posteriores a la hora de interpretar los modelos.

---

### SHAP con Transformers

**2.5** ¬øUsaremos `TreeExplainer` (como en M√≥dulo I) o `Explainer` gen√©rico? ¬øPor qu√©?
- Usaremos `Explainer` gen√©rico porque los Transformers no son modelos basados en √°rboles. `TreeExplainer` est√° optimizado para modelos como XGBoost o Random Forest, pero no funciona bien con redes neuronales complejas como DistilBERT.

**2.6** ¬øQu√© significa un SHAP value de +0.5 para la palabra "excelente" en an√°lisis de sentimientos?
- Significa que la palabra "excelente" contribuye positivamente a la predicci√≥n de sentimiento positivo, aumentando la probabilidad de esa clase en 0.5 unidades en la escala log-odds del modelo.

**2.7** ¬øC√≥mo agregamos SHAP values de m√∫ltiples ejemplos para obtener importancia global de palabras?
- Calculamos el valor absoluto promedio de SHAP para cada palabra a lo largo de todos los ejemplos. Esto nos da una medida de la importancia global de cada palabra en el conjunto de datos.

---

## üçã BLOQUE 3: LIME para Texto (LO NUEVO)

### Algoritmo de Perturbaci√≥n

**3.1** Para texto, ¬øc√≥mo "perturba" LIME una oraci√≥n? Da un ejemplo concreto.

Ejemplo: `"This movie is absolutely fantastic"`
¬øQu√© perturbaciones crear√≠a LIME?
- `"This movie is absolutely [MASK]"`
- Para generar la perturbaci√≥n, LIME enmascara palabras al azar en la oraci√≥n original.

**3.2** ¬øPor qu√© LIME usa un modelo lineal local si DistilBERT es s√∫per complejo?
- Porque LIME asume que cerca del punto de inter√©s (la oraci√≥n original), el comportamiento del modelo puede aproximarse linealmente. Esto simplifica la interpretaci√≥n y permite entender qu√© palabras son m√°s importantes para esa predicci√≥n espec√≠fica.

**3.3** ¬øCu√°ntas perturbaciones genera LIME t√≠picamente para una explicaci√≥n? ¬øM√°s es mejor?
- Usualmente entre 500 y 1000 perturbaciones. M√°s perturbaciones pueden mejorar la estabilidad de la explicaci√≥n, pero tambi√©n aumentan el tiempo de c√≥mputo. Hay un punto de rendimientos decrecientes donde agregar m√°s perturbaciones no mejora significativamente la explicaci√≥n.
- Opciones: Feature Subtitution, Random Deletion, Synonym Replacement.
---

### Configuraci√≥n para Transformers

**3.4** ¬øQu√© es el par√°metro `num_features` en LIME? ¬øCu√°nto usaremos (5, 10, 20)?
- `num_features` define cu√°ntas palabras (features) se incluir√°n en la explicaci√≥n final. Usaremos 10 para equilibrar detalle y claridad, ya que demasiadas pueden hacer la explicaci√≥n confusa.

**3.5** ¬øLIME da siempre las mismas explicaciones para el mismo input? ¬øPor qu√© s√≠ o no?
- No, LIME puede dar explicaciones ligeramente diferentes en cada ejecuci√≥n debido a la naturaleza aleatoria de las perturbaciones que genera. Sin embargo, con un n√∫mero suficiente de perturbaciones, las explicaciones tienden a ser consistentes.
- Se analiza la estabilidad de las explicaciones ejecutando LIME varias veces y comparando los resultados.

**3.6** ¬øQu√© diferencia hay entre aplicar LIME a un texto corto (1 l√≠nea) vs largo (p√°rrafo)?
- En textos cortos, cada palabra tiene un impacto m√°s significativo en la predicci√≥n, por lo que las explicaciones pueden ser m√°s claras y directas. En textos largos, la importancia de cada palabra puede diluirse, y LIME puede identificar m√°s palabras como relevantes, lo que puede complicar la interpretaci√≥n.
- Ejemplo, una palabra que se repite en un p√°rrafo largo puede tener una importancia acumulada mayor que en una oraci√≥n corta, y no ncesariamente deber√≠a ser la m√°s relevante para la predicci√≥n.
---

## ‚öñÔ∏è BLOQUE 4: SHAP vs LIME en NLP

### Comparaci√≥n Espec√≠fica para Texto

**4.1** Resume en una tabla para tu proyecto:
| Aspecto | SHAP (Transformers) | LIME (Texto) |
|---------|---------------------|--------------|
| Velocidad t√≠pica | ? segundos | ? segundos |
| Aspecto | SHAP (Transformers) | LIME (Texto) |
|---------|---------------------|--------------|
| **Base te√≥rica** | Teor√≠a de juegos (Shapley values) | Aproximaci√≥n lineal local |
| **Scope** | Global + Local | Solo Local |
| **Perturbaci√≥n** | Masking `[MASK]` | Removal (eliminar palabras) |
| **Num. perturbaciones** | Todas las coaliciones (aprox.) | 5,000 - 10,000 muestras |
| **Velocidad** | 30-60 seg/texto | 45-90 seg/texto (5k samples) |
| **Estabilidad** | ‚úÖ Alta (determin√≠stico) | üü° Media (estoc√°stico) |
| **Garant√≠as matem√°ticas** | ‚úÖ S√≠ (propiedades formales) | ‚ùå No |
| **Out-of-distribution** | üü° Moderado (BERT conoce [MASK]) | ‚ùå Alto (oraciones incompletas) |
| **Mejor para** | Importancia global + an√°lisis riguroso | Explicaci√≥n r√°pida individual |
| **Limitaci√≥n** | Muy lento | Inestable, sin garant√≠as |

**4.2** Si SHAP dice "excelente" es importante (+0.5) pero LIME dice "fantastic" (+0.8), ¬øc√≥mo interpretas eso?
- LIME usa referencia local, por lo que "fantastic" puede ser m√°s relevante en ese contexto espec√≠fico. 
- SHAP da una visi√≥n m√°s global, donde "excelente" tiene un impacto consistente en muchas predicciones. 
- Ambos pueden ser correctos, pero reflejan diferentes perspectivas.

**4.3** ¬øCu√°ndo preferir√≠as SHAP sobre LIME en este proyecto de sentimientos?
- Segun el modelo podemos tener mas o menos sentimientos.
- SHAP suele dar predciones m√°s estables y usa el modelo completo. En cambio LIME reproduce el modelo obteniendo una destilaci√≥n del original.
- Ambos m√©todos son complementarios y pueden usarse juntos para obtener una visi√≥n m√°s completa del modelo, Suelen tomar palabras diferentes para definir su importancia.

**4.4** ¬øPara qu√© usar√°s cada uno en tu dashboard final?
- SHAP: Para mostrar la importancia global de palabras en el dataset y explicar predicciones individuales con rigor. (Mas de an√°lisis riguroso)
- LIME: Para ofrecer explicaciones r√°pidas y visuales de predicciones individuales, permitiendo a los usuarios explorar diferentes ejemplos f√°cilmente. (Mas de estudio exploratorio)

---

## üéØ BLOQUE 5: Validaci√≥n de Explicaciones

### M√©tricas que Implementar√°s

**5.1** ¬øQu√© es **fidelidad** de una explicaci√≥n? Describe c√≥mo la medir√°s en c√≥digo.
- Fidelidad mide qu√© tan bien una explicaci√≥n refleja el resultado del modelo (este caso trabajamos con emociones).
- Esta muy ligado al modelo original, y se mide usando la teoria de juegos de shapley.
- De momento entiendo que es un campo muy nuevo, por lo opte por implementar mi Validaci√≥n de explicaciones usando la m√©tricas.

**5.2** Prop√≥n un test simple: Si elimino la palabra m√°s importante seg√∫n SHAP, ¬øqu√© deber√≠a pasar?
- Es interesante, pero si elimino la "palabra"(token), con mayor SHAP value positivo, no necesariamente cambie la predicci√≥n a negativa, ya que va a depender del tokenizador.
- Sin embargo, esto se resuelve definiendo "eliminar" y "palabra" correctamente. Si "eliminar" significa reemplazar el token con `[MASK]` o con una palabra aleatorea, y "palabra" se refiere al conjunto de tokens que forman la "palabra", entonces al hacer esto, deber√≠amos observar una disminuci√≥n significativa en la probabilidad de la clase positiva.

**5.3** ¬øC√≥mo medir√≠as si SHAP y LIME "est√°n de acuerdo" en un ejemplo?
- Las comparo una al lado de la otra obvio... pero lo mas probable es que sean similares, por ello estarian de acuerdo. Una comparacion ya requiere Spearman. 
- Calculo la correlaci√≥n de Spearman entre los rankings de importancia de palabras dados por SHAP y LIME. Una alta correlaci√≥n indicar√≠a que ambos m√©todos est√°n de acuerdo en qu√© palabras son m√°s importantes para esa predicci√≥n.

**5.4** ¬øQu√© experimento har√≠as para verificar que no est√°n dando explicaciones aleatorias?
- Como tengo backgorund en Diseno Indsutrial. 
- Un dashboard interactivo donde el usuario pueda ejecutar los mas rapido y facil multiples explicaciones en diferentes ejemplos y comparar los resultados visualmente, tanto de models distintos como de metodos distintos (SHAP vs LIME) y visualiar el impacto de cada palabra en la prediccion. Facilitando re escribir la oracion y ver como cambian las explicaciones.

---

## üöÄ BLOQUE 6: Implementaci√≥n Pr√°ctica

### Tu Stack T√©cnico

**6.1** ¬øQu√© librer√≠a usar√°s para cargar DistilBERT? (nombre exacto)
- Transformers
** Usamos HuggingFace Transformers**: `from transformers import DistilBertTokenizer, DistilBertForSequenceClassification`

**6.2** ¬øQu√© dataset de HuggingFace usar√°s? ¬øCu√°ntos ejemplos?
- Usaremos el dataset `SST-2` (Stanford Sentiment Treebank) de HuggingFace,.
- En el dashboard trabaja con textos de entrada manual del usuario y 5 ejemplos predefinidos.
---

### Visualizaciones

**6.4** Describe 2 visualizaciones que crear√°s:
   - Para SHAP: 
        - Gr√°fico de barras horizontales mostrando las palabras m√°s importantes con sus SHAP values para una predicci√≥n espec√≠fica.
        - Gr√°fico de resumen (summary plot) que muestre la importancia global de las palabras en el dataset.
   - Para LIME: 
        - Gr√°fico de barras horizontales similar al de SHAP, pero mostrando las palabras m√°s importantes seg√∫n LIME para una predicci√≥n espec√≠fica.
        - Listado de palabras resaltadas en el texto original, coloreadas seg√∫n su importancia (positiva o negativa) seg√∫n LIME.

**6.5** En tu dashboard Streamlit, ¬øqu√© podr√° hacer el usuario?
- Ingresar su propio texto para an√°lisis de sentimientos.
- Seleccionar el modelo DistilBERT, RoBERTa, DistilRoBERTa y BERT Emotion.
- Ver explicaciones SHAP y LIME para su texto.
- Ver una comparacion entre SHAP y LIME.
- Ver una validaci√≥n de explicaciones basada en fidelidad y correlaci√≥n.

---

## ‚úÖ Checklist de Completitud

- [ ] **BLOQUE 1:** Transformers (11 preguntas) - Arquitectura que no viste en M√≥dulo I
- [ ] **BLOQUE 2:** SHAP para NLP (7 preguntas) - Diferencias con SHAP tabular
- [ ] **BLOQUE 3:** LIME para Texto (6 preguntas) - M√©todo completamente nuevo
- [ ] **BLOQUE 4:** Comparaci√≥n espec√≠fica (4 preguntas) - Para tu proyecto
- [ ] **BLOQUE 5:** Validaci√≥n (4 preguntas) - M√©tricas nuevas
- [ ] **BLOQUE 6:** Implementaci√≥n (5 preguntas) - Tu c√≥digo espec√≠fico

**Total: 37 preguntas** (enfocadas en lo nuevo)

---

## üìö Recursos M√≠nimos Necesarios

**Para Transformers:**
- Tutorial: "The Illustrated Transformer" (jalammar.github.io)
- Paper: "Attention is All You Need" (solo secci√≥n 3)
- Docs: HuggingFace DistilBERT model card

**Para SHAP/LIME en NLP:**
- SHAP docs: Secci√≥n "Text models"
- LIME docs: `LimeTextExplainer` API
- Tutorial: SHAP text examples en GitHub oficial

---

## üéØ Siguiente Paso

1. Responde estas 37 preguntas (guarda en `docs/01_fundamentos_respuestas.md`)
2. Identifica las 3-5 preguntas m√°s dif√≠ciles
3. Busca recursos espec√≠ficos para esas
4. ¬°Pasa a Semana 2 (ejemplo toy)!

---

## üí° Tip: Estrategia de Estudio

**Prioridad ALTA (responde primero):**
- Bloque 1: Preguntas 1.1, 1.3, 1.4, 1.9
- Bloque 2: Preguntas 2.1, 2.3, 2.5
- Bloque 3: Preguntas 3.1, 3.2
- Bloque 6: Todas

**Prioridad MEDIA:**
- El resto de Bloques 2 y 3
- Bloque 4 completo

**Prioridad BAJA (puedes aprender haciendo):**
- Bloque 5 (aprender√°s implementando)