# Interpretabilidad en Machine Learning

Programa de estudio progresivo enfocado en t√©cnicas de interpretabilidad para modelos de ML. Cada m√≥dulo implementa m√©todos espec√≠ficos sobre diferentes tipos de datos, documentando decisiones t√©cnicas y resultados experimentales.

---

## Estructura del Repositorio

```
interpretability_ml/
‚îú‚îÄ‚îÄ interpretability_I/     # XGBoost + SHAP (Datos Tabulares)
‚îú‚îÄ‚îÄ interpretability_II/    # DistilBERT + SHAP + LIME (NLP)
‚îú‚îÄ‚îÄ interpretability_III/   # An√°lisis de Activaciones Neuronales
‚îú‚îÄ‚îÄ interpretability_IV/    # CNN + GradCAM (Computer Vision)
‚îú‚îÄ‚îÄ roadmap.md             # Planificaci√≥n detallada por semanas
‚îî‚îÄ‚îÄ roadmap_study.md       # Referencias y notas de estudio
```

Cada carpeta de m√≥dulo contiene:
- `notebooks/`: An√°lisis exploratorio, entrenamiento, evaluaci√≥n
- `LEARNINGS.md`: Hallazgos, decisiones t√©cnicas, reflexiones
- `README.md`: Documentaci√≥n espec√≠fica del experimento

---

## M√≥dulo I: Clasificaci√≥n Tabular con XGBoost

**Dataset:** German Credit Data (1000 instancias, 20 features)  
**Problema:** Clasificaci√≥n binaria de riesgo crediticio

### Objetivo
Implementar y comparar m√©todos model-agnostic de interpretabilidad en un clasificador de gradient boosting.

### Modelos
- **Baseline:** Logistic Regression (interpretable por dise√±o)
- **Target:** XGBoost Classifier

### T√©cnicas de Interpretabilidad
1. **SHAP (SHapley Additive exPlanations)**
   - TreeExplainer para modelos tree-based
   - Valores de Shapley para atribuci√≥n de features
   - Visualizaciones: waterfall, force plots, dependence plots

2. **Counterfactual Explanations**
   - Generaci√≥n de instancias contrafactuales via optimizaci√≥n
   - Distancia euclidiana para encontrar la instancia m√°s cercana en clase opuesta

### M√©tricas de Evaluaci√≥n
- **Modelo:** Accuracy, Precision, Recall, ROC-AUC
- **Interpretabilidad:** Consistencia SHAP vs Feature Importance, coverage de counterfactuals

### Artefactos
- 4 notebooks: EDA, modelado, SHAP analysis, counterfactuals
- Dashboard Streamlit para predicci√≥n + explicaci√≥n
- Modelos serializados (pickle)

**Estado:** Completado

---

## M√≥dulo II: Clasificaci√≥n de Texto con Transformers

**Dataset:** IMDb Reviews (50k reviews, clasificaci√≥n binaria sentimiento)  
**Problema:** An√°lisis de sentimientos

### Objetivo
Adaptar t√©cnicas de interpretabilidad model-agnostic (SHAP, LIME) a modelos de lenguaje pre-entrenados y comparar con m√©todos espec√≠ficos de Transformers (attention weights).

### Modelo
- **DistilBERT** fine-tuned para clasificaci√≥n binaria
- 6 capas, 12 attention heads por capa
- Tokenizaci√≥n WordPiece

### T√©cnicas de Interpretabilidad

#### 1. SHAP para Texto
- **Kernel SHAP** con perturbed sampling de tokens
- Atribuci√≥n a nivel de token individual
- **Desaf√≠o:** Manejo de contexto secuencial (tokens no independientes)

#### 2. LIME (Local Interpretable Model-agnostic Explanations)
- Perturbaci√≥n de texto mediante eliminaci√≥n de palabras
- Modelo sustituto lineal local
- **Comparaci√≥n:** SHAP vs LIME en t√©rminos de estabilidad/fidelidad

#### 3. Attention Visualization
- Extracci√≥n de attention weights de capas espec√≠ficas
- Visualizaci√≥n de patrones de atenci√≥n token-to-token
- **Limitaci√≥n:** Attention ‚â† explanation (debate abierto en literatura)

### Plan de Implementaci√≥n (3 Fases - 7-8 semanas)

**Fase I: Fundamentos (Semanas 1-2)**
- Setup de modelo pre-entrenado + fine-tuning pipeline
- Implementaci√≥n de SHAP para texto
- Validaci√≥n con casos sint√©ticos

**Fase II: Implementaci√≥n Modular (Semanas 3-5)**
- `ModelLoader`: carga de DistilBERT + tokenizer
- `SHAPAnalyzer`: wrapper para Kernel SHAP con sampling estrat√©gico
- `LIMEAnalyzer`: perturbador de texto + modelo lineal local
- `AttentionVisualizer`: extractor de attention matrices

**Fase III: An√°lisis Comparativo (Semanas 6-8)**
- M√©tricas de fidelidad: ¬øqu√© m√©todo predice mejor la salida del modelo?
- An√°lisis de casos donde SHAP/LIME/Attention divergen
- Documentaci√≥n de trade-offs (costo computacional vs interpretabilidad)

**Estado:** Planificado

---

## M√≥dulo III: An√°lisis de Activaciones Neuronales

**Objetivo:** Investigar qu√© representan neuronas individuales en capas ocultas de redes profundas.

### Enfoque (a definir seg√∫n hallazgos de M√≥dulo II)
- **Opci√≥n A:** Probing tasks sobre representaciones de DistilBERT
- **Opci√≥n B:** Feature visualization en CNN (si se pivotea a visi√≥n)

### T√©cnicas Candidatas
1. **Neuron Probing**
   - Entrenar clasificadores lineales sobre activaciones de capas espec√≠ficas
   - Evaluar qu√© informaci√≥n ling√º√≠stica/visual captura cada capa

2. **Activation Maximization**
   - Optimizaci√≥n de inputs para maximizar activaci√≥n de neurona espec√≠fica
   - Visualizaci√≥n de features que activan cada neurona

3. **Causal Intervention**
   - Ablaci√≥n de neuronas/atenci√≥n heads
   - Medici√≥n de impacto en performance downstream

**Estado:** Pendiente de definici√≥n

---

## M√≥dulo IV: Interpretabilidad en Computer Vision

**Dataset:** Por definir (ImageNet subset, CIFAR-10, o dominio espec√≠fico)  
**Problema:** Clasificaci√≥n de im√°genes

### Objetivo
Implementar m√©todos de visualizaci√≥n de saliency para entender qu√© regiones de la imagen influyen en la predicci√≥n del modelo.

### Modelo
- CNN pre-entrenada (ResNet, EfficientNet, o ViT)

### T√©cnicas
1. **GradCAM (Gradient-weighted Class Activation Mapping)**
   - Gradientes de la clase predicha respecto a feature maps
   - Heatmap de importancia espacial

2. **Integrated Gradients**
   - Integraci√≥n de gradientes a lo largo de un path (baseline ‚Üí input)
   - Atribuci√≥n pixel-level

3. **Saliency Maps**
   - Gradiente de la predicci√≥n respecto al input
   - Variantes: SmoothGrad, Guided Backprop

### Evaluaci√≥n
- **Sanity checks:** Comparaci√≥n con random model/input
- **Deletion/Insertion curves:** M√©trica de fidelidad de saliency maps

**Estado:** Planificado

---

## Progreso General

| M√≥dulo | Tipo de Datos | Modelo | T√©cnicas | Estado |
|--------|--------------|--------|----------|--------|
| **I** | Tabular | XGBoost | SHAP, Counterfactuals | ‚úÖ Completado |
| **II** | Texto | DistilBERT | SHAP, LIME, Attention | üìã Planificado (8 sem) |
| **III** | Variable | TBD | Neuron Probing, Activation Max | üìã Por definir |
| **IV** | Im√°genes | CNN/ViT | GradCAM, Integrated Gradients | üìã Planificado |

---

## Stack T√©cnico por M√≥dulo

| M√≥dulo | ML Framework | Interpretabilidad | Visualizaci√≥n |
|--------|-------------|-------------------|---------------|
| I | XGBoost, Scikit-learn | SHAP | Matplotlib, Streamlit |
| II | Transformers (HF), PyTorch | SHAP, LIME, BertViz | Plotly, Streamlit |
| III | PyTorch | Captum (TBD) | Matplotlib |
| IV | PyTorch/TensorFlow | Captum, tf-explain | Matplotlib, PIL |

---

## Setup de Entorno

```bash
# Clonar repositorio
git clone <repo-url>
cd interpretability_ml

# Navegar a m√≥dulo espec√≠fico
cd interpretability_I

# Crear entorno virtual
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar notebooks o dashboard
jupyter notebook
# o
streamlit run app.py
```

### Limpieza de Notebooks (pre-commit)
```bash
jupyter nbconvert --clear-output --inplace notebooks/*.ipynb
```

---

## Recursos de Referencia

- **Roadmap detallado:** `roadmap.md` - Desglose por cap√≠tulos y semanas
- **Notas de estudio:** `roadmap_study.md` - Papers, recursos te√≥ricos
- **Aprendizajes por m√≥dulo:** `<modulo>/LEARNINGS.md` - Decisiones t√©cnicas y hallazgos

---

## Principios del Proyecto

1. **Reproducibilidad:** Seeds fijados, versiones de librer√≠as especificadas
2. **Modularidad:** C√≥digo reutilizable entre experimentos
3. **Documentaci√≥n:** Decisiones t√©cnicas justificadas en LEARNINGS.md
4. **Rigor:** Comparaci√≥n con baselines y m√©tricas de fidelidad

**√öltima actualizaci√≥n:** Octubre 2025 - M√≥dulo I completado