{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Notebook 02: An√°lisis de Activaciones\n",
    "\n",
    "## üéØ Objetivo\n",
    "\n",
    "Analizar en profundidad las activaciones de capas intermedias de ResNet18 para entender:\n",
    "- ¬øQu√© patrones captura cada capa?\n",
    "- ¬øC√≥mo var√≠an las activaciones entre capas?\n",
    "- ¬øCu√°l es la sparsity de las activaciones?\n",
    "- ¬øHay neuronas \"muertas\" o poco activas?\n",
    "- ¬øC√≥mo se distribuyen las activaciones?\n",
    "\n",
    "## üìã Contenido\n",
    "\n",
    "1. Setup y carga del modelo\n",
    "2. Extracci√≥n de activaciones en batch\n",
    "3. An√°lisis estad√≠stico por capa\n",
    "4. Visualizaci√≥n de distribuciones\n",
    "5. An√°lisis de sparsity\n",
    "6. Identificaci√≥n de neuronas muertas\n",
    "7. Comparaci√≥n entre clases\n",
    "8. Conclusiones y siguientes pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üì¶ 1. Configuraci√≥n de paths y imports b√°sicos\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detectar el directorio ra√≠z del proyecto\n",
    "current_dir = Path.cwd()\n",
    "print(f\"üìÅ Directorio actual: {current_dir}\")\n",
    "\n",
    "# Si estamos en notebooks/, subir un nivel\n",
    "if 'notebooks' in str(current_dir):\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "print(f\"üìÅ Directorio ra√≠z del proyecto: {project_root}\")\n",
    "\n",
    "# Agregar el directorio ra√≠z al path de Python\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"‚úÖ Agregado al sys.path: {project_root}\")\n",
    "\n",
    "# Verificar que src/ existe\n",
    "src_path = project_root / 'src'\n",
    "if src_path.exists():\n",
    "    print(f\"‚úÖ Directorio src encontrado: {src_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Directorio src NO encontrado en {src_path}\")\n",
    "\n",
    "# Verificar que analyze_neuron.py existe\n",
    "analyze_neuron_path = src_path / 'utils' / 'analyze_neuron.py'\n",
    "if analyze_neuron_path.exists():\n",
    "    print(f\"‚úÖ Archivo analyze_neuron.py encontrado\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: analyze_neuron.py NO encontrado\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ 1. Imports y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports est√°ndar\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "# Librer√≠as cient√≠ficas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuraci√≥n de estilos\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Configuraci√≥n de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Agregar src al path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "print(f\"‚úÖ Project root: {project_root}\")\n",
    "print(f\"‚úÖ Python version: {sys.version.split()[0]}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß 2. Cargar Modelo y Dataset\n",
    "\n",
    "Reutilizamos las clases implementadas en el Notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar nuestras utilidades\n",
    "from models.model_loader import ModelLoader\n",
    "from utils.image_loader import ImageLoader\n",
    "from utils.hooks import ActivationHook, get_layer_types\n",
    "\n",
    "# Configuraci√≥n\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "print(f\"üñ•Ô∏è  Device: {DEVICE}\")\n",
    "print(f\"üìä Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo ResNet18\n",
    "model_loader = ModelLoader(\n",
    "    model_name='resnet18',\n",
    "    pretrained=True,\n",
    "    device=str(DEVICE)\n",
    ")\n",
    "\n",
    "model = model_loader.load_model()\n",
    "model.eval()  # Modo evaluaci√≥n\n",
    "\n",
    "# Mostrar informaci√≥n del modelo\n",
    "arch_info = model_loader.get_architecture_info()\n",
    "print(f\"\\nüìä Modelo: ResNet18\")\n",
    "print(f\"   - Par√°metros totales: {arch_info['total_params']:,}\")\n",
    "print(f\"   - Tama√±o: {arch_info['model_size_mb']:.2f} MB\")\n",
    "print(f\"   - Capas: {arch_info['num_layers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset CIFAR-10\n",
    "image_loader = ImageLoader(\n",
    "    dataset_name='cifar10',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader, test_loader = image_loader.get_dataloaders()\n",
    "dataset_info = image_loader.get_dataset_info()\n",
    "\n",
    "print(f\"\\nüì¶ Dataset: {dataset_info['name'].upper()}\")\n",
    "print(f\"   - Clases: {dataset_info['num_classes']}\")\n",
    "print(f\"   - Train samples: {dataset_info['train_samples']:,}\")\n",
    "print(f\"   - Test samples: {dataset_info['test_samples']:,}\")\n",
    "print(f\"   - Image size: {dataset_info['image_size']}\")\n",
    "print(f\"\\nüè∑Ô∏è  Clases: {', '.join(dataset_info['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ 3. Seleccionar Capas Objetivo para An√°lisis\n",
    "\n",
    "Vamos a analizar capas estrat√©gicas de ResNet18:\n",
    "- **conv1**: Primera capa convolucional (features de bajo nivel)\n",
    "- **layer1**: Primer bloque residual\n",
    "- **layer2**: Segundo bloque residual\n",
    "- **layer3**: Tercer bloque residual\n",
    "- **layer4**: Cuarto bloque residual (features de alto nivel)\n",
    "- **fc**: Capa fully connected final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener estructura de capas del modelo\n",
    "layer_types = get_layer_types(model)\n",
    "\n",
    "print(\"üîç Tipos de capas en ResNet18:\")\n",
    "for layer_type, layers in layer_types.items():\n",
    "    print(f\"   - {layer_type}: {len(layers)} capas\")\n",
    "    if len(layers) <= 5:\n",
    "        print(f\"     {layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir capas objetivo para an√°lisis\n",
    "# Seleccionamos capas conv2d estrat√©gicas de cada bloque\n",
    "TARGET_LAYERS = [\n",
    "    'relu',                    # ReLU despu√©s de conv1: 64 filtros\n",
    "    'layer1.0.relu',          # ReLU del bloque 1.0\n",
    "    'layer1.1.relu',          # ReLU del bloque 1.1\n",
    "    'layer2.0.relu',          # ReLU del bloque 2.0\n",
    "    'layer2.1.relu',          # ReLU del bloque 2.1\n",
    "    'layer3.0.relu',          # ReLU del bloque 3.0\n",
    "    'layer3.1.relu',          # ReLU del bloque 3.1\n",
    "    'layer4.0.relu',          # ReLU del bloque 4.0\n",
    "    'layer4.1.relu',          # ReLU del bloque 4.1\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ Capas seleccionadas para an√°lisis: {len(TARGET_LAYERS)}\")\n",
    "for i, layer in enumerate(TARGET_LAYERS, 1):\n",
    "    print(f\"   {i}. {layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîå 4. Registrar Hooks y Extraer Activaciones\n",
    "\n",
    "Usamos la clase `ActivationHook` para capturar activaciones de las capas seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear hook para capturar activaciones\n",
    "activation_hook = ActivationHook(model, target_layers=TARGET_LAYERS)\n",
    "activation_hook.register_hooks(capture_gradients=False)\n",
    "\n",
    "print(f\"‚úÖ Hooks registrados en {len(TARGET_LAYERS)} capas\")\n",
    "print(f\"üìã Capas monitoreadas: {activation_hook.get_layer_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para extraer activaciones de un batch\n",
    "def extract_activations_from_batch(\n",
    "    model: nn.Module,\n",
    "    hook: ActivationHook,\n",
    "    images: torch.Tensor,\n",
    "    device: torch.device\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Extrae activaciones de un batch de im√°genes.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de PyTorch\n",
    "        hook: ActivationHook registrado\n",
    "        images: Batch de im√°genes [B, C, H, W]\n",
    "        device: Device (CPU/GPU)\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con activaciones por capa\n",
    "    \"\"\"\n",
    "    # Limpiar activaciones previas\n",
    "    hook.clear_activations()\n",
    "    \n",
    "    # Forward pass\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(images)\n",
    "    \n",
    "    # Obtener activaciones (se mantienen en GPU para eficiencia)\n",
    "    activations = hook.get_activations()\n",
    "    \n",
    "    return activations\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de extracci√≥n definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä 5. Extraer Activaciones de M√∫ltiples Batches\n",
    "\n",
    "Para obtener estad√≠sticas robustas, extraemos activaciones de varios batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de extracci√≥n\n",
    "NUM_BATCHES_TO_ANALYZE = 10  # Analizar 10 batches (640 im√°genes)\n",
    "SAMPLE_SIZE = NUM_BATCHES_TO_ANALYZE * BATCH_SIZE\n",
    "\n",
    "print(f\"üî¢ Configuraci√≥n de extracci√≥n:\")\n",
    "print(f\"   - Batches a analizar: {NUM_BATCHES_TO_ANALYZE}\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Total de im√°genes: {SAMPLE_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura para acumular activaciones\n",
    "# accumulated_activations[layer_name] = lista de tensors\n",
    "accumulated_activations = defaultdict(list)\n",
    "accumulated_labels = []\n",
    "\n",
    "print(\"\\nüöÄ Extrayendo activaciones...\")\n",
    "\n",
    "# Iterar sobre batches del test set\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(test_loader, total=NUM_BATCHES_TO_ANALYZE)):\n",
    "        if batch_idx >= NUM_BATCHES_TO_ANALYZE:\n",
    "            break\n",
    "        \n",
    "        # Extraer activaciones de este batch\n",
    "        batch_activations = extract_activations_from_batch(\n",
    "            model, activation_hook, images, DEVICE\n",
    "        )\n",
    "        \n",
    "        # Acumular activaciones (mantener en GPU por ahora)\n",
    "        for layer_name, activation in batch_activations.items():\n",
    "            accumulated_activations[layer_name].append(activation.detach())\n",
    "        \n",
    "        # Acumular labels\n",
    "        accumulated_labels.extend(labels.tolist())\n",
    "\n",
    "print(f\"\\n‚úÖ Activaciones extra√≠das de {len(accumulated_labels)} im√°genes\")\n",
    "print(f\"üìä Capas analizadas: {len(accumulated_activations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar activaciones de todos los batches\n",
    "# Ahora cada capa tendr√° un tensor de shape [total_images, ...]\n",
    "print(\"üîÑ Concatenando activaciones...\")\n",
    "\n",
    "concatenated_activations = {}\n",
    "for layer_name, activation_list in accumulated_activations.items():\n",
    "    concatenated_activations[layer_name] = torch.cat(activation_list, dim=0)\n",
    "\n",
    "print(\"\\nüìê Shapes de activaciones concatenadas:\")\n",
    "for layer_name, activation in concatenated_activations.items():\n",
    "    print(f\"   {layer_name}: {list(activation.shape)}\")\n",
    "\n",
    "# Convertir labels a numpy\n",
    "labels_array = np.array(accumulated_labels)\n",
    "print(f\"\\nüè∑Ô∏è  Labels shape: {labels_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà 6. An√°lisis Estad√≠stico de Activaciones\n",
    "\n",
    "Calculamos estad√≠sticas clave para cada capa:\n",
    "- Media y desviaci√≥n est√°ndar\n",
    "- Valores m√≠nimos y m√°ximos\n",
    "- Sparsity (porcentaje de activaciones == 0)\n",
    "- N√∫mero de neuronas activas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para calcular estad√≠sticas detalladas\n",
    "def compute_activation_statistics(\n",
    "    activations: Dict[str, torch.Tensor]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula estad√≠sticas completas de activaciones por capa.\n",
    "    \n",
    "    Args:\n",
    "        activations: Diccionario con activaciones por capa\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con estad√≠sticas por capa\n",
    "    \"\"\"\n",
    "    stats_list = []\n",
    "    \n",
    "    for layer_name, activation in activations.items():\n",
    "        # Convertir a numpy para c√°lculos (mover a CPU si est√° en GPU)\n",
    "        act_np = activation.cpu().numpy()\n",
    "        \n",
    "        # Calcular estad√≠sticas\n",
    "        stats = {\n",
    "            'layer': layer_name,\n",
    "            'shape': str(list(activation.shape)),\n",
    "            'total_elements': activation.numel(),\n",
    "            'mean': float(act_np.mean()),\n",
    "            'std': float(act_np.std()),\n",
    "            'min': float(act_np.min()),\n",
    "            'max': float(act_np.max()),\n",
    "            'median': float(np.median(act_np.flatten())),\n",
    "            'q25': float(np.percentile(act_np.flatten(), 25)),\n",
    "            'q75': float(np.percentile(act_np.flatten(), 75)),\n",
    "            'sparsity_%': float((act_np == 0).sum() / act_np.size * 100),\n",
    "            'active_neurons': int((act_np != 0).any(axis=(0, 2, 3)).sum() if len(act_np.shape) == 4 else (act_np != 0).any(axis=0).sum()),\n",
    "            'total_neurons': activation.shape[1] if len(activation.shape) == 4 else activation.shape[-1],\n",
    "        }\n",
    "        \n",
    "        stats_list.append(stats)\n",
    "    \n",
    "    return pd.DataFrame(stats_list)\n",
    "\n",
    "# Calcular estad√≠sticas\n",
    "stats_df = compute_activation_statistics(concatenated_activations)\n",
    "\n",
    "print(\"\\nüìä Estad√≠sticas de Activaciones por Capa:\")\n",
    "print(\"=\" * 80)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar estad√≠sticas clave\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Media de activaciones por capa\n",
    "axes[0, 0].bar(range(len(stats_df)), stats_df['mean'], color='steelblue')\n",
    "axes[0, 0].set_xticks(range(len(stats_df)))\n",
    "axes[0, 0].set_xticklabels(stats_df['layer'], rotation=45, ha='right')\n",
    "axes[0, 0].set_ylabel('Media')\n",
    "axes[0, 0].set_title('Media de Activaciones por Capa')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Desviaci√≥n est√°ndar por capa\n",
    "axes[0, 1].bar(range(len(stats_df)), stats_df['std'], color='coral')\n",
    "axes[0, 1].set_xticks(range(len(stats_df)))\n",
    "axes[0, 1].set_xticklabels(stats_df['layer'], rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Desviaci√≥n Est√°ndar')\n",
    "axes[0, 1].set_title('Desviaci√≥n Est√°ndar por Capa')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Sparsity por capa\n",
    "axes[1, 0].bar(range(len(stats_df)), stats_df['sparsity_%'], color='mediumseagreen')\n",
    "axes[1, 0].set_xticks(range(len(stats_df)))\n",
    "axes[1, 0].set_xticklabels(stats_df['layer'], rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Sparsity (%)')\n",
    "axes[1, 0].set_title('Sparsity de Activaciones por Capa')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Rango (max - min) por capa\n",
    "activation_range = stats_df['max'] - stats_df['min']\n",
    "axes[1, 1].bar(range(len(stats_df)), activation_range, color='orchid')\n",
    "axes[1, 1].set_xticks(range(len(stats_df)))\n",
    "axes[1, 1].set_xticklabels(stats_df['layer'], rotation=45, ha='right')\n",
    "axes[1, 1].set_ylabel('Rango')\n",
    "axes[1, 1].set_title('Rango de Activaciones (Max - Min) por Capa')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä 7. Distribuciones de Activaciones\n",
    "\n",
    "Visualizamos la distribuci√≥n de activaciones para entender mejor su comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar algunas capas representativas para visualizar distribuciones\n",
    "LAYERS_TO_PLOT = ['relu', 'layer1.1.relu', 'layer2.1.relu', 'layer3.1.relu', 'layer4.1.relu']\n",
    "\n",
    "print(f\"üìä Capas seleccionadas para visualizar distribuciones: {len(LAYERS_TO_PLOT)}\")\n",
    "for layer in LAYERS_TO_PLOT:\n",
    "    if layer in concatenated_activations:\n",
    "        print(f\"   ‚úÖ {layer}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {layer} - NO ENCONTRADA\")\n",
    "        \n",
    "fig, axes = plt.subplots(len(LAYERS_TO_PLOT), 2, figsize=(15, 4 * len(LAYERS_TO_PLOT)))\n",
    "\n",
    "for idx, layer_name in enumerate(LAYERS_TO_PLOT):\n",
    "    if layer_name not in concatenated_activations:\n",
    "        continue\n",
    "    \n",
    "    activation = concatenated_activations[layer_name].cpu().numpy()\n",
    "    activation_flat = activation.flatten()\n",
    "    \n",
    "    # Histograma\n",
    "    axes[idx, 0].hist(activation_flat, bins=100, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    axes[idx, 0].set_xlabel('Valor de Activaci√≥n')\n",
    "    axes[idx, 0].set_ylabel('Frecuencia')\n",
    "    axes[idx, 0].set_title(f'Distribuci√≥n de Activaciones - {layer_name}')\n",
    "    axes[idx, 0].set_yscale('log')  # Escala log para mejor visualizaci√≥n\n",
    "    axes[idx, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    # Tomar muestra para box plot (demasiados puntos pueden ser lentos)\n",
    "    sample_size = min(10000, len(activation_flat))\n",
    "    sample_indices = np.random.choice(len(activation_flat), sample_size, replace=False)\n",
    "    activation_sample = activation_flat[sample_indices]\n",
    "    \n",
    "    axes[idx, 1].boxplot(activation_sample, vert=True, patch_artist=True,\n",
    "                         boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "    axes[idx, 1].set_ylabel('Valor de Activaci√≥n')\n",
    "    axes[idx, 1].set_title(f'Box Plot - {layer_name}')\n",
    "    axes[idx, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observaciones:\")\n",
    "print(\"   - Capas tempranas tienden a tener distribuciones m√°s amplias\")\n",
    "print(\"   - Capas profundas pueden mostrar mayor sparsity\")\n",
    "print(\"   - La escala logar√≠tmica ayuda a visualizar valores peque√±os\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç 8. An√°lisis de Sparsity y Neuronas Muertas\n",
    "\n",
    "Identificamos neuronas que nunca se activan (o muy raramente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para identificar neuronas muertas\n",
    "def find_dead_neurons_detailed(\n",
    "    activations: Dict[str, torch.Tensor],\n",
    "    threshold: float = 1e-6\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Identifica neuronas \"muertas\" (que nunca se activan significativamente).\n",
    "    \n",
    "    Args:\n",
    "        activations: Diccionario con activaciones por capa\n",
    "        threshold: Umbral para considerar una neurona como activa\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con informaci√≥n de neuronas muertas por capa\n",
    "    \"\"\"\n",
    "    dead_neurons_info = {}\n",
    "    \n",
    "    for layer_name, activation in activations.items():\n",
    "        act_np = activation.cpu().numpy()\n",
    "        \n",
    "        # Para capas conv: [B, C, H, W] -> analizar por canal\n",
    "        if len(act_np.shape) == 4:\n",
    "            # M√°ximo de cada neurona (canal) a trav√©s de batch y spatial dims\n",
    "            max_per_neuron = act_np.max(axis=(0, 2, 3))\n",
    "        # Para capas fc: [B, N] -> analizar por neurona\n",
    "        else:\n",
    "            max_per_neuron = act_np.max(axis=0)\n",
    "        \n",
    "        # Identificar neuronas muertas\n",
    "        dead_mask = max_per_neuron <= threshold\n",
    "        dead_indices = np.where(dead_mask)[0]\n",
    "        \n",
    "        total_neurons = len(max_per_neuron)\n",
    "        dead_count = len(dead_indices)\n",
    "        dead_percentage = (dead_count / total_neurons * 100) if total_neurons > 0 else 0\n",
    "        \n",
    "        dead_neurons_info[layer_name] = {\n",
    "            'total_neurons': total_neurons,\n",
    "            'dead_neurons': dead_count,\n",
    "            'dead_percentage': dead_percentage,\n",
    "            'dead_indices': dead_indices.tolist(),\n",
    "            'max_activations': max_per_neuron.tolist()\n",
    "        }\n",
    "    \n",
    "    return dead_neurons_info\n",
    "\n",
    "# Analizar neuronas muertas\n",
    "dead_neurons_info = find_dead_neurons_detailed(concatenated_activations, threshold=1e-6)\n",
    "\n",
    "print(\"\\nüîç An√°lisis de Neuronas Muertas:\")\n",
    "print(\"=\" * 80)\n",
    "for layer_name, info in dead_neurons_info.items():\n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    print(f\"   Total de neuronas: {info['total_neurons']}\")\n",
    "    print(f\"   Neuronas muertas: {info['dead_neurons']} ({info['dead_percentage']:.2f}%)\")\n",
    "    if info['dead_neurons'] > 0:\n",
    "        print(f\"   √çndices: {info['dead_indices'][:10]}{'...' if len(info['dead_indices']) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver la neurona muerta #38 de conv1\n",
    "dead_filter_weights = model.conv1.weight[38].detach().cpu().numpy()\n",
    "# Shape: [3, 7, 7] (canal RGB, kernel 7x7)\n",
    "\n",
    "plt.imshow(dead_filter_weights.transpose(1, 2, 0))\n",
    "plt.title('Filtro #38 de conv1 (MUERTO)')\n",
    "plt.show()\n",
    "\n",
    "# Comparar con uno activo\n",
    "active_filter = model.conv1.weight[0].detach().cpu().numpy()\n",
    "plt.imshow(active_filter.transpose(1, 2, 0))\n",
    "plt.title('Filtro #0 de conv1 (ACTIVO)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar porcentaje de neuronas muertas por capa\n",
    "layers = list(dead_neurons_info.keys())\n",
    "dead_percentages = [dead_neurons_info[layer]['dead_percentage'] for layer in layers]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(layers)), dead_percentages, color='crimson', alpha=0.7, edgecolor='black')\n",
    "plt.xticks(range(len(layers)), layers, rotation=45, ha='right')\n",
    "plt.ylabel('Porcentaje de Neuronas Muertas (%)')\n",
    "plt.title('Neuronas Muertas por Capa (threshold = 1e-6)')\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretaci√≥n:\")\n",
    "print(\"   - Un alto % de neuronas muertas puede indicar:\")\n",
    "print(\"     1. Over-parametrizaci√≥n del modelo\")\n",
    "print(\"     2. ReLU 'muriendo' (dying ReLU problem)\")\n",
    "print(\"     3. Neuronas especializadas que solo se activan con ciertos inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üé® 9. Comparaci√≥n de Activaciones entre Clases\n",
    "\n",
    "¬øLas activaciones var√≠an seg√∫n la clase de la imagen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para calcular estad√≠sticas por clase\n",
    "def compute_class_wise_statistics(\n",
    "    activations: Dict[str, torch.Tensor],\n",
    "    labels: np.ndarray,\n",
    "    class_names: List[str]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calcula estad√≠sticas de activaciones separadas por clase.\n",
    "    \n",
    "    Args:\n",
    "        activations: Diccionario con activaciones por capa\n",
    "        labels: Array con labels de cada imagen\n",
    "        class_names: Lista con nombres de clases\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con DataFrames de estad√≠sticas por clase\n",
    "    \"\"\"\n",
    "    class_stats = {}\n",
    "    \n",
    "    for layer_name, activation in activations.items():\n",
    "        act_np = activation.cpu().numpy()\n",
    "        \n",
    "        # Calcular estad√≠sticas por clase\n",
    "        stats_by_class = []\n",
    "        \n",
    "        for class_id, class_name in enumerate(class_names):\n",
    "            # Filtrar activaciones de esta clase\n",
    "            class_mask = labels == class_id\n",
    "            if class_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            class_activations = act_np[class_mask]\n",
    "            \n",
    "            stats = {\n",
    "                'class_id': class_id,\n",
    "                'class_name': class_name,\n",
    "                'num_samples': class_mask.sum(),\n",
    "                'mean': float(class_activations.mean()),\n",
    "                'std': float(class_activations.std()),\n",
    "                'median': float(np.median(class_activations)),\n",
    "                'sparsity_%': float((class_activations == 0).sum() / class_activations.size * 100)\n",
    "            }\n",
    "            stats_by_class.append(stats)\n",
    "        \n",
    "        class_stats[layer_name] = pd.DataFrame(stats_by_class)\n",
    "    \n",
    "    return class_stats\n",
    "\n",
    "# Calcular estad√≠sticas por clase\n",
    "class_wise_stats = compute_class_wise_statistics(\n",
    "    concatenated_activations,\n",
    "    labels_array,\n",
    "    dataset_info['classes']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Estad√≠sticas por clase calculadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar estad√≠sticas de una capa espec√≠fica por clase\n",
    "LAYER_TO_ANALYZE = 'layer4.1.relu'  # √öltima capa convolucional\n",
    "\n",
    "if LAYER_TO_ANALYZE in class_wise_stats:\n",
    "    df = class_wise_stats[LAYER_TO_ANALYZE]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Media por clase\n",
    "    axes[0].bar(df['class_name'], df['mean'], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Clase')\n",
    "    axes[0].set_ylabel('Media de Activaciones')\n",
    "    axes[0].set_title(f'Media de Activaciones por Clase - {LAYER_TO_ANALYZE}')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Sparsity por clase\n",
    "    axes[1].bar(df['class_name'], df['sparsity_%'], color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Clase')\n",
    "    axes[1].set_ylabel('Sparsity (%)')\n",
    "    axes[1].set_title(f'Sparsity por Clase - {LAYER_TO_ANALYZE}')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Estad√≠sticas por Clase - {LAYER_TO_ANALYZE}:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(df)\n",
    "else:\n",
    "    print(f\"‚ùå Capa {LAYER_TO_ANALYZE} no encontrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä 10. Heatmap de Activaciones Promedio\n",
    "\n",
    "Visualizamos la activaci√≥n promedio de cada neurona para identificar patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar una capa para visualizar heatmap\n",
    "LAYER_FOR_HEATMAP = 'layer3.1.relu'\n",
    "\n",
    "if LAYER_FOR_HEATMAP in concatenated_activations:\n",
    "    activation = concatenated_activations[LAYER_FOR_HEATMAP].cpu().numpy()\n",
    "    # Shape: [num_images, num_channels, H, W]\n",
    "    \n",
    "    # Calcular activaci√≥n promedio por canal a trav√©s de spatial dimensions\n",
    "    # Resultado: [num_images, num_channels]\n",
    "    avg_per_channel = activation.mean(axis=(2, 3))\n",
    "    \n",
    "    # Seleccionar subset de im√°genes para visualizaci√≥n (primeras 50)\n",
    "    num_images_to_plot = min(50, avg_per_channel.shape[0])\n",
    "    avg_per_channel_subset = avg_per_channel[:num_images_to_plot]\n",
    "\n",
    "    print(f\"Este gr√°fico visualiza cu√°nto se activa cada neurona de la capa layer3.1.relu \")\n",
    "    print(f\"para cada una de las primeras 50 im√°genes del dataset.\")\n",
    "    # Crear heatmap\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(\n",
    "        avg_per_channel_subset.T,\n",
    "        cmap='YlOrRd',\n",
    "        xticklabels=5,\n",
    "        yticklabels=10,\n",
    "        cbar_kws={'label': 'Activaci√≥n Promedio'}\n",
    "    )\n",
    "    plt.xlabel('√çndice de Imagen')\n",
    "    plt.ylabel('Canal (Neurona)')\n",
    "    plt.title(f'Heatmap de Activaciones Promedio - {LAYER_FOR_HEATMAP}\\n(Primeras {num_images_to_plot} im√°genes)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Interpretaci√≥n del Heatmap:\")\n",
    "    print(\"   - Filas = canales/neuronas de la capa\")\n",
    "    print(\"   - Columnas = im√°genes procesadas\")\n",
    "    print(\"   - Colores c√°lidos = activaciones altas\")\n",
    "    print(\"   - Color amarillo claro = Activaci√≥n ‚âà 0\")\n",
    "    print(\"   - Patrones horizontales = neuronas especializadas\")\n",
    "    print(\"   - Patrones verticales = im√°genes con caracter√≠sticas similares\")\n",
    "\n",
    "    print(\"--------------------------\")\n",
    "    print(\"** HEATMAP SALUDABLE **\")\n",
    "    print(\"   - Mayor√≠a amarillo (sparsity esperada)\")\n",
    "    print(\"   - Puntos rojos dispersos (selectividad)\")\n",
    "    print(\"   - Sin filas completamente rojas (no overfitting)\")\n",
    "    print(\"   - Sin filas completamente rojas (no overfitting)\")\n",
    "    print(\"--------------------------\")\n",
    "    print(\"** HEATMAP PROBLEM√ÅTICO **\")\n",
    "    print(\"   - Todo naranja/rojo (sin sparsity, ReLU no funciona)\")\n",
    "    print(\"   - Filas completamente rojas (neurona siempre activa, no selectiva)\")\n",
    "    print(\"   - Columnas completamente amarillas (imagen no reconocida)\")\n",
    "    print(\"   - Patr√≥n de cuadr√≠cula (neuronas redundantes)\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Capa {LAYER_FOR_HEATMAP} no encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nueva celda para investigar\n",
    "print(\"=\"*80)\n",
    "print(\"üîç INVESTIGACI√ìN: ¬øQu√© neuronas se activaron FUERTE con imagen #5?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "IMAGE_INDEX = 5\n",
    "LAYER_TO_INSPECT = 'layer3.1.relu'\n",
    "\n",
    "if LAYER_TO_INSPECT in concatenated_activations:\n",
    "    # Obtener activaciones de TODAS las neuronas para imagen #5\n",
    "    all_neurons_img5 = concatenated_activations[LAYER_TO_INSPECT][IMAGE_INDEX]\n",
    "    # Shape: [256 neuronas, 2, 2]\n",
    "    \n",
    "    # Calcular activaci√≥n promedio espacial de cada neurona\n",
    "    avg_per_neuron = all_neurons_img5.mean(dim=(1, 2)).cpu().numpy()\n",
    "    # Shape: [256]\n",
    "    \n",
    "    # Ordenar neuronas por activaci√≥n (de mayor a menor)\n",
    "    neuron_indices = np.argsort(avg_per_neuron)[::-1]\n",
    "    activations_sorted = avg_per_neuron[neuron_indices]\n",
    "    \n",
    "    # Top 10 neuronas m√°s activas\n",
    "    print(f\"\\nüèÜ TOP 10 NEURONAS M√ÅS ACTIVAS para imagen #5 (frog):\")\n",
    "    print(\"=\"*80)\n",
    "    for i in range(10):\n",
    "        neuron_id = neuron_indices[i]\n",
    "        activation = activations_sorted[i]\n",
    "        print(f\"   #{i+1}. Neurona #{neuron_id:3d}: Activaci√≥n = {activation:.4f}\")\n",
    "    \n",
    "    # ¬øD√≥nde est√° la neurona #85?\n",
    "    rank_85 = np.where(neuron_indices == 85)[0][0] + 1\n",
    "    activation_85 = avg_per_neuron[85]\n",
    "    \n",
    "    print(f\"\\nüéØ Neurona #85 (la que analizamos):\")\n",
    "    print(f\"   - Ranking: #{rank_85}/256\")\n",
    "    print(f\"   - Activaci√≥n: {activation_85:.4f}\")\n",
    "    \n",
    "    if rank_85 <= 50:\n",
    "        print(f\"   ‚úÖ Est√° en el top 50 (relativamente activa)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  NO est√° en el top 50 (moderadamente activa)\")\n",
    "    \n",
    "    # Visualizar distribuci√≥n\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histograma de activaciones de todas las neuronas\n",
    "    axes[0].hist(avg_per_neuron, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    axes[0].axvline(activation_85, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Neurona #85 (rank #{rank_85})')\n",
    "    axes[0].set_xlabel('Activaci√≥n Promedio')\n",
    "    axes[0].set_ylabel('Frecuencia (n√∫mero de neuronas)')\n",
    "    axes[0].set_title(f'Distribuci√≥n de Activaciones de las 256 Neuronas\\nImagen #5 (frog)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Bar plot del top 20\n",
    "    top_20_indices = neuron_indices[:20]\n",
    "    top_20_activations = activations_sorted[:20]\n",
    "    \n",
    "    colors = ['red' if idx == 85 else 'steelblue' for idx in top_20_indices]\n",
    "    bars = axes[1].bar(range(20), top_20_activations, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xticks(range(20))\n",
    "    axes[1].set_xticklabels([f'#{idx}' for idx in top_20_indices], rotation=45, ha='right')\n",
    "    axes[1].set_xlabel('Neurona ID')\n",
    "    axes[1].set_ylabel('Activaci√≥n')\n",
    "    axes[1].set_title('Top 20 Neuronas M√°s Activas\\n(Rojo = Neurona #85)')\n",
    "    axes[1].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° EXPLICACI√ìN DE LA 'RAYA ROJA' EN EL HEATMAP:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"   - El heatmap muestra TODAS las 256 neuronas\")\n",
    "    print(f\"   - Para imagen #5, hay {(avg_per_neuron > 1.0).sum()} neuronas con activaci√≥n > 1.0\")\n",
    "    print(f\"   - Estas neuronas 'rojas' crearon la raya visible en la columna\")\n",
    "    print(f\"   - La neurona #85 (rank #{rank_85}) contribuy√≥ moderadamente\")\n",
    "    print(f\"   - Las neuronas del top 10 son las que realmente causaron la raya roja\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Capa no encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üî¨ AN√ÅLISIS DETALLADO: Neurona espec√≠fica + Imagen espec√≠fica\n",
    "# ============================================================================\n",
    "\n",
    "# Configuraci√≥n\n",
    "IMAGE_INDEX = 5          # √çndice de la imagen a analizar\n",
    "NEURON_INDEX = 83        # √çndice de la neurona que queremos investigar\n",
    "LAYER_TO_INSPECT = 'layer3.1.relu'  # Capa donde est√° la neurona\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç AN√ÅLISIS DETALLADO DE ACTIVACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä Capa: {LAYER_TO_INSPECT}\")\n",
    "print(f\"üß† Neurona: #{NEURON_INDEX}\")\n",
    "print(f\"üñºÔ∏è  Imagen: #{IMAGE_INDEX}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1Ô∏è‚É£ VISUALIZAR LA IMAGEN\n",
    "# ============================================================================\n",
    "\n",
    "# Obtener la imagen espec√≠fica del test set\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "\n",
    "# Extraer imagen y label\n",
    "image = test_images[IMAGE_INDEX]\n",
    "label = test_labels[IMAGE_INDEX]\n",
    "class_name = dataset_info['classes'][label]\n",
    "\n",
    "# Denormalizar para visualizaci√≥n\n",
    "image_denorm = image_loader.denormalize_image(image)\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Imagen original\n",
    "axes[0].imshow(image_denorm)\n",
    "axes[0].set_title(f'Imagen #{IMAGE_INDEX}\\nClase: {class_name.upper()}', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Imagen con informaci√≥n\n",
    "axes[1].imshow(image_denorm)\n",
    "axes[1].set_title(f'Dimensiones: {image.shape}\\nNormalizaci√≥n: ImageNet', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Imagen cargada correctamente\")\n",
    "print(f\"   - Clase real: {class_name}\")\n",
    "print(f\"   - Label ID: {label}\")\n",
    "print(f\"   - Shape original: {image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2Ô∏è‚É£ EXTRAER ACTIVACI√ìN DE LA NEURONA PARA ESTA IMAGEN\n",
    "# ============================================================================\n",
    "\n",
    "# Verificar que tenemos las activaciones\n",
    "if LAYER_TO_INSPECT in concatenated_activations:\n",
    "    # Obtener activaciones de la capa completa\n",
    "    layer_activations = concatenated_activations[LAYER_TO_INSPECT]\n",
    "    # Shape esperado: [num_images, num_channels, H, W]\n",
    "    \n",
    "    # Extraer activaci√≥n de la imagen espec√≠fica\n",
    "    image_activation = layer_activations[IMAGE_INDEX]  # Shape: [num_channels, H, W]\n",
    "    \n",
    "    # Extraer activaci√≥n de la neurona espec√≠fica\n",
    "    neuron_activation = image_activation[NEURON_INDEX]  # Shape: [H, W]\n",
    "    \n",
    "    print(f\"\\nüìä INFORMACI√ìN DE ACTIVACIONES:\")\n",
    "    print(f\"   - Shape de la capa completa: {list(layer_activations.shape)}\")\n",
    "    print(f\"   - Shape para imagen #{IMAGE_INDEX}: {list(image_activation.shape)}\")\n",
    "    print(f\"   - Shape de neurona #{NEURON_INDEX}: {list(neuron_activation.shape)}\")\n",
    "    \n",
    "    # Convertir a numpy para an√°lisis\n",
    "    neuron_activation_np = neuron_activation.cpu().numpy()\n",
    "    \n",
    "    # Estad√≠sticas de la activaci√≥n\n",
    "    print(f\"\\nüî¢ ESTAD√çSTICAS DE LA ACTIVACI√ìN:\")\n",
    "    print(f\"   - Valor m√°ximo: {neuron_activation_np.max():.4f}\")\n",
    "    print(f\"   - Valor m√≠nimo: {neuron_activation_np.min():.4f}\")\n",
    "    print(f\"   - Valor promedio: {neuron_activation_np.mean():.4f}\")\n",
    "    print(f\"   - Desviaci√≥n est√°ndar: {neuron_activation_np.std():.4f}\")\n",
    "    print(f\"   - Sparsity: {(neuron_activation_np == 0).sum() / neuron_activation_np.size * 100:.1f}%\")\n",
    "    \n",
    "    # Calcular activaci√≥n promedio espacial (el valor que vimos en el heatmap)\n",
    "    avg_activation = neuron_activation_np.mean()\n",
    "    print(f\"\\nüéØ ACTIVACI√ìN PROMEDIO (valor del heatmap): {avg_activation:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Capa '{LAYER_TO_INSPECT}' no encontrada en las activaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3Ô∏è‚É£ VISUALIZAR EL MAPA DE ACTIVACI√ìN ESPACIAL\n",
    "# ============================================================================\n",
    "\n",
    "if LAYER_TO_INSPECT in concatenated_activations:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Imagen original\n",
    "    axes[0].imshow(image_denorm)\n",
    "    axes[0].set_title(f'Imagen Original\\n{class_name.upper()}', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # 2. Mapa de activaci√≥n (heatmap)\n",
    "    im = axes[1].imshow(neuron_activation_np, cmap='hot', interpolation='nearest')\n",
    "    axes[1].set_title(f'Activaci√≥n de Neurona #{NEURON_INDEX}\\n{LAYER_TO_INSPECT}', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel(f'Ancho espacial (shape: {neuron_activation_np.shape[1]})')\n",
    "    axes[1].set_ylabel(f'Alto espacial (shape: {neuron_activation_np.shape[0]})')\n",
    "    plt.colorbar(im, ax=axes[1], label='Intensidad de activaci√≥n')\n",
    "    \n",
    "    # 3. Superposici√≥n (overlay)\n",
    "    # Redimensionar activaci√≥n al tama√±o de la imagen original\n",
    "    from scipy.ndimage import zoom\n",
    "    zoom_factors = (image_denorm.shape[0] / neuron_activation_np.shape[0],\n",
    "                    image_denorm.shape[1] / neuron_activation_np.shape[1])\n",
    "    activation_resized = zoom(neuron_activation_np, zoom_factors, order=1)\n",
    "    \n",
    "    # Normalizar para overlay\n",
    "    activation_norm = (activation_resized - activation_resized.min()) / (activation_resized.max() - activation_resized.min() + 1e-8)\n",
    "    \n",
    "    # Mostrar imagen con overlay\n",
    "    axes[2].imshow(image_denorm)\n",
    "    axes[2].imshow(activation_norm, cmap='hot', alpha=0.5)  # Alpha para transparencia\n",
    "    axes[2].set_title(f'Superposici√≥n: ¬øD√≥nde se activa?\\n(Rojo = alta activaci√≥n)', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° INTERPRETACI√ìN DEL MAPA DE ACTIVACI√ìN:\")\n",
    "    print(\"   - Mapa central: Muestra la activaci√≥n espacial (d√≥nde 'mira' la neurona)\")\n",
    "    print(\"   - Mapa derecho: Superposici√≥n sobre la imagen original\")\n",
    "    print(\"   - Zonas rojas/brillantes = La neurona se activ√≥ fuertemente ah√≠\")\n",
    "    print(\"   - Zonas oscuras = La neurona no detect√≥ nada interesante\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4Ô∏è‚É£ AN√ÅLISIS DE SESGO ESPACIAL\n",
    "# ============================================================================\n",
    "\n",
    "from src.utils.analyze_neuron import analyze_spatial_bias, visualize_spatial_bias\n",
    "\n",
    "# Analizar sesgo espacial de la neurona #83\n",
    "spatial_results = analyze_spatial_bias(\n",
    "    neuron_index=83,\n",
    "    layer_name='layer3.1.relu',\n",
    "    concatenated_activations=concatenated_activations,\n",
    "    num_samples=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualizar resultados\n",
    "visualize_spatial_bias(spatial_results)\n",
    "\n",
    "# Acceso r√°pido a resultados\n",
    "print(f\"\\nüéØ Resumen:\")\n",
    "print(f\"   Sesgo horizontal: {spatial_results['horizontal_bias']['bias_type']}\")\n",
    "print(f\"   Sesgo vertical: {spatial_results['vertical_bias']['bias_type']}\")\n",
    "print(f\"   Sesgo dominante: {spatial_results['dominant_bias']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5Ô∏è‚É£ AN√ÅLISIS POR CLASE: ¬øEsta neurona es selectiva a alguna clase?\n",
    "# ============================================================================\n",
    "\n",
    "from src.utils.analyze_neuron import analyze_class_selectivity, visualize_class_selectivity\n",
    "\n",
    "# Analizar selectividad por clase\n",
    "selectivity_results = analyze_class_selectivity(\n",
    "    neuron_index=83,\n",
    "    layer_name='layer3.1.relu',\n",
    "    concatenated_activations=concatenated_activations,\n",
    "    labels=labels_array,\n",
    "    class_names=dataset_info['classes'],\n",
    "    num_samples=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualizar resultados\n",
    "visualize_class_selectivity(selectivity_results)\n",
    "\n",
    "# Acceso program√°tico a resultados\n",
    "print(f\"\\nüéØ Resumen r√°pido:\")\n",
    "print(f\"   Clase preferida: {selectivity_results['selectivity']['top_class']}\")\n",
    "print(f\"   Nivel de selectividad: {selectivity_results['selectivity']['level']}\")\n",
    "print(f\"   Ratio: {selectivity_results['selectivity']['activation_ratio']:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6Ô∏è‚É£ AN√ÅLISIS DE TEXTURAS: Funci√≥n modular\n",
    "# ============================================================================\n",
    "\n",
    "from src.utils.analyze_neuron import analyze_neuron_texture_and_features, visualize_texture_analysis\n",
    "\n",
    "# Llamar directamente sin crear instancia\n",
    "results = analyze_neuron_texture_and_features(\n",
    "    neuron_index=83,\n",
    "    image_index=5,\n",
    "    layer_name='layer3.1.relu',\n",
    "    concatenated_activations=concatenated_activations,\n",
    "    test_loader=test_loader,\n",
    "    image_loader=image_loader,\n",
    "    dataset_info=dataset_info,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualizar resultados\n",
    "visualize_texture_analysis(results)\n",
    "\n",
    "# Acceder a resultados\n",
    "print(f\"\\nüéØ Acceso r√°pido a resultados:\")\n",
    "print(f\"   Feature principal: {results['correlations']['top_feature']}\")\n",
    "print(f\"   Correlaci√≥n: {results['correlations']['top_correlation']:.3f}\")\n",
    "print(f\"   Color dominante: {results['color_analysis']['dominant_channel']}\")\n",
    "print(f\"   Tipo de textura: {results['texture_analysis']['type']}\")\n",
    "print(f\"   Complejidad de forma: {results['shape_analysis']['complexity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ 11. An√°lisis de Evoluci√≥n de Activaciones a trav√©s de Capas\n",
    "\n",
    "¬øC√≥mo cambian las activaciones a medida que avanzamos en la red?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer m√©tricas clave de todas las capas para visualizar evoluci√≥n\n",
    "layer_names = list(concatenated_activations.keys())\n",
    "layer_means = []\n",
    "layer_stds = []\n",
    "layer_sparsities = []\n",
    "layer_ranges = []\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    act_np = concatenated_activations[layer_name].cpu().numpy()\n",
    "    \n",
    "    layer_means.append(act_np.mean())\n",
    "    layer_stds.append(act_np.std())\n",
    "    layer_sparsities.append((act_np == 0).sum() / act_np.size * 100)\n",
    "    layer_ranges.append(act_np.max() - act_np.min())\n",
    "\n",
    "# Crear gr√°ficos de evoluci√≥n\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Evoluci√≥n de la media\n",
    "axes[0, 0].plot(range(len(layer_names)), layer_means, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0, 0].set_xticks(range(len(layer_names)))\n",
    "axes[0, 0].set_xticklabels(layer_names, rotation=45, ha='right', fontsize=8)\n",
    "axes[0, 0].set_ylabel('Media')\n",
    "axes[0, 0].set_title('Evoluci√≥n de la Media de Activaciones')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Evoluci√≥n de la desviaci√≥n est√°ndar\n",
    "axes[0, 1].plot(range(len(layer_names)), layer_stds, marker='s', linewidth=2, markersize=8, color='coral')\n",
    "axes[0, 1].set_xticks(range(len(layer_names)))\n",
    "axes[0, 1].set_xticklabels(layer_names, rotation=45, ha='right', fontsize=8)\n",
    "axes[0, 1].set_ylabel('Desviaci√≥n Est√°ndar')\n",
    "axes[0, 1].set_title('Evoluci√≥n de la Desviaci√≥n Est√°ndar')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Evoluci√≥n de la sparsity\n",
    "axes[1, 0].plot(range(len(layer_names)), layer_sparsities, marker='^', linewidth=2, markersize=8, color='mediumseagreen')\n",
    "axes[1, 0].set_xticks(range(len(layer_names)))\n",
    "axes[1, 0].set_xticklabels(layer_names, rotation=45, ha='right', fontsize=8)\n",
    "axes[1, 0].set_ylabel('Sparsity (%)')\n",
    "axes[1, 0].set_title('Evoluci√≥n de la Sparsity')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Evoluci√≥n del rango\n",
    "axes[1, 1].plot(range(len(layer_names)), layer_ranges, marker='D', linewidth=2, markersize=8, color='orchid')\n",
    "axes[1, 1].set_xticks(range(len(layer_names)))\n",
    "axes[1, 1].set_xticklabels(layer_names, rotation=45, ha='right', fontsize=8)\n",
    "axes[1, 1].set_ylabel('Rango (Max - Min)')\n",
    "axes[1, 1].set_title('Evoluci√≥n del Rango de Activaciones')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- An√°lisis autom√°tico de tendencias\n",
    "print(\"\\nüí° Observaciones sobre la Evoluci√≥n:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. MEDIA\n",
    "mean_values = [stats_df.loc[stats_df['layer'] == layer, 'mean'].values[0] \n",
    "               for layer in layer_names]\n",
    "mean_start = mean_values[0]\n",
    "mean_end = mean_values[-1]\n",
    "mean_max = max(mean_values)\n",
    "mean_max_layer = layer_names[mean_values.index(mean_max)]\n",
    "mean_min = min(mean_values)\n",
    "mean_min_layer = layer_names[mean_values.index(mean_min)]\n",
    "\n",
    "if mean_end > mean_start:\n",
    "    mean_trend = \"AUMENTA\"\n",
    "    mean_emoji = \"üìà\"\n",
    "else:\n",
    "    mean_trend = \"DISMINUYE\"\n",
    "    mean_emoji = \"üìâ\"\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  MEDIA: {mean_emoji} {mean_trend} con la profundidad\")\n",
    "print(f\"   ‚Ä¢ Inicial (relu):     {mean_start:.3f}\")\n",
    "print(f\"   ‚Ä¢ Final (layer4.1):   {mean_end:.3f}\")\n",
    "print(f\"   ‚Ä¢ Cambio total:       {mean_end - mean_start:+.3f} ({((mean_end/mean_start - 1)*100):+.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Pico M√ÅXIMO:        {mean_max:.3f} en {mean_max_layer}\")\n",
    "print(f\"   ‚Ä¢ Valle M√çNIMO:       {mean_min:.3f} en {mean_min_layer}\")\n",
    "print(f\"   üìä Interpretaci√≥n: \", end=\"\")\n",
    "if mean_max_layer == 'layer1.1.relu':\n",
    "    print(\"Pico en layer1.1 indica features ricos antes del downsampling.\")\n",
    "else:\n",
    "    print(f\"El pico en {mean_max_layer} es inesperado, revisar.\")\n",
    "\n",
    "# 2. DESVIACI√ìN EST√ÅNDAR\n",
    "std_values = [stats_df.loc[stats_df['layer'] == layer, 'std'].values[0] \n",
    "              for layer in layer_names]\n",
    "std_start = std_values[0]\n",
    "std_end = std_values[-1]\n",
    "std_max = max(std_values)\n",
    "std_max_layer = layer_names[std_values.index(std_max)]\n",
    "\n",
    "if std_end > std_start * 1.5:\n",
    "    std_trend = \"SE VUELVEN MUCHO M√ÅS VARIABLES\"\n",
    "    std_emoji = \"üìäüìä\"\n",
    "elif std_end > std_start:\n",
    "    std_trend = \"aumentan ligeramente\"\n",
    "    std_emoji = \"üìä\"\n",
    "else:\n",
    "    std_trend = \"se mantienen estables\"\n",
    "    std_emoji = \"‚û°Ô∏è\"\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  DESVIACI√ìN EST√ÅNDAR: {std_emoji} {std_trend}\")\n",
    "print(f\"   ‚Ä¢ Inicial:            {std_start:.3f}\")\n",
    "print(f\"   ‚Ä¢ Final:              {std_end:.3f}\")\n",
    "print(f\"   ‚Ä¢ Incremento:         {std_end - std_start:+.3f} ({((std_end/std_start - 1)*100):+.1f}%)\")\n",
    "print(f\"   ‚Ä¢ M√°ximo:             {std_max:.3f} en {std_max_layer}\")\n",
    "print(f\"   üìä Interpretaci√≥n: \", end=\"\")\n",
    "if std_max_layer == layer_names[-1]:\n",
    "    print(\"Variabilidad m√°xima en √∫ltima capa ‚Üí neuronas ultra-especializadas ‚úÖ\")\n",
    "else:\n",
    "    print(f\"Alta variabilidad en {std_max_layer} indica respuestas diversas en esa capa.\")\n",
    "\n",
    "# 3. SPARSITY\n",
    "sparsity_values = [stats_df.loc[stats_df['layer'] == layer, 'sparsity_%'].values[0] \n",
    "                   for layer in layer_names]\n",
    "sparsity_start = sparsity_values[0]\n",
    "sparsity_end = sparsity_values[-1]\n",
    "sparsity_max = max(sparsity_values)\n",
    "sparsity_max_layer = layer_names[sparsity_values.index(sparsity_max)]\n",
    "sparsity_min = min(sparsity_values)\n",
    "sparsity_min_layer = layer_names[sparsity_values.index(sparsity_min)]\n",
    "\n",
    "if sparsity_end > sparsity_start + 10:\n",
    "    sparsity_trend = \"AUMENTA SIGNIFICATIVAMENTE\"\n",
    "    sparsity_emoji = \"üî∫\"\n",
    "elif sparsity_end > sparsity_start:\n",
    "    sparsity_trend = \"aumenta moderadamente\"\n",
    "    sparsity_emoji = \"‚ÜóÔ∏è\"\n",
    "else:\n",
    "    sparsity_trend = \"se mantiene estable\"\n",
    "    sparsity_emoji = \"‚û°Ô∏è\"\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  SPARSITY: {sparsity_emoji} {sparsity_trend} en capas profundas\")\n",
    "print(f\"   ‚Ä¢ Inicial:            {sparsity_start:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Final:              {sparsity_end:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Incremento:         {sparsity_end - sparsity_start:+.1f} puntos\")\n",
    "print(f\"   ‚Ä¢ M√°ximo:             {sparsity_max:.1f}% en {sparsity_max_layer}\")\n",
    "print(f\"   ‚Ä¢ M√≠nimo:             {sparsity_min:.1f}% en {sparsity_min_layer}\")\n",
    "print(f\"   üìä Interpretaci√≥n: \", end=\"\")\n",
    "if sparsity_max > 50:\n",
    "    print(f\"¬°M√°s del 50% de ceros en {sparsity_max_layer}! Neuronas MUY selectivas ‚úÖ\")\n",
    "else:\n",
    "    print(f\"Sparsity moderada indica balance entre activaci√≥n y selectividad.\")\n",
    "\n",
    "# 4. RANGO\n",
    "range_values = [stats_df.loc[stats_df['layer'] == layer, 'max'].values[0] - \n",
    "                stats_df.loc[stats_df['layer'] == layer, 'min'].values[0]\n",
    "                for layer in layer_names]\n",
    "range_start = range_values[0]\n",
    "range_end = range_values[-1]\n",
    "range_max = max(range_values)\n",
    "range_max_layer = layer_names[range_values.index(range_max)]\n",
    "range_min = min(range_values)\n",
    "\n",
    "if range_end > range_start * 2:\n",
    "    range_trend = \"SE EXPANDEN DRAM√ÅTICAMENTE\"\n",
    "    range_emoji = \"üí•üí•\"\n",
    "elif range_end > range_start * 1.2:\n",
    "    range_trend = \"se expanden\"\n",
    "    range_emoji = \"üìà\"\n",
    "else:\n",
    "    range_trend = \"se comprimen\"\n",
    "    range_emoji = \"üìâ\"\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  RANGO: {range_emoji} Los valores {range_trend}\")\n",
    "print(f\"   ‚Ä¢ Inicial:            {range_start:.2f}\")\n",
    "print(f\"   ‚Ä¢ Final:              {range_end:.2f}\")\n",
    "print(f\"   ‚Ä¢ Multiplicador:      {range_end/range_start:.2f}x\")\n",
    "print(f\"   ‚Ä¢ M√°ximo:             {range_max:.2f} en {range_max_layer}\")\n",
    "print(f\"   ‚Ä¢ M√≠nimo:             {range_min:.2f}\")\n",
    "print(f\"   üìä Interpretaci√≥n: \", end=\"\")\n",
    "if range_end > 10:\n",
    "    print(f\"Rango >10 en √∫ltima capa ‚Üí activaciones EXTREMAS cuando detectan algo üî•\")\n",
    "else:\n",
    "    print(f\"Rango moderado indica activaciones controladas.\")\n",
    "\n",
    "# RESUMEN FINAL\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ RESUMEN EJECUTIVO:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "ResNet18 muestra un patr√≥n claro de ESPECIALIZACI√ìN PROGRESIVA:\n",
    "\n",
    "‚úÖ Capas tempranas (layer1):\n",
    "   - Baja sparsity ({sparsity_min:.1f}%) ‚Üí muchas neuronas activas\n",
    "   - Variabilidad moderada ‚Üí detectan features generales\n",
    "   \n",
    "‚ö†Ô∏è  Transiciones entre bloques:\n",
    "   - Ca√≠das en media por downsampling\n",
    "   - Saltos en sparsity (+10-20 puntos)\n",
    "   \n",
    "üî• √öltima capa (layer4.1):\n",
    "   - Alta sparsity ({sparsity_end:.1f}%) ‚Üí neuronas selectivas\n",
    "   - Alta variabilidad (std={std_end:.2f}) ‚Üí respuestas diversas\n",
    "   - Rango explosivo ({range_end:.1f}) ‚Üí activaciones extremas\n",
    "   \n",
    "üí° Conclusi√≥n: El modelo funciona correctamente con neuronas cada vez\n",
    "   m√°s especializadas en features de alto nivel.\n",
    "\"\"\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ 12. Guardar Resultados del An√°lisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio para resultados\n",
    "results_dir = project_root / 'results' / 'activation_analysis'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Directorio de resultados: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar estad√≠sticas en CSV\n",
    "stats_df.to_csv(results_dir / 'activation_statistics.csv', index=False)\n",
    "print(\"‚úÖ Estad√≠sticas guardadas en activation_statistics.csv\")\n",
    "\n",
    "# Guardar informaci√≥n de neuronas muertas\n",
    "import json\n",
    "\n",
    "with open(results_dir / 'dead_neurons_info.json', 'w') as f:\n",
    "    # Convertir a formato serializable (sin listas muy largas)\n",
    "    serializable_info = {}\n",
    "    for layer, info in dead_neurons_info.items():\n",
    "        serializable_info[layer] = {\n",
    "            'total_neurons': info['total_neurons'],\n",
    "            'dead_neurons': info['dead_neurons'],\n",
    "            'dead_percentage': info['dead_percentage'],\n",
    "            'dead_indices_count': len(info['dead_indices'])\n",
    "        }\n",
    "    json.dump(serializable_info, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Informaci√≥n de neuronas muertas guardada en dead_neurons_info.json\")\n",
    "\n",
    "# Guardar estad√≠sticas por clase\n",
    "for layer_name, df in class_wise_stats.items():\n",
    "    safe_layer_name = layer_name.replace('.', '_')\n",
    "    df.to_csv(results_dir / f'class_stats_{safe_layer_name}.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Estad√≠sticas por clase guardadas ({len(class_wise_stats)} archivos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: Guardar activaciones completas para an√°lisis posterior\n",
    "# ADVERTENCIA: Esto puede ocupar mucho espacio\n",
    "SAVE_ACTIVATIONS = False\n",
    "\n",
    "if SAVE_ACTIVATIONS:\n",
    "    activations_file = results_dir / 'concatenated_activations.pth'\n",
    "    torch.save({\n",
    "        'activations': {k: v.cpu() for k, v in concatenated_activations.items()},\n",
    "        'labels': labels_array,\n",
    "        'class_names': dataset_info['classes'],\n",
    "        'num_samples': len(labels_array)\n",
    "    }, activations_file)\n",
    "    \n",
    "    file_size_mb = activations_file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úÖ Activaciones guardadas en {activations_file}\")\n",
    "    print(f\"üì¶ Tama√±o del archivo: {file_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Activaciones NO guardadas (SAVE_ACTIVATIONS=False)\")\n",
    "    print(\"   Cambia a True si necesitas guardarlas para an√°lisis futuro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù 13. Conclusiones y Observaciones Clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä RESUMEN DEL AN√ÅLISIS DE ACTIVACIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîç Hallazgos Principales:\\n\")\n",
    "\n",
    "# 1. Capa con mayor sparsity\n",
    "max_sparsity_idx = stats_df['sparsity_%'].idxmax()\n",
    "max_sparsity_layer = stats_df.loc[max_sparsity_idx, 'layer']\n",
    "max_sparsity_value = stats_df.loc[max_sparsity_idx, 'sparsity_%']\n",
    "print(f\"1. Capa con MAYOR sparsity: {max_sparsity_layer} ({max_sparsity_value:.2f}%)\")\n",
    "\n",
    "# 2. Capa con menor sparsity\n",
    "min_sparsity_idx = stats_df['sparsity_%'].idxmin()\n",
    "min_sparsity_layer = stats_df.loc[min_sparsity_idx, 'layer']\n",
    "min_sparsity_value = stats_df.loc[min_sparsity_idx, 'sparsity_%']\n",
    "print(f\"2. Capa con MENOR sparsity: {min_sparsity_layer} ({min_sparsity_value:.2f}%)\")\n",
    "\n",
    "# 3. Capa con m√°s neuronas muertas\n",
    "max_dead_layer = max(dead_neurons_info.items(), key=lambda x: x[1]['dead_percentage'])\n",
    "print(f\"3. Capa con M√ÅS neuronas muertas: {max_dead_layer[0]} ({max_dead_layer[1]['dead_percentage']:.2f}%)\")\n",
    "\n",
    "# 4. Total de neuronas muertas\n",
    "total_dead = sum(info['dead_neurons'] for info in dead_neurons_info.values())\n",
    "total_neurons = sum(info['total_neurons'] for info in dead_neurons_info.values())\n",
    "print(f\"4. Total de neuronas muertas: {total_dead}/{total_neurons} ({total_dead/total_neurons*100:.2f}%)\")\n",
    "\n",
    "# 5. Rango de medias\n",
    "print(f\"\\n5. Rango de medias de activaciones:\")\n",
    "print(f\"   - M√≠nima: {stats_df['mean'].min():.4f} ({stats_df.loc[stats_df['mean'].idxmin(), 'layer']})\")\n",
    "print(f\"   - M√°xima: {stats_df['mean'].max():.4f} ({stats_df.loc[stats_df['mean'].idxmax(), 'layer']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° INTERPRETACIONES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. SPARSITY:\n",
    "   - Alta sparsity (>50%) en capas profundas es normal debido a ReLU\n",
    "   - Indica selectividad: pocas neuronas se activan para cada input\n",
    "   - Puede ser beneficioso para eficiencia computacional\n",
    "\n",
    "2. NEURONAS MUERTAS:\n",
    "   - Algunas neuronas pueden estar especializadas en features raros\n",
    "   - Un % muy alto (>70%) puede indicar sobreajuste o dying ReLU\n",
    "   - Considerar: dropout, batch normalization, learning rate\n",
    "\n",
    "3. EVOLUCI√ìN DE ACTIVACIONES:\n",
    "   - Capas tempranas: features generales (bordes, texturas)\n",
    "   - Capas profundas: features espec√≠ficos (objetos, partes)\n",
    "   - La media deber√≠a estabilizarse en capas profundas\n",
    "\n",
    "4. DIFERENCIAS ENTRE CLASES:\n",
    "   - Clases visualmente similares ‚Üí activaciones similares\n",
    "   - Clases muy diferentes ‚Üí activaciones divergentes\n",
    "   - √ötil para entender confusiones del modelo\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ PR√ìXIMOS PASOS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "‚úÖ Completado: An√°lisis estad√≠stico de activaciones\n",
    "\n",
    "üìã Siguiente (Notebook 03): Feature Visualization\n",
    "   - Generar im√°genes sint√©ticas que maximicen activaciones\n",
    "   - Visualizar qu√© detecta cada neurona\n",
    "   - Identificar features de bajo y alto nivel\n",
    "\n",
    "üìã Siguiente (Notebook 04): Neuron Probing\n",
    "   - Entrenar clasificadores sobre activaciones\n",
    "   - Entender qu√© informaci√≥n codifica cada capa\n",
    "   - An√°lisis de emergencia de conceptos\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üßπ 14. Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover hooks para liberar memoria\n",
    "activation_hook.remove_hooks()\n",
    "print(\"‚úÖ Hooks removidos\")\n",
    "\n",
    "# Limpiar cache de CUDA si es necesario\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ Cache de CUDA limpiado\")\n",
    "\n",
    "print(\"\\nüéâ Notebook 02 completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Funciones Helper (Si se necesitan agregar)\n",
    "\n",
    "Las siguientes funciones pueden agregarse a `src/interpretability/activation_analyzer.py` si se reutilizan frecuentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES HELPER - Copiar a src/interpretability/activation_analyzer.py si es necesario\n",
    "\n",
    "def compute_activation_statistics(activations: Dict[str, torch.Tensor]) -> pd.DataFrame:\n",
    "    \"\"\"Calcula estad√≠sticas completas de activaciones por capa.\"\"\"\n",
    "    # Implementaci√≥n ya mostrada arriba\n",
    "    pass\n",
    "\n",
    "def find_dead_neurons_detailed(\n",
    "    activations: Dict[str, torch.Tensor],\n",
    "    threshold: float = 1e-6\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"Identifica neuronas muertas por capa.\"\"\"\n",
    "    # Implementaci√≥n ya mostrada arriba\n",
    "    pass\n",
    "\n",
    "def compute_class_wise_statistics(\n",
    "    activations: Dict[str, torch.Tensor],\n",
    "    labels: np.ndarray,\n",
    "    class_names: List[str]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Calcula estad√≠sticas de activaciones por clase.\"\"\"\n",
    "    # Implementaci√≥n ya mostrada arriba\n",
    "    pass\n",
    "\n",
    "def extract_activations_from_batch(\n",
    "    model: nn.Module,\n",
    "    hook: ActivationHook,\n",
    "    images: torch.Tensor,\n",
    "    device: torch.device\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Extrae activaciones de un batch de im√°genes.\"\"\"\n",
    "    # Implementaci√≥n ya mostrada arriba\n",
    "    pass\n",
    "\n",
    "print(\"üí° Estas funciones pueden moverse a src/interpretability/activation_analyzer.py\")\n",
    "print(\"   para reutilizaci√≥n en futuros notebooks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_III",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
