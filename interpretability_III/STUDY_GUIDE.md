# ğŸ“š GuÃ­a de Estudio - MÃ³dulo III: Neuron Activation Analysis

## ğŸ¯ Objetivo del MÃ³dulo
Entender **quÃ© representan las neuronas individuales** en redes profundas y cÃ³mo visualizar/analizar sus activaciones.

---

## ğŸ“– Temas de Estudio por Orden de Prioridad

### ğŸ”´ **NIVEL 1: FUNDAMENTALES (Estudiar ANTES del Notebook 01)**

#### 1.1 Redes Neuronales Convolucionales (CNN)
**Â¿QuÃ© estudiar?**
- CÃ³mo funcionan las **capas convolucionales** (filtros, kernels, stride, padding)
- Concepto de **feature maps** / **activation maps**
- **Pooling layers** (max pooling, average pooling)
- **Fully connected layers** al final
- Flujo de informaciÃ³n: entrada â†’ conv â†’ activaciÃ³n â†’ pool â†’ fc â†’ salida

**Recursos:**
- [ ] Video: "CNN Explained" de StatQuest (~20 min)
- [ ] ArtÃ­culo: "A Beginner's Guide to CNNs" en Medium
- [ ] PrÃ¡ctica: Visualizar filtros de una CNN simple

**Conceptos clave:**
- âœ… Un filtro detecta un patrÃ³n especÃ­fico
- âœ… Las primeras capas detectan bordes/texturas
- âœ… Las capas profundas detectan objetos complejos
- âœ… Cada neurona tiene un "campo receptivo"

---

#### 1.2 Arquitectura ResNet
**Â¿QuÃ© estudiar?**
- **Conexiones residuales** (skip connections): Â¿Por quÃ© existen?
- Problema del **vanishing gradient** que ResNet soluciona
- Estructura de **bloques residuales** (BasicBlock, Bottleneck)
- Diferencia entre ResNet-18, 50, 101, etc.

**Recursos:**
- [ ] Paper original: "Deep Residual Learning for Image Recognition" (2015) - Leer solo SecciÃ³n 3
- [ ] Video: "ResNet Explained" de Yannic Kilcher (~15 min)
- [ ] Diagrama: Dibujar un bloque residual con skip connection

**Conceptos clave:**
- âœ… F(x) + x permite flujo directo de gradientes
- âœ… ResNet-18 tiene 4 "layers" con mÃºltiples bloques cada uno
- âœ… Los bloques pueden tener "downsample" (reducir resoluciÃ³n)

**Pregunta para ti:**
> Â¿Por quÃ© una red de 152 capas puede entrenarse mejor con skip connections que una de 34 sin ellas?

---

#### 1.3 Forward Pass y Activaciones
**Â¿QuÃ© estudiar?**
- Â¿QuÃ© es una **activaciÃ³n**? (output de una neurona/capa)
- Funciones de activaciÃ³n: **ReLU**, Sigmoid, Tanh
- Shape de las activaciones: [batch, channels, height, width]
- Diferencia entre **pre-activaciÃ³n** y **post-activaciÃ³n**

**Recursos:**
- [ ] Implementar forward pass manualmente en NumPy
- [ ] Visualizar activaciones de una capa simple

**Conceptos clave:**
- âœ… ReLU(x) = max(0, x) â†’ introduce no-linealidad
- âœ… Las activaciones cambian con cada input
- âœ… Una neurona "se activa" cuando su output > 0

**Ejercicio:**
```python
# Si tengo una imagen 32x32x3 (CIFAR-10)
# Y aplico Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
# Â¿QuÃ© shape tiene la activaciÃ³n resultante?
# Respuesta: [batch, 64, 16, 16]
```

---

### ğŸŸ¡ **NIVEL 2: CONCEPTOS CLAVE (Estudiar DURANTE el Notebook 01-02)**

#### 2.1 PyTorch Hooks
**Â¿QuÃ© estudiar?**
- Â¿QuÃ© es un **hook** en PyTorch?
- **Forward hooks** vs **Backward hooks**
- `register_forward_hook()` - Capturar activaciones
- `register_backward_hook()` - Capturar gradientes

**Recursos:**
- [ ] PyTorch Docs: "torch.nn.Module.register_forward_hook"
- [ ] Tutorial prÃ¡ctico: Capturar activaciones de una capa

**Conceptos clave:**
- âœ… Un hook es una funciÃ³n callback que se ejecuta automÃ¡ticamente
- âœ… Se "engancha" a una capa especÃ­fica
- âœ… Captura activaciones sin modificar el modelo

**Ejemplo mental:**
```python
# Sin hook: output = model(input)  # No veo activaciones internas
# Con hook: output = model(input)  # Hook captura conv1, layer1, etc.
```

---

#### 2.2 VisualizaciÃ³n de Activaciones
**Â¿QuÃ© estudiar?**
- CÃ³mo interpretar un **heatmap de activaciÃ³n**
- NormalizaciÃ³n de activaciones para visualizaciÃ³n
- Diferencia entre visualizar **pesos** vs **activaciones**
- Colormap apropiados (viridis, jet, etc.)

**Recursos:**
- [ ] Ejemplos de visualizaciones en papers de interpretabilidad
- [ ] Matplotlib: imshow() para visualizar matrices

**Conceptos clave:**
- âœ… Valores altos (brillantes) = neurona muy activada
- âœ… Valores bajos/cero (oscuros) = neurona inactiva
- âœ… Cada filtro produce un "mapa" diferente

---

#### 2.3 EstadÃ­sticas de Activaciones
**Â¿QuÃ© estudiar?**
- **Sparsity**: % de neuronas con activaciÃ³n = 0
- **Dead neurons**: Neuronas que nunca se activan
- **Mean, Std, Min, Max** de activaciones
- Distribuciones de activaciones (histogramas)

**Conceptos clave:**
- âœ… Alta sparsity = muchas neuronas inactivas (normal con ReLU)
- âœ… Dead neurons = posible problema de entrenamiento
- âœ… EstadÃ­sticas varÃ­an por capa (primeras capas mÃ¡s densas)

---

### ğŸŸ¢ **NIVEL 3: TÃ‰CNICAS AVANZADAS (Estudiar ANTES del Notebook 03-05)**

#### 3.1 Feature Visualization
**Â¿QuÃ© estudiar?**
- **Activation Maximization**: Optimizar input para maximizar activaciÃ³n de neurona
- **DeepDream**: Variante de activation maximization
- RegularizaciÃ³n en feature visualization (suavizado, prior natural)
- TÃ©cnicas de optimizaciÃ³n (gradient ascent)

**Recursos:**
- [ ] Paper: "Feature Visualization" (Olah et al., 2017) - Distill.pub
- [ ] Video: "Deep Visualization" de Two Minute Papers
- [ ] ImplementaciÃ³n: Lucid library o Captum

**Conceptos clave:**
- âœ… Crear una imagen que "engaÃ±e" a una neurona
- âœ… Muestra quÃ© patrÃ³n busca esa neurona
- âœ… Requiere optimizaciÃ³n iterativa

**Pregunta:**
> Si maximizo la activaciÃ³n del filtro 23 en conv1, Â¿quÃ© tipo de patrÃ³n esperarÃ­as ver?
> Respuesta: Probablemente bordes o texturas simples

---

#### 3.2 Neuron Probing
**Â¿QuÃ© estudiar?**
- **Probing classifiers**: Entrenar clasificador lineal sobre activaciones
- Â¿QuÃ© informaciÃ³n codifica cada capa?
- Concepto de **emergencia** de conceptos en capas profundas
- AnÃ¡lisis de representaciones aprendidas

**Recursos:**
- [ ] Paper: "What do you learn from context?" (Peters et al., 2018)
- [ ] Paper: "Network Dissection" (Bau et al., 2017)

**Conceptos clave:**
- âœ… Si un clasificador lineal puede predecir "color" desde layer1, esa capa codifica color
- âœ… Capas tempranas = features simples
- âœ… Capas profundas = conceptos abstractos

**Ejercicio mental:**
> Si entreno un clasificador para predecir "Â¿es un gato?" usando activaciones de layer4, Â¿tendrÃ¡ mejor accuracy que usando conv1?
> Respuesta: SÃ­, porque layer4 tiene representaciones de alto nivel

---

#### 3.3 Activation Patterns y Class Activation Maps
**Â¿QuÃ© estudiar?**
- **GradCAM** (Gradient-weighted Class Activation Mapping)
- Diferencia entre visualizar activaciones vs importancia
- Mapas de calor de atenciÃ³n
- Regiones de la imagen que activan cada neurona

**Recursos:**
- [ ] Paper: "Grad-CAM" (Selvaraju et al., 2017)
- [ ] Tutorial: Implementar GradCAM en PyTorch

**Conceptos clave:**
- âœ… GradCAM muestra "dÃ³nde mira" el modelo
- âœ… Combina gradientes + activaciones
- âœ… Ãštil para debugging de modelos

---

## ğŸ§ª Conceptos MatemÃ¡ticos Necesarios

### Ãlgebra Lineal BÃ¡sica
- [ ] MultiplicaciÃ³n de matrices
- [ ] ConvoluciÃ³n 2D (operaciÃ³n matemÃ¡tica)
- [ ] Broadcasting en NumPy/PyTorch

### CÃ¡lculo
- [ ] Derivadas parciales (para entender backprop)
- [ ] Gradiente de una funciÃ³n
- [ ] Gradient ascent vs descent

### EstadÃ­stica
- [ ] Media, varianza, desviaciÃ³n estÃ¡ndar
- [ ] Distribuciones (normal, uniforme)
- [ ] CorrelaciÃ³n

---

## ğŸ“ Checklist de PreparaciÃ³n por Notebook

### âœ… Antes del Notebook 01:
- [ ] Entender CNNs bÃ¡sicas
- [ ] Conocer arquitectura ResNet
- [ ] Saber quÃ© es una activaciÃ³n
- [ ] Concepto de forward pass
- [ ] Familiaridad con PyTorch bÃ¡sico

### âœ… Antes del Notebook 02:
- [ ] PyTorch hooks
- [ ] EstadÃ­sticas de activaciones (mean, std, sparsity)
- [ ] VisualizaciÃ³n de heatmaps

### âœ… Antes del Notebook 03:
- [ ] Feature visualization
- [ ] Activation maximization
- [ ] Gradient ascent

### âœ… Antes del Notebook 04:
- [ ] Neuron probing
- [ ] Clasificadores lineales
- [ ] Representaciones aprendidas

### âœ… Antes del Notebook 05:
- [ ] GradCAM
- [ ] Class activation maps
- [ ] IntegraciÃ³n de todas las tÃ©cnicas

---

## ğŸ“ Recursos Recomendados (Orden de Prioridad)

### Videos (MÃ¡s RÃ¡pido)
1. **StatQuest**: "Neural Networks Explained" (~30 min)
2. **3Blue1Brown**: "What is a neural network?" (~20 min)
3. **Yannic Kilcher**: "ResNet Explained" (~15 min)
4. **Two Minute Papers**: "Deep Visualization" (~5 min)

### ArtÃ­culos (Profundidad Media)
1. **Distill.pub**: "Feature Visualization" â­â­â­â­â­
2. **Distill.pub**: "The Building Blocks of Interpretability"
3. **CS231n**: Lecture Notes on CNNs
4. **PyTorch Docs**: Hooks Tutorial

### Papers (MÃ¡s Profundo)
1. **ResNet**: "Deep Residual Learning" (2015) - SecciÃ³n 3 solamente
2. **Network Dissection** (Bau et al., 2017) - IntroducciÃ³n + Figuras
3. **Grad-CAM** (Selvaraju et al., 2017) - MetodologÃ­a

---

## ğŸ’¡ Consejos de Estudio

### Estrategia 80/20:
- **80% prÃ¡ctica** (ejecutar cÃ³digo, modificar, experimentar)
- **20% teorÃ­a** (leer papers, ver videos)

### Plan de 3 DÃ­as:
**DÃ­a 1 (2 horas):**
- Ver videos sobre CNNs y ResNet
- Leer artÃ­culo de Distill.pub sobre Feature Visualization
- Ejecutar Notebook 01 celdas 1-7

**DÃ­a 2 (2 horas):**
- Estudiar PyTorch hooks (tutorial + ejemplos)
- Ejecutar Notebook 01 celdas 8-15
- Experimentar con diferentes capas

**DÃ­a 3 (2 horas):**
- Revisar conceptos de estadÃ­sticas de activaciones
- Analizar resultados del Notebook 01
- Documentar hallazgos en LEARNINGS.md

---

## â“ Preguntas de Auto-EvaluaciÃ³n

### Nivel BÃ¡sico:
1. Â¿QuÃ© es un filtro convolucional y quÃ© detecta?
2. Â¿Por quÃ© ResNet usa skip connections?
3. Â¿QuÃ© es una activaciÃ³n en una red neuronal?
4. Â¿QuÃ© hace la funciÃ³n ReLU?

### Nivel Intermedio:
1. Â¿CÃ³mo funciona un forward hook en PyTorch?
2. Â¿QuÃ© significa que una capa tenga 50% de sparsity?
3. Â¿Por quÃ© las primeras capas detectan bordes y las Ãºltimas objetos?
4. Â¿CÃ³mo interpretar un heatmap de activaciones?

### Nivel Avanzado:
1. Â¿CÃ³mo generar una imagen que maximice una neurona especÃ­fica?
2. Â¿QuÃ© revela un probing classifier sobre una capa?
3. Â¿CuÃ¡l es la diferencia entre visualizar pesos vs activaciones?
4. Â¿CÃ³mo funciona GradCAM internamente?

---

## ğŸ¯ Resultado Esperado

DespuÃ©s de estudiar estos temas, deberÃ­as poder:

âœ… Explicar quÃ© hace cada capa de ResNet  
âœ… Capturar activaciones de cualquier capa  
âœ… Interpretar visualizaciones de activaciones  
âœ… Identificar quÃ© detecta una neurona especÃ­fica  
âœ… Analizar estadÃ­sticas de activaciones  
âœ… Implementar tÃ©cnicas bÃ¡sicas de feature visualization  
âœ… DiseÃ±ar experimentos para entender quÃ© aprendiÃ³ tu modelo  

---

## ğŸ“Œ TL;DR - MÃ­nimo Necesario

Si solo tienes **1 hora**, estudia:
1. âœ… CNNs bÃ¡sicas (quÃ© es un filtro)
2. âœ… ResNet (skip connections)
3. âœ… Forward pass y activaciones
4. âœ… PyTorch hooks (cÃ³mo capturar activaciones)

Esto es suficiente para empezar con el Notebook 01.

---

**Â¿Listo para comenzar? ğŸš€**

Siguiente paso: Ejecutar `python verify_setup.py` y abrir el Notebook 01.